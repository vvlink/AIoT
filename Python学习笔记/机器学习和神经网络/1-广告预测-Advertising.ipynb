{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 神经网络和机器学习之广告预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "案例说明：Advertising（广告预测），使用全连接神经网络层。\n",
    "\n",
    "案例选择了keras框架，需要先安装keras和tensorflow。虚谷号教育版已经预装必要的库，可以直接使用。\n",
    "\n",
    "本案例已经提供了训练好的模型，放在`model`文件夹中，文件名称为：`1-model-vv.h5`。如果想直接测试模型，请跳到“导入模型”或者“应用模型”环节，输入数据开始识别。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.环境搭建\n",
    "\n",
    "下面是安装命令：\n",
    "\n",
    "pip install keras\n",
    "\n",
    "pip install -i https://pypi.tuna.tsinghua.edu.cn/simple tensorflow\n",
    "\n",
    "建议选择清华源，速度将快很多。参考命令如下：\n",
    "\n",
    "pip install -i https://pypi.tuna.tsinghua.edu.cn/simple tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.数据说明\n",
    "\n",
    "企业为了提高产品销售额，往往会在各种媒体投入资金展示自己的广告，常见的媒体有电视、广播以及报纸。作为决策者，需要确定在不同媒体的最佳广告投入，以期待最好的产品收益。为了寻求各媒体广告投入与收益之间的关系，还收集到了之前两百个不同产品在各个媒体中的广告投入资金与最终的销售业绩数据，这些数据被制成表格保存在“Advertising.csv”文件中。\n",
    "\n",
    "数据总共有5列，第1列表示数据的行号，没有列名；第2列到第4列分别是在电视、广播、报纸上的广告投入，最后一列则是对应的销售额。我们让机器学习这一组数据，然后再输入新的数据，让机器来预测可能的销售额。\n",
    "\n",
    "开始导入数据集吧，数据文件在`data`文件夹中。\n",
    "\n",
    "`Advertising.csv`文件是以纯文本形式存储表格数据（数字和文本）,文件一共201行，第一行为列名。文本内容如下：\n",
    "\n",
    ",TV,radio,newspaper,sales\n",
    "\n",
    "1,230.1,37.8,69.2,22.1\n",
    "\n",
    "2,44.5,39.3,45.1,10.4\n",
    "\n",
    "3,17.2,45.9,69.3,9.3\n",
    "\n",
    "……"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data=pd.read_csv('./data/Advertising.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用pandas的read_csv功能，读入csv文件中的数据。data是pandas中的DataFrame对象，是一个二维数组。head 和 tail 方法可以分别查看最前面几行和最后面几行的数据（默认为5）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>TV</th>\n",
       "      <th>radio</th>\n",
       "      <th>newspaper</th>\n",
       "      <th>sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>230.1</td>\n",
       "      <td>37.8</td>\n",
       "      <td>69.2</td>\n",
       "      <td>22.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>44.5</td>\n",
       "      <td>39.3</td>\n",
       "      <td>45.1</td>\n",
       "      <td>10.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>17.2</td>\n",
       "      <td>45.9</td>\n",
       "      <td>69.3</td>\n",
       "      <td>9.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>151.5</td>\n",
       "      <td>41.3</td>\n",
       "      <td>58.5</td>\n",
       "      <td>18.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>180.8</td>\n",
       "      <td>10.8</td>\n",
       "      <td>58.4</td>\n",
       "      <td>12.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     TV  radio  newspaper  sales\n",
       "0           1  230.1   37.8       69.2   22.1\n",
       "1           2   44.5   39.3       45.1   10.4\n",
       "2           3   17.2   45.9       69.3    9.3\n",
       "3           4  151.5   41.3       58.5   18.5\n",
       "4           5  180.8   10.8       58.4   12.9"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>TV</th>\n",
       "      <th>radio</th>\n",
       "      <th>newspaper</th>\n",
       "      <th>sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>196</td>\n",
       "      <td>38.2</td>\n",
       "      <td>3.7</td>\n",
       "      <td>13.8</td>\n",
       "      <td>7.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>197</td>\n",
       "      <td>94.2</td>\n",
       "      <td>4.9</td>\n",
       "      <td>8.1</td>\n",
       "      <td>9.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>198</td>\n",
       "      <td>177.0</td>\n",
       "      <td>9.3</td>\n",
       "      <td>6.4</td>\n",
       "      <td>12.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>199</td>\n",
       "      <td>283.6</td>\n",
       "      <td>42.0</td>\n",
       "      <td>66.2</td>\n",
       "      <td>25.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>200</td>\n",
       "      <td>232.1</td>\n",
       "      <td>8.6</td>\n",
       "      <td>8.7</td>\n",
       "      <td>13.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0     TV  radio  newspaper  sales\n",
       "195         196   38.2    3.7       13.8    7.6\n",
       "196         197   94.2    4.9        8.1    9.7\n",
       "197         198  177.0    9.3        6.4   12.8\n",
       "198         199  283.6   42.0       66.2   25.5\n",
       "199         200  232.1    8.6        8.7   13.4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来用pandas的iloc，对数据进行切片。`data.iloc[: , 1:-1]`表示x等于，y=data.iloc[: , -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=data.iloc[: , 1:-1]\n",
    "y=data.iloc[: , -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，x与y的形状分别是(200,3)和(200,1)，即x具有200行、3列，y具有200行、1列。其中x是输入的数据(电视、广播、报纸上的广告投入)，y是输出的结果(销售额)。可以输出其中某一行看看。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[151.5  41.3  58.5]\n",
      "18.5\n"
     ]
    }
   ],
   "source": [
    "print(x.values[3])\n",
    "print(y.values[3])  "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "为了让模型可以更好的用于对未知数据的预测，一般会把已有的数据分割成三个数据集，分别是训练集、验证集、测试集。\n",
    "\n",
    "对于这里的200组数据，可以将其先后顺序随机打乱，然后取前160组作为训练集数据，之后的20组为验证集数据，最后的20组为测试集数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.sample(frac=1).reset_index(drop=True)   #打乱数据的先后顺序\n",
    "x=data.iloc[:,1:-1]\n",
    "y=data.iloc[:,-1]\n",
    "x_train,y_train=x[:160],y[:160]\n",
    "x_val,y_val=x[160:180],y[160:180]\n",
    "x_test,y_test=x[180:],y[180:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.建立模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了调用方便，直接导入了keras的layers子集。keras支持建立序惯模型与函数式模型，在一般情况下，建立一个序贯模型就可以了。接着，为模型添加层，keras支持很多类型的神经网络层，这里使用add方法添加2个全连接神经网络层（Dense层）。\n",
    "\n",
    "第一层通过input_dim参数指定接收输入数据的维度为3（电视、广播、报纸3列），units=32表示将这个3维数据全连接到32个神经元，并通过relu激活函数进行激活，当然，这一层中的神经元个数并不一定需要设置为32个，但是较多的神经元个数使得模型具有更强的拟合能力；从第二层开始，输入数据维度默认为前一层的输出维度，因此不再需要指定输入数据的维度，只需要指定神经元个数即可，在上述代码中，第一层的32维输出再次全连接到第二层的1个神经元中，最后这1个神经元的输出就是模型的预测结果了。\n",
    "\n",
    "代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=keras.models.Sequential()\n",
    "model.add(layers.Dense(units=32,input_dim=3,activation='relu'))\n",
    "model.add(layers.Dense(units=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义好模型的层之后，需要对模型进行编译，同时指定训练模型所需要的优化器以及损失的估算方法。在keras中，可以通过optimizer参数来指定优化器。经验证明，adam优化器具有非常良好的表现。loss='mse'表示使用均方误差（mse）作为损失函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 编译模型\n",
    "model.compile(optimizer='adam',loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后对模型进行训练，一下代码利用现有数据x和y对模型进行训练1000次，epochs表示训练轮次，batch_size表示每次有多少行数据参与训练，最后把整个训练过程记录到history中。程序运行后，在控制台会打印出每轮次的训练情况。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1000\n",
      "160/160 [==============================] - 1s 7ms/step - loss: 2323.8715 - val_loss: 2612.5061\n",
      "Epoch 2/1000\n",
      "160/160 [==============================] - 0s 182us/step - loss: 2095.5718 - val_loss: 2364.4341\n",
      "Epoch 3/1000\n",
      "160/160 [==============================] - 0s 187us/step - loss: 1907.8130 - val_loss: 2123.1221\n",
      "Epoch 4/1000\n",
      "160/160 [==============================] - 0s 236us/step - loss: 1715.9583 - val_loss: 1898.7461\n",
      "Epoch 5/1000\n",
      "160/160 [==============================] - 0s 188us/step - loss: 1540.6941 - val_loss: 1691.2332\n",
      "Epoch 6/1000\n",
      "160/160 [==============================] - 0s 187us/step - loss: 1374.2649 - val_loss: 1501.2092\n",
      "Epoch 7/1000\n",
      "160/160 [==============================] - 0s 213us/step - loss: 1227.6837 - val_loss: 1327.0850\n",
      "Epoch 8/1000\n",
      "160/160 [==============================] - 0s 205us/step - loss: 1089.5367 - val_loss: 1170.2323\n",
      "Epoch 9/1000\n",
      "160/160 [==============================] - 0s 197us/step - loss: 966.9759 - val_loss: 1029.5011\n",
      "Epoch 10/1000\n",
      "160/160 [==============================] - 0s 190us/step - loss: 862.3857 - val_loss: 903.2527\n",
      "Epoch 11/1000\n",
      "160/160 [==============================] - 0s 205us/step - loss: 763.4689 - val_loss: 792.0186\n",
      "Epoch 12/1000\n",
      "160/160 [==============================] - 0s 200us/step - loss: 677.3984 - val_loss: 694.9216\n",
      "Epoch 13/1000\n",
      "160/160 [==============================] - 0s 211us/step - loss: 607.1591 - val_loss: 610.2282\n",
      "Epoch 14/1000\n",
      "160/160 [==============================] - 0s 202us/step - loss: 547.5773 - val_loss: 537.1460\n",
      "Epoch 15/1000\n",
      "160/160 [==============================] - 0s 221us/step - loss: 488.4575 - val_loss: 476.7660\n",
      "Epoch 16/1000\n",
      "160/160 [==============================] - 0s 207us/step - loss: 442.7407 - val_loss: 426.4207\n",
      "Epoch 17/1000\n",
      "160/160 [==============================] - 0s 202us/step - loss: 404.3964 - val_loss: 384.3831\n",
      "Epoch 18/1000\n",
      "160/160 [==============================] - 0s 219us/step - loss: 373.2605 - val_loss: 349.6093\n",
      "Epoch 19/1000\n",
      "160/160 [==============================] - 0s 220us/step - loss: 347.7056 - val_loss: 321.2261\n",
      "Epoch 20/1000\n",
      "160/160 [==============================] - 0s 215us/step - loss: 326.1151 - val_loss: 298.0489\n",
      "Epoch 21/1000\n",
      "160/160 [==============================] - 0s 181us/step - loss: 308.0205 - val_loss: 279.4639\n",
      "Epoch 22/1000\n",
      "160/160 [==============================] - 0s 249us/step - loss: 295.4550 - val_loss: 264.3066\n",
      "Epoch 23/1000\n",
      "160/160 [==============================] - 0s 215us/step - loss: 282.4314 - val_loss: 252.0886\n",
      "Epoch 24/1000\n",
      "160/160 [==============================] - 0s 193us/step - loss: 272.6535 - val_loss: 242.1785\n",
      "Epoch 25/1000\n",
      "160/160 [==============================] - 0s 186us/step - loss: 264.2041 - val_loss: 233.9742\n",
      "Epoch 26/1000\n",
      "160/160 [==============================] - 0s 198us/step - loss: 255.9370 - val_loss: 227.1424\n",
      "Epoch 27/1000\n",
      "160/160 [==============================] - 0s 177us/step - loss: 249.2853 - val_loss: 221.2064\n",
      "Epoch 28/1000\n",
      "160/160 [==============================] - 0s 203us/step - loss: 242.7656 - val_loss: 215.9398\n",
      "Epoch 29/1000\n",
      "160/160 [==============================] - 0s 194us/step - loss: 235.9996 - val_loss: 211.1934\n",
      "Epoch 30/1000\n",
      "160/160 [==============================] - 0s 202us/step - loss: 230.5469 - val_loss: 206.6849\n",
      "Epoch 31/1000\n",
      "160/160 [==============================] - 0s 207us/step - loss: 223.8646 - val_loss: 202.4250\n",
      "Epoch 32/1000\n",
      "160/160 [==============================] - 0s 176us/step - loss: 217.6609 - val_loss: 198.3107\n",
      "Epoch 33/1000\n",
      "160/160 [==============================] - 0s 172us/step - loss: 211.7111 - val_loss: 194.2189\n",
      "Epoch 34/1000\n",
      "160/160 [==============================] - 0s 186us/step - loss: 205.2708 - val_loss: 190.1300\n",
      "Epoch 35/1000\n",
      "160/160 [==============================] - 0s 198us/step - loss: 198.9144 - val_loss: 186.0368\n",
      "Epoch 36/1000\n",
      "160/160 [==============================] - ETA: 0s - loss: 188.905 - 0s 210us/step - loss: 192.6523 - val_loss: 181.8528\n",
      "Epoch 37/1000\n",
      "160/160 [==============================] - 0s 191us/step - loss: 186.1780 - val_loss: 177.6011\n",
      "Epoch 38/1000\n",
      "160/160 [==============================] - 0s 196us/step - loss: 180.2511 - val_loss: 173.3832\n",
      "Epoch 39/1000\n",
      "160/160 [==============================] - 0s 209us/step - loss: 173.5894 - val_loss: 168.8687\n",
      "Epoch 40/1000\n",
      "160/160 [==============================] - 0s 177us/step - loss: 167.4987 - val_loss: 164.4444\n",
      "Epoch 41/1000\n",
      "160/160 [==============================] - 0s 211us/step - loss: 161.7418 - val_loss: 159.9505\n",
      "Epoch 42/1000\n",
      "160/160 [==============================] - 0s 236us/step - loss: 155.7442 - val_loss: 155.4262\n",
      "Epoch 43/1000\n",
      "160/160 [==============================] - 0s 202us/step - loss: 150.3512 - val_loss: 150.8980\n",
      "Epoch 44/1000\n",
      "160/160 [==============================] - 0s 228us/step - loss: 144.6253 - val_loss: 145.7278\n",
      "Epoch 45/1000\n",
      "160/160 [==============================] - 0s 227us/step - loss: 139.0628 - val_loss: 140.5329\n",
      "Epoch 46/1000\n",
      "160/160 [==============================] - 0s 209us/step - loss: 133.8166 - val_loss: 135.5394\n",
      "Epoch 47/1000\n",
      "160/160 [==============================] - 0s 248us/step - loss: 128.5378 - val_loss: 130.7774\n",
      "Epoch 48/1000\n",
      "160/160 [==============================] - 0s 209us/step - loss: 123.4865 - val_loss: 126.1259\n",
      "Epoch 49/1000\n",
      "160/160 [==============================] - 0s 254us/step - loss: 119.2561 - val_loss: 121.5479\n",
      "Epoch 50/1000\n",
      "160/160 [==============================] - 0s 194us/step - loss: 114.4974 - val_loss: 117.1259\n",
      "Epoch 51/1000\n",
      "160/160 [==============================] - 0s 229us/step - loss: 110.1348 - val_loss: 112.6695\n",
      "Epoch 52/1000\n",
      "160/160 [==============================] - 0s 216us/step - loss: 105.8030 - val_loss: 108.3643\n",
      "Epoch 53/1000\n",
      "160/160 [==============================] - 0s 227us/step - loss: 101.4450 - val_loss: 104.1922\n",
      "Epoch 54/1000\n",
      "160/160 [==============================] - 0s 189us/step - loss: 97.5434 - val_loss: 100.1140\n",
      "Epoch 55/1000\n",
      "160/160 [==============================] - 0s 188us/step - loss: 93.6175 - val_loss: 96.1807\n",
      "Epoch 56/1000\n",
      "160/160 [==============================] - 0s 198us/step - loss: 90.0600 - val_loss: 92.3748\n",
      "Epoch 57/1000\n",
      "160/160 [==============================] - 0s 226us/step - loss: 85.8947 - val_loss: 88.7175\n",
      "Epoch 58/1000\n",
      "160/160 [==============================] - 0s 236us/step - loss: 82.3589 - val_loss: 85.0083\n",
      "Epoch 59/1000\n",
      "160/160 [==============================] - 0s 199us/step - loss: 78.5179 - val_loss: 80.6152\n",
      "Epoch 60/1000\n",
      "160/160 [==============================] - 0s 215us/step - loss: 74.5217 - val_loss: 76.1378\n",
      "Epoch 61/1000\n",
      "160/160 [==============================] - 0s 181us/step - loss: 70.6106 - val_loss: 71.4396\n",
      "Epoch 62/1000\n",
      "160/160 [==============================] - 0s 184us/step - loss: 66.8333 - val_loss: 66.7002\n",
      "Epoch 63/1000\n",
      "160/160 [==============================] - 0s 220us/step - loss: 63.2664 - val_loss: 61.7332\n",
      "Epoch 64/1000\n",
      "160/160 [==============================] - 0s 190us/step - loss: 59.7959 - val_loss: 57.3861\n",
      "Epoch 65/1000\n",
      "160/160 [==============================] - 0s 185us/step - loss: 56.7488 - val_loss: 54.3004\n",
      "Epoch 66/1000\n",
      "160/160 [==============================] - ETA: 0s - loss: 67.58 - 0s 193us/step - loss: 53.8331 - val_loss: 52.2041\n",
      "Epoch 67/1000\n",
      "160/160 [==============================] - 0s 190us/step - loss: 50.9882 - val_loss: 50.9793\n",
      "Epoch 68/1000\n",
      "160/160 [==============================] - ETA: 0s - loss: 40.56 - 0s 209us/step - loss: 48.7149 - val_loss: 49.5448\n",
      "Epoch 69/1000\n",
      "160/160 [==============================] - 0s 226us/step - loss: 46.2836 - val_loss: 47.6777\n",
      "Epoch 70/1000\n",
      "160/160 [==============================] - ETA: 0s - loss: 42.18 - 0s 190us/step - loss: 44.0626 - val_loss: 45.5856\n",
      "Epoch 71/1000\n",
      "160/160 [==============================] - 0s 244us/step - loss: 41.9408 - val_loss: 43.1805\n",
      "Epoch 72/1000\n",
      "160/160 [==============================] - 0s 190us/step - loss: 39.7028 - val_loss: 40.7827\n",
      "Epoch 73/1000\n",
      "160/160 [==============================] - 0s 180us/step - loss: 37.5838 - val_loss: 38.6111\n",
      "Epoch 74/1000\n",
      "160/160 [==============================] - 0s 198us/step - loss: 35.7787 - val_loss: 36.6796\n",
      "Epoch 75/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 215us/step - loss: 34.0912 - val_loss: 35.0171\n",
      "Epoch 76/1000\n",
      "160/160 [==============================] - 0s 171us/step - loss: 32.6524 - val_loss: 33.4719\n",
      "Epoch 77/1000\n",
      "160/160 [==============================] - 0s 234us/step - loss: 31.3453 - val_loss: 32.1311\n",
      "Epoch 78/1000\n",
      "160/160 [==============================] - 0s 199us/step - loss: 30.0326 - val_loss: 30.8799\n",
      "Epoch 79/1000\n",
      "160/160 [==============================] - 0s 193us/step - loss: 28.8449 - val_loss: 29.7343\n",
      "Epoch 80/1000\n",
      "160/160 [==============================] - 0s 202us/step - loss: 27.6425 - val_loss: 28.6197\n",
      "Epoch 81/1000\n",
      "160/160 [==============================] - 0s 188us/step - loss: 26.5045 - val_loss: 27.5366\n",
      "Epoch 82/1000\n",
      "160/160 [==============================] - 0s 181us/step - loss: 25.5356 - val_loss: 26.4596\n",
      "Epoch 83/1000\n",
      "160/160 [==============================] - 0s 192us/step - loss: 24.6114 - val_loss: 25.4443\n",
      "Epoch 84/1000\n",
      "160/160 [==============================] - 0s 175us/step - loss: 23.7291 - val_loss: 24.4032\n",
      "Epoch 85/1000\n",
      "160/160 [==============================] - 0s 220us/step - loss: 22.8971 - val_loss: 23.4399\n",
      "Epoch 86/1000\n",
      "160/160 [==============================] - 0s 177us/step - loss: 22.0576 - val_loss: 22.4532\n",
      "Epoch 87/1000\n",
      "160/160 [==============================] - 0s 223us/step - loss: 21.2972 - val_loss: 21.5221\n",
      "Epoch 88/1000\n",
      "160/160 [==============================] - 0s 190us/step - loss: 20.5567 - val_loss: 20.6159\n",
      "Epoch 89/1000\n",
      "160/160 [==============================] - 0s 176us/step - loss: 19.7333 - val_loss: 19.7689\n",
      "Epoch 90/1000\n",
      "160/160 [==============================] - 0s 205us/step - loss: 19.0947 - val_loss: 18.9696\n",
      "Epoch 91/1000\n",
      "160/160 [==============================] - 0s 203us/step - loss: 18.3882 - val_loss: 18.2842\n",
      "Epoch 92/1000\n",
      "160/160 [==============================] - 0s 253us/step - loss: 17.8066 - val_loss: 17.6508\n",
      "Epoch 93/1000\n",
      "160/160 [==============================] - 0s 186us/step - loss: 17.1323 - val_loss: 17.0300\n",
      "Epoch 94/1000\n",
      "160/160 [==============================] - 0s 185us/step - loss: 16.5767 - val_loss: 16.4931\n",
      "Epoch 95/1000\n",
      "160/160 [==============================] - 0s 202us/step - loss: 16.0230 - val_loss: 15.9395\n",
      "Epoch 96/1000\n",
      "160/160 [==============================] - 0s 203us/step - loss: 15.5053 - val_loss: 15.4440\n",
      "Epoch 97/1000\n",
      "160/160 [==============================] - 0s 217us/step - loss: 15.0284 - val_loss: 14.9673\n",
      "Epoch 98/1000\n",
      "160/160 [==============================] - 0s 214us/step - loss: 14.5759 - val_loss: 14.5315\n",
      "Epoch 99/1000\n",
      "160/160 [==============================] - 0s 203us/step - loss: 14.1404 - val_loss: 14.1171\n",
      "Epoch 100/1000\n",
      "160/160 [==============================] - 0s 204us/step - loss: 13.7509 - val_loss: 13.7129\n",
      "Epoch 101/1000\n",
      "160/160 [==============================] - 0s 215us/step - loss: 13.3441 - val_loss: 13.3424\n",
      "Epoch 102/1000\n",
      "160/160 [==============================] - 0s 206us/step - loss: 12.9748 - val_loss: 12.9825\n",
      "Epoch 103/1000\n",
      "160/160 [==============================] - 0s 201us/step - loss: 12.6577 - val_loss: 12.6561\n",
      "Epoch 104/1000\n",
      "160/160 [==============================] - 0s 194us/step - loss: 12.3173 - val_loss: 12.3369\n",
      "Epoch 105/1000\n",
      "160/160 [==============================] - 0s 214us/step - loss: 12.0111 - val_loss: 12.0456\n",
      "Epoch 106/1000\n",
      "160/160 [==============================] - 0s 193us/step - loss: 11.7221 - val_loss: 11.7586\n",
      "Epoch 107/1000\n",
      "160/160 [==============================] - 0s 191us/step - loss: 11.4463 - val_loss: 11.5138\n",
      "Epoch 108/1000\n",
      "160/160 [==============================] - 0s 202us/step - loss: 11.1906 - val_loss: 11.2512\n",
      "Epoch 109/1000\n",
      "160/160 [==============================] - 0s 207us/step - loss: 10.9346 - val_loss: 11.0212\n",
      "Epoch 110/1000\n",
      "160/160 [==============================] - 0s 214us/step - loss: 10.6919 - val_loss: 10.7944\n",
      "Epoch 111/1000\n",
      "160/160 [==============================] - ETA: 0s - loss: 12.07 - 0s 205us/step - loss: 10.4629 - val_loss: 10.5917\n",
      "Epoch 112/1000\n",
      "160/160 [==============================] - 0s 181us/step - loss: 10.2301 - val_loss: 10.4003\n",
      "Epoch 113/1000\n",
      "160/160 [==============================] - 0s 198us/step - loss: 10.0334 - val_loss: 10.1970\n",
      "Epoch 114/1000\n",
      "160/160 [==============================] - 0s 220us/step - loss: 9.8042 - val_loss: 10.0013\n",
      "Epoch 115/1000\n",
      "160/160 [==============================] - 0s 211us/step - loss: 9.6129 - val_loss: 9.8219\n",
      "Epoch 116/1000\n",
      "160/160 [==============================] - 0s 213us/step - loss: 9.4302 - val_loss: 9.6658\n",
      "Epoch 117/1000\n",
      "160/160 [==============================] - 0s 212us/step - loss: 9.2585 - val_loss: 9.5072\n",
      "Epoch 118/1000\n",
      "160/160 [==============================] - 0s 191us/step - loss: 9.0888 - val_loss: 9.3565\n",
      "Epoch 119/1000\n",
      "160/160 [==============================] - 0s 194us/step - loss: 8.9046 - val_loss: 9.2104\n",
      "Epoch 120/1000\n",
      "160/160 [==============================] - 0s 209us/step - loss: 8.7407 - val_loss: 9.0626\n",
      "Epoch 121/1000\n",
      "160/160 [==============================] - 0s 294us/step - loss: 8.5769 - val_loss: 8.9247\n",
      "Epoch 122/1000\n",
      "160/160 [==============================] - 0s 183us/step - loss: 8.4312 - val_loss: 8.7881\n",
      "Epoch 123/1000\n",
      "160/160 [==============================] - 0s 177us/step - loss: 8.2919 - val_loss: 8.6487\n",
      "Epoch 124/1000\n",
      "160/160 [==============================] - 0s 189us/step - loss: 8.1325 - val_loss: 8.5390\n",
      "Epoch 125/1000\n",
      "160/160 [==============================] - 0s 230us/step - loss: 7.9702 - val_loss: 8.4191\n",
      "Epoch 126/1000\n",
      "160/160 [==============================] - 0s 189us/step - loss: 7.8454 - val_loss: 8.2987\n",
      "Epoch 127/1000\n",
      "160/160 [==============================] - 0s 187us/step - loss: 7.7049 - val_loss: 8.1823\n",
      "Epoch 128/1000\n",
      "160/160 [==============================] - 0s 194us/step - loss: 7.5850 - val_loss: 8.0825\n",
      "Epoch 129/1000\n",
      "160/160 [==============================] - 0s 182us/step - loss: 7.4513 - val_loss: 7.9579\n",
      "Epoch 130/1000\n",
      "160/160 [==============================] - 0s 215us/step - loss: 7.3183 - val_loss: 7.8344\n",
      "Epoch 131/1000\n",
      "160/160 [==============================] - 0s 193us/step - loss: 7.1885 - val_loss: 7.7178\n",
      "Epoch 132/1000\n",
      "160/160 [==============================] - 0s 201us/step - loss: 7.0723 - val_loss: 7.6049\n",
      "Epoch 133/1000\n",
      "160/160 [==============================] - 0s 207us/step - loss: 6.9493 - val_loss: 7.4802\n",
      "Epoch 134/1000\n",
      "160/160 [==============================] - 0s 199us/step - loss: 6.8309 - val_loss: 7.3746\n",
      "Epoch 135/1000\n",
      "160/160 [==============================] - 0s 204us/step - loss: 6.7389 - val_loss: 7.2628\n",
      "Epoch 136/1000\n",
      "160/160 [==============================] - 0s 192us/step - loss: 6.6177 - val_loss: 7.1597\n",
      "Epoch 137/1000\n",
      "160/160 [==============================] - 0s 199us/step - loss: 6.5061 - val_loss: 7.0550\n",
      "Epoch 138/1000\n",
      "160/160 [==============================] - 0s 312us/step - loss: 6.3846 - val_loss: 6.9622\n",
      "Epoch 139/1000\n",
      "160/160 [==============================] - 0s 183us/step - loss: 6.2846 - val_loss: 6.8703\n",
      "Epoch 140/1000\n",
      "160/160 [==============================] - 0s 213us/step - loss: 6.1987 - val_loss: 6.7807\n",
      "Epoch 141/1000\n",
      "160/160 [==============================] - 0s 183us/step - loss: 6.0857 - val_loss: 6.6873\n",
      "Epoch 142/1000\n",
      "160/160 [==============================] - 0s 175us/step - loss: 5.9917 - val_loss: 6.5931\n",
      "Epoch 143/1000\n",
      "160/160 [==============================] - 0s 228us/step - loss: 5.8871 - val_loss: 6.5000\n",
      "Epoch 144/1000\n",
      "160/160 [==============================] - 0s 185us/step - loss: 5.7963 - val_loss: 6.4087\n",
      "Epoch 145/1000\n",
      "160/160 [==============================] - 0s 198us/step - loss: 5.7096 - val_loss: 6.3160\n",
      "Epoch 146/1000\n",
      "160/160 [==============================] - 0s 191us/step - loss: 5.6144 - val_loss: 6.2289\n",
      "Epoch 147/1000\n",
      "160/160 [==============================] - 0s 210us/step - loss: 5.5314 - val_loss: 6.1453\n",
      "Epoch 148/1000\n",
      "160/160 [==============================] - 0s 248us/step - loss: 5.4511 - val_loss: 6.0720\n",
      "Epoch 149/1000\n",
      "160/160 [==============================] - 0s 233us/step - loss: 5.3663 - val_loss: 6.0008\n",
      "Epoch 150/1000\n",
      "160/160 [==============================] - 0s 242us/step - loss: 5.2938 - val_loss: 5.9374\n",
      "Epoch 151/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 219us/step - loss: 5.2117 - val_loss: 5.8682\n",
      "Epoch 152/1000\n",
      "160/160 [==============================] - 0s 189us/step - loss: 5.1322 - val_loss: 5.8064\n",
      "Epoch 153/1000\n",
      "160/160 [==============================] - 0s 225us/step - loss: 5.0600 - val_loss: 5.7488\n",
      "Epoch 154/1000\n",
      "160/160 [==============================] - 0s 230us/step - loss: 4.9874 - val_loss: 5.6910\n",
      "Epoch 155/1000\n",
      "160/160 [==============================] - 0s 231us/step - loss: 4.9241 - val_loss: 5.6366\n",
      "Epoch 156/1000\n",
      "160/160 [==============================] - 0s 242us/step - loss: 4.8520 - val_loss: 5.5769\n",
      "Epoch 157/1000\n",
      "160/160 [==============================] - 0s 228us/step - loss: 4.7813 - val_loss: 5.5205\n",
      "Epoch 158/1000\n",
      "160/160 [==============================] - 0s 240us/step - loss: 4.7213 - val_loss: 5.4641\n",
      "Epoch 159/1000\n",
      "160/160 [==============================] - 0s 247us/step - loss: 4.6560 - val_loss: 5.4153\n",
      "Epoch 160/1000\n",
      "160/160 [==============================] - 0s 249us/step - loss: 4.5956 - val_loss: 5.3593\n",
      "Epoch 161/1000\n",
      "160/160 [==============================] - 0s 215us/step - loss: 4.5425 - val_loss: 5.3144\n",
      "Epoch 162/1000\n",
      "160/160 [==============================] - 0s 242us/step - loss: 4.4783 - val_loss: 5.2620\n",
      "Epoch 163/1000\n",
      "160/160 [==============================] - 0s 221us/step - loss: 4.4220 - val_loss: 5.2134\n",
      "Epoch 164/1000\n",
      "160/160 [==============================] - 0s 243us/step - loss: 4.3723 - val_loss: 5.1756\n",
      "Epoch 165/1000\n",
      "160/160 [==============================] - 0s 227us/step - loss: 4.3155 - val_loss: 5.1256\n",
      "Epoch 166/1000\n",
      "160/160 [==============================] - 0s 208us/step - loss: 4.2648 - val_loss: 5.0749\n",
      "Epoch 167/1000\n",
      "160/160 [==============================] - 0s 246us/step - loss: 4.2199 - val_loss: 5.0250\n",
      "Epoch 168/1000\n",
      "160/160 [==============================] - 0s 219us/step - loss: 4.1703 - val_loss: 4.9700\n",
      "Epoch 169/1000\n",
      "160/160 [==============================] - 0s 202us/step - loss: 4.1329 - val_loss: 4.9196\n",
      "Epoch 170/1000\n",
      "160/160 [==============================] - 0s 198us/step - loss: 4.0757 - val_loss: 4.8749\n",
      "Epoch 171/1000\n",
      "160/160 [==============================] - 0s 199us/step - loss: 4.0361 - val_loss: 4.8319\n",
      "Epoch 172/1000\n",
      "160/160 [==============================] - 0s 241us/step - loss: 3.9876 - val_loss: 4.7979\n",
      "Epoch 173/1000\n",
      "160/160 [==============================] - 0s 199us/step - loss: 3.9444 - val_loss: 4.7625\n",
      "Epoch 174/1000\n",
      "160/160 [==============================] - 0s 195us/step - loss: 3.9070 - val_loss: 4.7277\n",
      "Epoch 175/1000\n",
      "160/160 [==============================] - 0s 208us/step - loss: 3.8686 - val_loss: 4.6881\n",
      "Epoch 176/1000\n",
      "160/160 [==============================] - 0s 200us/step - loss: 3.8343 - val_loss: 4.6426\n",
      "Epoch 177/1000\n",
      "160/160 [==============================] - 0s 196us/step - loss: 3.7856 - val_loss: 4.6073\n",
      "Epoch 178/1000\n",
      "160/160 [==============================] - 0s 195us/step - loss: 3.7558 - val_loss: 4.5741\n",
      "Epoch 179/1000\n",
      "160/160 [==============================] - 0s 193us/step - loss: 3.7175 - val_loss: 4.5405\n",
      "Epoch 180/1000\n",
      "160/160 [==============================] - 0s 237us/step - loss: 3.6887 - val_loss: 4.5067\n",
      "Epoch 181/1000\n",
      "160/160 [==============================] - 0s 181us/step - loss: 3.6532 - val_loss: 4.4757\n",
      "Epoch 182/1000\n",
      "160/160 [==============================] - 0s 211us/step - loss: 3.6235 - val_loss: 4.4459\n",
      "Epoch 183/1000\n",
      "160/160 [==============================] - 0s 213us/step - loss: 3.5982 - val_loss: 4.4148\n",
      "Epoch 184/1000\n",
      "160/160 [==============================] - 0s 225us/step - loss: 3.5660 - val_loss: 4.3874\n",
      "Epoch 185/1000\n",
      "160/160 [==============================] - 0s 199us/step - loss: 3.5362 - val_loss: 4.3561\n",
      "Epoch 186/1000\n",
      "160/160 [==============================] - 0s 234us/step - loss: 3.5145 - val_loss: 4.3222\n",
      "Epoch 187/1000\n",
      "160/160 [==============================] - 0s 230us/step - loss: 3.4885 - val_loss: 4.2924\n",
      "Epoch 188/1000\n",
      "160/160 [==============================] - 0s 218us/step - loss: 3.4675 - val_loss: 4.2653\n",
      "Epoch 189/1000\n",
      "160/160 [==============================] - 0s 192us/step - loss: 3.4423 - val_loss: 4.2403\n",
      "Epoch 190/1000\n",
      "160/160 [==============================] - 0s 176us/step - loss: 3.4231 - val_loss: 4.2152\n",
      "Epoch 191/1000\n",
      "160/160 [==============================] - 0s 222us/step - loss: 3.4041 - val_loss: 4.1898\n",
      "Epoch 192/1000\n",
      "160/160 [==============================] - 0s 204us/step - loss: 3.3837 - val_loss: 4.1562\n",
      "Epoch 193/1000\n",
      "160/160 [==============================] - 0s 209us/step - loss: 3.3595 - val_loss: 4.1299\n",
      "Epoch 194/1000\n",
      "160/160 [==============================] - 0s 209us/step - loss: 3.3408 - val_loss: 4.1015\n",
      "Epoch 195/1000\n",
      "160/160 [==============================] - 0s 201us/step - loss: 3.3266 - val_loss: 4.0749\n",
      "Epoch 196/1000\n",
      "160/160 [==============================] - 0s 225us/step - loss: 3.3053 - val_loss: 4.0518\n",
      "Epoch 197/1000\n",
      "160/160 [==============================] - ETA: 0s - loss: 3.335 - 0s 212us/step - loss: 3.2879 - val_loss: 4.0258\n",
      "Epoch 198/1000\n",
      "160/160 [==============================] - 0s 225us/step - loss: 3.2710 - val_loss: 3.9999\n",
      "Epoch 199/1000\n",
      "160/160 [==============================] - 0s 206us/step - loss: 3.2551 - val_loss: 3.9783\n",
      "Epoch 200/1000\n",
      "160/160 [==============================] - 0s 206us/step - loss: 3.2430 - val_loss: 3.9579\n",
      "Epoch 201/1000\n",
      "160/160 [==============================] - 0s 205us/step - loss: 3.2268 - val_loss: 3.9381\n",
      "Epoch 202/1000\n",
      "160/160 [==============================] - 0s 227us/step - loss: 3.2135 - val_loss: 3.9204\n",
      "Epoch 203/1000\n",
      "160/160 [==============================] - 0s 186us/step - loss: 3.2084 - val_loss: 3.9006\n",
      "Epoch 204/1000\n",
      "160/160 [==============================] - 0s 191us/step - loss: 3.1887 - val_loss: 3.8852\n",
      "Epoch 205/1000\n",
      "160/160 [==============================] - 0s 220us/step - loss: 3.1787 - val_loss: 3.8715\n",
      "Epoch 206/1000\n",
      "160/160 [==============================] - 0s 194us/step - loss: 3.1684 - val_loss: 3.8547\n",
      "Epoch 207/1000\n",
      "160/160 [==============================] - 0s 210us/step - loss: 3.1567 - val_loss: 3.8391\n",
      "Epoch 208/1000\n",
      "160/160 [==============================] - 0s 212us/step - loss: 3.1547 - val_loss: 3.8255\n",
      "Epoch 209/1000\n",
      "160/160 [==============================] - 0s 230us/step - loss: 3.1417 - val_loss: 3.8054\n",
      "Epoch 210/1000\n",
      "160/160 [==============================] - 0s 201us/step - loss: 3.1290 - val_loss: 3.7883\n",
      "Epoch 211/1000\n",
      "160/160 [==============================] - 0s 174us/step - loss: 3.1229 - val_loss: 3.7707\n",
      "Epoch 212/1000\n",
      "160/160 [==============================] - 0s 265us/step - loss: 3.1130 - val_loss: 3.7559\n",
      "Epoch 213/1000\n",
      "160/160 [==============================] - 0s 201us/step - loss: 3.1051 - val_loss: 3.7428\n",
      "Epoch 214/1000\n",
      "160/160 [==============================] - 0s 184us/step - loss: 3.0985 - val_loss: 3.7293\n",
      "Epoch 215/1000\n",
      "160/160 [==============================] - 0s 214us/step - loss: 3.0948 - val_loss: 3.7197\n",
      "Epoch 216/1000\n",
      "160/160 [==============================] - 0s 193us/step - loss: 3.0838 - val_loss: 3.7073\n",
      "Epoch 217/1000\n",
      "160/160 [==============================] - 0s 205us/step - loss: 3.0773 - val_loss: 3.6963\n",
      "Epoch 218/1000\n",
      "160/160 [==============================] - 0s 209us/step - loss: 3.0699 - val_loss: 3.6862\n",
      "Epoch 219/1000\n",
      "160/160 [==============================] - 0s 195us/step - loss: 3.0620 - val_loss: 3.6765\n",
      "Epoch 220/1000\n",
      "160/160 [==============================] - 0s 202us/step - loss: 3.0574 - val_loss: 3.6660\n",
      "Epoch 221/1000\n",
      "160/160 [==============================] - 0s 201us/step - loss: 3.0512 - val_loss: 3.6578\n",
      "Epoch 222/1000\n",
      "160/160 [==============================] - 0s 208us/step - loss: 3.0499 - val_loss: 3.6479\n",
      "Epoch 223/1000\n",
      "160/160 [==============================] - 0s 194us/step - loss: 3.0414 - val_loss: 3.6413\n",
      "Epoch 224/1000\n",
      "160/160 [==============================] - 0s 194us/step - loss: 3.0333 - val_loss: 3.6304\n",
      "Epoch 225/1000\n",
      "160/160 [==============================] - 0s 183us/step - loss: 3.0271 - val_loss: 3.6187\n",
      "Epoch 226/1000\n",
      "160/160 [==============================] - 0s 241us/step - loss: 3.0220 - val_loss: 3.6094\n",
      "Epoch 227/1000\n",
      "160/160 [==============================] - ETA: 0s - loss: 2.910 - 0s 186us/step - loss: 3.0169 - val_loss: 3.6001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 228/1000\n",
      "160/160 [==============================] - 0s 218us/step - loss: 3.0102 - val_loss: 3.5879\n",
      "Epoch 229/1000\n",
      "160/160 [==============================] - 0s 195us/step - loss: 3.0075 - val_loss: 3.5793\n",
      "Epoch 230/1000\n",
      "160/160 [==============================] - 0s 191us/step - loss: 3.0008 - val_loss: 3.5671\n",
      "Epoch 231/1000\n",
      "160/160 [==============================] - 0s 226us/step - loss: 2.9975 - val_loss: 3.5591\n",
      "Epoch 232/1000\n",
      "160/160 [==============================] - 0s 180us/step - loss: 2.9884 - val_loss: 3.5512\n",
      "Epoch 233/1000\n",
      "160/160 [==============================] - 0s 188us/step - loss: 2.9888 - val_loss: 3.5436\n",
      "Epoch 234/1000\n",
      "160/160 [==============================] - 0s 213us/step - loss: 2.9787 - val_loss: 3.5324\n",
      "Epoch 235/1000\n",
      "160/160 [==============================] - 0s 208us/step - loss: 2.9726 - val_loss: 3.5237\n",
      "Epoch 236/1000\n",
      "160/160 [==============================] - 0s 218us/step - loss: 2.9698 - val_loss: 3.5144\n",
      "Epoch 237/1000\n",
      "160/160 [==============================] - 0s 224us/step - loss: 2.9641 - val_loss: 3.5067\n",
      "Epoch 238/1000\n",
      "160/160 [==============================] - 0s 211us/step - loss: 2.9583 - val_loss: 3.5007\n",
      "Epoch 239/1000\n",
      "160/160 [==============================] - 0s 190us/step - loss: 2.9562 - val_loss: 3.4988\n",
      "Epoch 240/1000\n",
      "160/160 [==============================] - 0s 223us/step - loss: 2.9528 - val_loss: 3.4953\n",
      "Epoch 241/1000\n",
      "160/160 [==============================] - 0s 177us/step - loss: 2.9442 - val_loss: 3.4800\n",
      "Epoch 242/1000\n",
      "160/160 [==============================] - 0s 180us/step - loss: 2.9387 - val_loss: 3.4667\n",
      "Epoch 243/1000\n",
      "160/160 [==============================] - 0s 205us/step - loss: 2.9337 - val_loss: 3.4570\n",
      "Epoch 244/1000\n",
      "160/160 [==============================] - 0s 204us/step - loss: 2.9286 - val_loss: 3.4484\n",
      "Epoch 245/1000\n",
      "160/160 [==============================] - 0s 228us/step - loss: 2.9247 - val_loss: 3.4398\n",
      "Epoch 246/1000\n",
      "160/160 [==============================] - 0s 206us/step - loss: 2.9199 - val_loss: 3.4351\n",
      "Epoch 247/1000\n",
      "160/160 [==============================] - 0s 225us/step - loss: 2.9138 - val_loss: 3.4297\n",
      "Epoch 248/1000\n",
      "160/160 [==============================] - 0s 218us/step - loss: 2.9097 - val_loss: 3.4255\n",
      "Epoch 249/1000\n",
      "160/160 [==============================] - 0s 223us/step - loss: 2.9063 - val_loss: 3.4177\n",
      "Epoch 250/1000\n",
      "160/160 [==============================] - 0s 229us/step - loss: 2.9019 - val_loss: 3.4066\n",
      "Epoch 251/1000\n",
      "160/160 [==============================] - 0s 196us/step - loss: 2.8957 - val_loss: 3.3964\n",
      "Epoch 252/1000\n",
      "160/160 [==============================] - 0s 205us/step - loss: 2.8923 - val_loss: 3.3866\n",
      "Epoch 253/1000\n",
      "160/160 [==============================] - 0s 237us/step - loss: 2.8876 - val_loss: 3.3780\n",
      "Epoch 254/1000\n",
      "160/160 [==============================] - 0s 213us/step - loss: 2.8838 - val_loss: 3.3709\n",
      "Epoch 255/1000\n",
      "160/160 [==============================] - 0s 186us/step - loss: 2.8793 - val_loss: 3.3644\n",
      "Epoch 256/1000\n",
      "160/160 [==============================] - 0s 205us/step - loss: 2.8747 - val_loss: 3.3563\n",
      "Epoch 257/1000\n",
      "160/160 [==============================] - 0s 238us/step - loss: 2.8704 - val_loss: 3.3472\n",
      "Epoch 258/1000\n",
      "160/160 [==============================] - 0s 199us/step - loss: 2.8674 - val_loss: 3.3359\n",
      "Epoch 259/1000\n",
      "160/160 [==============================] - 0s 214us/step - loss: 2.8630 - val_loss: 3.3267\n",
      "Epoch 260/1000\n",
      "160/160 [==============================] - 0s 229us/step - loss: 2.8588 - val_loss: 3.3178\n",
      "Epoch 261/1000\n",
      "160/160 [==============================] - 0s 214us/step - loss: 2.8551 - val_loss: 3.3117\n",
      "Epoch 262/1000\n",
      "160/160 [==============================] - 0s 193us/step - loss: 2.8513 - val_loss: 3.3040\n",
      "Epoch 263/1000\n",
      "160/160 [==============================] - 0s 221us/step - loss: 2.8467 - val_loss: 3.2939\n",
      "Epoch 264/1000\n",
      "160/160 [==============================] - 0s 207us/step - loss: 2.8431 - val_loss: 3.2834\n",
      "Epoch 265/1000\n",
      "160/160 [==============================] - 0s 179us/step - loss: 2.8388 - val_loss: 3.2747\n",
      "Epoch 266/1000\n",
      "160/160 [==============================] - 0s 209us/step - loss: 2.8356 - val_loss: 3.2689\n",
      "Epoch 267/1000\n",
      "160/160 [==============================] - 0s 199us/step - loss: 2.8349 - val_loss: 3.2641\n",
      "Epoch 268/1000\n",
      "160/160 [==============================] - 0s 216us/step - loss: 2.8273 - val_loss: 3.2552\n",
      "Epoch 269/1000\n",
      "160/160 [==============================] - 0s 202us/step - loss: 2.8233 - val_loss: 3.2488\n",
      "Epoch 270/1000\n",
      "160/160 [==============================] - 0s 249us/step - loss: 2.8211 - val_loss: 3.2419\n",
      "Epoch 271/1000\n",
      "160/160 [==============================] - 0s 204us/step - loss: 2.8162 - val_loss: 3.2367\n",
      "Epoch 272/1000\n",
      "160/160 [==============================] - 0s 191us/step - loss: 2.8123 - val_loss: 3.2311\n",
      "Epoch 273/1000\n",
      "160/160 [==============================] - 0s 206us/step - loss: 2.8099 - val_loss: 3.2245\n",
      "Epoch 274/1000\n",
      "160/160 [==============================] - 0s 198us/step - loss: 2.8057 - val_loss: 3.2152\n",
      "Epoch 275/1000\n",
      "160/160 [==============================] - 0s 200us/step - loss: 2.8035 - val_loss: 3.2048\n",
      "Epoch 276/1000\n",
      "160/160 [==============================] - 0s 216us/step - loss: 2.8023 - val_loss: 3.1989\n",
      "Epoch 277/1000\n",
      "160/160 [==============================] - 0s 212us/step - loss: 2.7965 - val_loss: 3.1900\n",
      "Epoch 278/1000\n",
      "160/160 [==============================] - 0s 237us/step - loss: 2.7915 - val_loss: 3.1839\n",
      "Epoch 279/1000\n",
      "160/160 [==============================] - ETA: 0s - loss: 3.104 - 0s 238us/step - loss: 2.7893 - val_loss: 3.1790\n",
      "Epoch 280/1000\n",
      "160/160 [==============================] - 0s 220us/step - loss: 2.7887 - val_loss: 3.1684\n",
      "Epoch 281/1000\n",
      "160/160 [==============================] - 0s 209us/step - loss: 2.7858 - val_loss: 3.1626\n",
      "Epoch 282/1000\n",
      "160/160 [==============================] - 0s 204us/step - loss: 2.7896 - val_loss: 3.1500\n",
      "Epoch 283/1000\n",
      "160/160 [==============================] - 0s 220us/step - loss: 2.7754 - val_loss: 3.1416\n",
      "Epoch 284/1000\n",
      "160/160 [==============================] - 0s 214us/step - loss: 2.7720 - val_loss: 3.1356\n",
      "Epoch 285/1000\n",
      "160/160 [==============================] - 0s 222us/step - loss: 2.7729 - val_loss: 3.1320\n",
      "Epoch 286/1000\n",
      "160/160 [==============================] - 0s 226us/step - loss: 2.7667 - val_loss: 3.1242\n",
      "Epoch 287/1000\n",
      "160/160 [==============================] - ETA: 0s - loss: 3.412 - 0s 238us/step - loss: 2.7618 - val_loss: 3.1128\n",
      "Epoch 288/1000\n",
      "160/160 [==============================] - 0s 217us/step - loss: 2.7578 - val_loss: 3.1028\n",
      "Epoch 289/1000\n",
      "160/160 [==============================] - 0s 223us/step - loss: 2.7575 - val_loss: 3.0927\n",
      "Epoch 290/1000\n",
      "160/160 [==============================] - 0s 217us/step - loss: 2.7539 - val_loss: 3.0858\n",
      "Epoch 291/1000\n",
      "160/160 [==============================] - 0s 199us/step - loss: 2.7499 - val_loss: 3.0790\n",
      "Epoch 292/1000\n",
      "160/160 [==============================] - 0s 237us/step - loss: 2.7453 - val_loss: 3.0743\n",
      "Epoch 293/1000\n",
      "160/160 [==============================] - 0s 223us/step - loss: 2.7422 - val_loss: 3.0714\n",
      "Epoch 294/1000\n",
      "160/160 [==============================] - 0s 212us/step - loss: 2.7401 - val_loss: 3.0658\n",
      "Epoch 295/1000\n",
      "160/160 [==============================] - 0s 196us/step - loss: 2.7368 - val_loss: 3.0559\n",
      "Epoch 296/1000\n",
      "160/160 [==============================] - 0s 214us/step - loss: 2.7328 - val_loss: 3.0468\n",
      "Epoch 297/1000\n",
      "160/160 [==============================] - 0s 225us/step - loss: 2.7338 - val_loss: 3.0381\n",
      "Epoch 298/1000\n",
      "160/160 [==============================] - 0s 186us/step - loss: 2.7275 - val_loss: 3.0318\n",
      "Epoch 299/1000\n",
      "160/160 [==============================] - 0s 197us/step - loss: 2.7254 - val_loss: 3.0270\n",
      "Epoch 300/1000\n",
      "160/160 [==============================] - 0s 224us/step - loss: 2.7195 - val_loss: 3.0187\n",
      "Epoch 301/1000\n",
      "160/160 [==============================] - 0s 201us/step - loss: 2.7161 - val_loss: 3.0101\n",
      "Epoch 302/1000\n",
      "160/160 [==============================] - 0s 204us/step - loss: 2.7134 - val_loss: 3.0019\n",
      "Epoch 303/1000\n",
      "160/160 [==============================] - 0s 191us/step - loss: 2.7117 - val_loss: 2.9961\n",
      "Epoch 304/1000\n",
      "160/160 [==============================] - 0s 214us/step - loss: 2.7070 - val_loss: 2.9894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 305/1000\n",
      "160/160 [==============================] - 0s 208us/step - loss: 2.7035 - val_loss: 2.9834\n",
      "Epoch 306/1000\n",
      "160/160 [==============================] - 0s 239us/step - loss: 2.7017 - val_loss: 2.9786\n",
      "Epoch 307/1000\n",
      "160/160 [==============================] - 0s 234us/step - loss: 2.6971 - val_loss: 2.9706\n",
      "Epoch 308/1000\n",
      "160/160 [==============================] - 0s 215us/step - loss: 2.6949 - val_loss: 2.9636\n",
      "Epoch 309/1000\n",
      "160/160 [==============================] - 0s 240us/step - loss: 2.6911 - val_loss: 2.9588\n",
      "Epoch 310/1000\n",
      "160/160 [==============================] - 0s 226us/step - loss: 2.6882 - val_loss: 2.9530\n",
      "Epoch 311/1000\n",
      "160/160 [==============================] - 0s 191us/step - loss: 2.6850 - val_loss: 2.9465\n",
      "Epoch 312/1000\n",
      "160/160 [==============================] - 0s 190us/step - loss: 2.6848 - val_loss: 2.9395\n",
      "Epoch 313/1000\n",
      "160/160 [==============================] - 0s 196us/step - loss: 2.6781 - val_loss: 2.9322\n",
      "Epoch 314/1000\n",
      "160/160 [==============================] - 0s 197us/step - loss: 2.6748 - val_loss: 2.9248\n",
      "Epoch 315/1000\n",
      "160/160 [==============================] - 0s 207us/step - loss: 2.6730 - val_loss: 2.9204\n",
      "Epoch 316/1000\n",
      "160/160 [==============================] - 0s 204us/step - loss: 2.6730 - val_loss: 2.9145\n",
      "Epoch 317/1000\n",
      "160/160 [==============================] - 0s 244us/step - loss: 2.6661 - val_loss: 2.9068\n",
      "Epoch 318/1000\n",
      "160/160 [==============================] - 0s 204us/step - loss: 2.6648 - val_loss: 2.9004\n",
      "Epoch 319/1000\n",
      "160/160 [==============================] - 0s 225us/step - loss: 2.6632 - val_loss: 2.8962\n",
      "Epoch 320/1000\n",
      "160/160 [==============================] - 0s 193us/step - loss: 2.6643 - val_loss: 2.8911\n",
      "Epoch 321/1000\n",
      "160/160 [==============================] - 0s 208us/step - loss: 2.6558 - val_loss: 2.8868\n",
      "Epoch 322/1000\n",
      "160/160 [==============================] - 0s 195us/step - loss: 2.6516 - val_loss: 2.8838\n",
      "Epoch 323/1000\n",
      "160/160 [==============================] - 0s 191us/step - loss: 2.6499 - val_loss: 2.8796\n",
      "Epoch 324/1000\n",
      "160/160 [==============================] - 0s 192us/step - loss: 2.6495 - val_loss: 2.8731\n",
      "Epoch 325/1000\n",
      "160/160 [==============================] - 0s 210us/step - loss: 2.6423 - val_loss: 2.8676\n",
      "Epoch 326/1000\n",
      "160/160 [==============================] - 0s 204us/step - loss: 2.6415 - val_loss: 2.8604\n",
      "Epoch 327/1000\n",
      "160/160 [==============================] - 0s 218us/step - loss: 2.6363 - val_loss: 2.8536\n",
      "Epoch 328/1000\n",
      "160/160 [==============================] - 0s 230us/step - loss: 2.6361 - val_loss: 2.8488\n",
      "Epoch 329/1000\n",
      "160/160 [==============================] - 0s 242us/step - loss: 2.6315 - val_loss: 2.8455\n",
      "Epoch 330/1000\n",
      "160/160 [==============================] - 0s 228us/step - loss: 2.6277 - val_loss: 2.8420\n",
      "Epoch 331/1000\n",
      "160/160 [==============================] - 0s 242us/step - loss: 2.6268 - val_loss: 2.8368\n",
      "Epoch 332/1000\n",
      "160/160 [==============================] - 0s 212us/step - loss: 2.6249 - val_loss: 2.8311\n",
      "Epoch 333/1000\n",
      "160/160 [==============================] - 0s 202us/step - loss: 2.6203 - val_loss: 2.8286\n",
      "Epoch 334/1000\n",
      "160/160 [==============================] - 0s 221us/step - loss: 2.6163 - val_loss: 2.8227\n",
      "Epoch 335/1000\n",
      "160/160 [==============================] - 0s 239us/step - loss: 2.6129 - val_loss: 2.8180\n",
      "Epoch 336/1000\n",
      "160/160 [==============================] - 0s 223us/step - loss: 2.6103 - val_loss: 2.8119\n",
      "Epoch 337/1000\n",
      "160/160 [==============================] - 0s 197us/step - loss: 2.6081 - val_loss: 2.8065\n",
      "Epoch 338/1000\n",
      "160/160 [==============================] - 0s 201us/step - loss: 2.6043 - val_loss: 2.8008\n",
      "Epoch 339/1000\n",
      "160/160 [==============================] - 0s 221us/step - loss: 2.6008 - val_loss: 2.7956\n",
      "Epoch 340/1000\n",
      "160/160 [==============================] - 0s 197us/step - loss: 2.5992 - val_loss: 2.7915\n",
      "Epoch 341/1000\n",
      "160/160 [==============================] - 0s 218us/step - loss: 2.5964 - val_loss: 2.7862\n",
      "Epoch 342/1000\n",
      "160/160 [==============================] - ETA: 0s - loss: 2.469 - 0s 179us/step - loss: 2.5927 - val_loss: 2.7798\n",
      "Epoch 343/1000\n",
      "160/160 [==============================] - 0s 200us/step - loss: 2.5902 - val_loss: 2.7752\n",
      "Epoch 344/1000\n",
      "160/160 [==============================] - 0s 179us/step - loss: 2.5873 - val_loss: 2.7700\n",
      "Epoch 345/1000\n",
      "160/160 [==============================] - 0s 175us/step - loss: 2.5855 - val_loss: 2.7655\n",
      "Epoch 346/1000\n",
      "160/160 [==============================] - 0s 223us/step - loss: 2.5838 - val_loss: 2.7614\n",
      "Epoch 347/1000\n",
      "160/160 [==============================] - 0s 192us/step - loss: 2.5786 - val_loss: 2.7593\n",
      "Epoch 348/1000\n",
      "160/160 [==============================] - 0s 197us/step - loss: 2.5759 - val_loss: 2.7559\n",
      "Epoch 349/1000\n",
      "160/160 [==============================] - 0s 179us/step - loss: 2.5754 - val_loss: 2.7530\n",
      "Epoch 350/1000\n",
      "160/160 [==============================] - 0s 168us/step - loss: 2.5729 - val_loss: 2.7462\n",
      "Epoch 351/1000\n",
      "160/160 [==============================] - 0s 197us/step - loss: 2.5706 - val_loss: 2.7416\n",
      "Epoch 352/1000\n",
      "160/160 [==============================] - 0s 213us/step - loss: 2.5650 - val_loss: 2.7362\n",
      "Epoch 353/1000\n",
      "160/160 [==============================] - 0s 199us/step - loss: 2.5664 - val_loss: 2.7308\n",
      "Epoch 354/1000\n",
      "160/160 [==============================] - 0s 198us/step - loss: 2.5595 - val_loss: 2.7273\n",
      "Epoch 355/1000\n",
      "160/160 [==============================] - 0s 218us/step - loss: 2.5585 - val_loss: 2.7231\n",
      "Epoch 356/1000\n",
      "160/160 [==============================] - 0s 197us/step - loss: 2.5576 - val_loss: 2.7198\n",
      "Epoch 357/1000\n",
      "160/160 [==============================] - 0s 204us/step - loss: 2.5533 - val_loss: 2.7176\n",
      "Epoch 358/1000\n",
      "160/160 [==============================] - 0s 210us/step - loss: 2.5490 - val_loss: 2.7136\n",
      "Epoch 359/1000\n",
      "160/160 [==============================] - 0s 222us/step - loss: 2.5470 - val_loss: 2.7093\n",
      "Epoch 360/1000\n",
      "160/160 [==============================] - 0s 207us/step - loss: 2.5447 - val_loss: 2.7044\n",
      "Epoch 361/1000\n",
      "160/160 [==============================] - 0s 206us/step - loss: 2.5432 - val_loss: 2.7001\n",
      "Epoch 362/1000\n",
      "160/160 [==============================] - 0s 193us/step - loss: 2.5409 - val_loss: 2.6951\n",
      "Epoch 363/1000\n",
      "160/160 [==============================] - 0s 243us/step - loss: 2.5396 - val_loss: 2.6904\n",
      "Epoch 364/1000\n",
      "160/160 [==============================] - 0s 229us/step - loss: 2.5375 - val_loss: 2.6848\n",
      "Epoch 365/1000\n",
      "160/160 [==============================] - 0s 248us/step - loss: 2.5324 - val_loss: 2.6809\n",
      "Epoch 366/1000\n",
      "160/160 [==============================] - 0s 202us/step - loss: 2.5302 - val_loss: 2.6750\n",
      "Epoch 367/1000\n",
      "160/160 [==============================] - 0s 208us/step - loss: 2.5306 - val_loss: 2.6710\n",
      "Epoch 368/1000\n",
      "160/160 [==============================] - 0s 217us/step - loss: 2.5233 - val_loss: 2.6672\n",
      "Epoch 369/1000\n",
      "160/160 [==============================] - 0s 181us/step - loss: 2.5213 - val_loss: 2.6638\n",
      "Epoch 370/1000\n",
      "160/160 [==============================] - 0s 218us/step - loss: 2.5260 - val_loss: 2.6625\n",
      "Epoch 371/1000\n",
      "160/160 [==============================] - 0s 193us/step - loss: 2.5206 - val_loss: 2.6555\n",
      "Epoch 372/1000\n",
      "160/160 [==============================] - 0s 188us/step - loss: 2.5152 - val_loss: 2.6508\n",
      "Epoch 373/1000\n",
      "160/160 [==============================] - 0s 198us/step - loss: 2.5130 - val_loss: 2.6467\n",
      "Epoch 374/1000\n",
      "160/160 [==============================] - 0s 204us/step - loss: 2.5102 - val_loss: 2.6399\n",
      "Epoch 375/1000\n",
      "160/160 [==============================] - 0s 220us/step - loss: 2.5097 - val_loss: 2.6349\n",
      "Epoch 376/1000\n",
      "160/160 [==============================] - 0s 230us/step - loss: 2.5130 - val_loss: 2.6294\n",
      "Epoch 377/1000\n",
      "160/160 [==============================] - 0s 248us/step - loss: 2.5101 - val_loss: 2.6238\n",
      "Epoch 378/1000\n",
      "160/160 [==============================] - 0s 238us/step - loss: 2.5001 - val_loss: 2.6184\n",
      "Epoch 379/1000\n",
      "160/160 [==============================] - 0s 173us/step - loss: 2.4981 - val_loss: 2.6125\n",
      "Epoch 380/1000\n",
      "160/160 [==============================] - 0s 195us/step - loss: 2.4966 - val_loss: 2.6056\n",
      "Epoch 381/1000\n",
      "160/160 [==============================] - 0s 182us/step - loss: 2.4914 - val_loss: 2.5989\n",
      "Epoch 382/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 223us/step - loss: 2.4908 - val_loss: 2.5936\n",
      "Epoch 383/1000\n",
      "160/160 [==============================] - 0s 230us/step - loss: 2.4854 - val_loss: 2.5878\n",
      "Epoch 384/1000\n",
      "160/160 [==============================] - 0s 222us/step - loss: 2.4833 - val_loss: 2.5826\n",
      "Epoch 385/1000\n",
      "160/160 [==============================] - 0s 212us/step - loss: 2.4813 - val_loss: 2.5777\n",
      "Epoch 386/1000\n",
      "160/160 [==============================] - 0s 222us/step - loss: 2.4795 - val_loss: 2.5717\n",
      "Epoch 387/1000\n",
      "160/160 [==============================] - 0s 207us/step - loss: 2.4751 - val_loss: 2.5651\n",
      "Epoch 388/1000\n",
      "160/160 [==============================] - 0s 281us/step - loss: 2.4731 - val_loss: 2.5605\n",
      "Epoch 389/1000\n",
      "160/160 [==============================] - 0s 199us/step - loss: 2.4697 - val_loss: 2.5548\n",
      "Epoch 390/1000\n",
      "160/160 [==============================] - 0s 202us/step - loss: 2.4683 - val_loss: 2.5462\n",
      "Epoch 391/1000\n",
      "160/160 [==============================] - 0s 203us/step - loss: 2.4647 - val_loss: 2.5397\n",
      "Epoch 392/1000\n",
      "160/160 [==============================] - 0s 199us/step - loss: 2.4632 - val_loss: 2.5338\n",
      "Epoch 393/1000\n",
      "160/160 [==============================] - 0s 180us/step - loss: 2.4626 - val_loss: 2.5258\n",
      "Epoch 394/1000\n",
      "160/160 [==============================] - 0s 212us/step - loss: 2.4671 - val_loss: 2.5201\n",
      "Epoch 395/1000\n",
      "160/160 [==============================] - 0s 207us/step - loss: 2.4576 - val_loss: 2.5155\n",
      "Epoch 396/1000\n",
      "160/160 [==============================] - 0s 200us/step - loss: 2.4500 - val_loss: 2.5097\n",
      "Epoch 397/1000\n",
      "160/160 [==============================] - 0s 209us/step - loss: 2.4472 - val_loss: 2.5062\n",
      "Epoch 398/1000\n",
      "160/160 [==============================] - 0s 197us/step - loss: 2.4492 - val_loss: 2.5044\n",
      "Epoch 399/1000\n",
      "160/160 [==============================] - 0s 192us/step - loss: 2.4448 - val_loss: 2.4994\n",
      "Epoch 400/1000\n",
      "160/160 [==============================] - 0s 178us/step - loss: 2.4403 - val_loss: 2.4966\n",
      "Epoch 401/1000\n",
      "160/160 [==============================] - 0s 193us/step - loss: 2.4396 - val_loss: 2.4937\n",
      "Epoch 402/1000\n",
      "160/160 [==============================] - 0s 197us/step - loss: 2.4351 - val_loss: 2.4873\n",
      "Epoch 403/1000\n",
      "160/160 [==============================] - 0s 194us/step - loss: 2.4311 - val_loss: 2.4824\n",
      "Epoch 404/1000\n",
      "160/160 [==============================] - 0s 195us/step - loss: 2.4296 - val_loss: 2.4781\n",
      "Epoch 405/1000\n",
      "160/160 [==============================] - 0s 188us/step - loss: 2.4276 - val_loss: 2.4730\n",
      "Epoch 406/1000\n",
      "160/160 [==============================] - 0s 185us/step - loss: 2.4319 - val_loss: 2.4700\n",
      "Epoch 407/1000\n",
      "160/160 [==============================] - 0s 196us/step - loss: 2.4216 - val_loss: 2.4636\n",
      "Epoch 408/1000\n",
      "160/160 [==============================] - 0s 224us/step - loss: 2.4209 - val_loss: 2.4595\n",
      "Epoch 409/1000\n",
      "160/160 [==============================] - 0s 255us/step - loss: 2.4170 - val_loss: 2.4538\n",
      "Epoch 410/1000\n",
      "160/160 [==============================] - 0s 265us/step - loss: 2.4150 - val_loss: 2.4489\n",
      "Epoch 411/1000\n",
      "160/160 [==============================] - 0s 224us/step - loss: 2.4144 - val_loss: 2.4454\n",
      "Epoch 412/1000\n",
      "160/160 [==============================] - 0s 231us/step - loss: 2.4118 - val_loss: 2.4407\n",
      "Epoch 413/1000\n",
      "160/160 [==============================] - 0s 219us/step - loss: 2.4065 - val_loss: 2.4388\n",
      "Epoch 414/1000\n",
      "160/160 [==============================] - 0s 211us/step - loss: 2.4051 - val_loss: 2.4338\n",
      "Epoch 415/1000\n",
      "160/160 [==============================] - 0s 184us/step - loss: 2.4033 - val_loss: 2.4292\n",
      "Epoch 416/1000\n",
      "160/160 [==============================] - 0s 219us/step - loss: 2.3991 - val_loss: 2.4256\n",
      "Epoch 417/1000\n",
      "160/160 [==============================] - 0s 208us/step - loss: 2.3966 - val_loss: 2.4235\n",
      "Epoch 418/1000\n",
      "160/160 [==============================] - 0s 206us/step - loss: 2.3933 - val_loss: 2.4194\n",
      "Epoch 419/1000\n",
      "160/160 [==============================] - 0s 234us/step - loss: 2.3927 - val_loss: 2.4140\n",
      "Epoch 420/1000\n",
      "160/160 [==============================] - 0s 239us/step - loss: 2.3896 - val_loss: 2.4119\n",
      "Epoch 421/1000\n",
      "160/160 [==============================] - 0s 218us/step - loss: 2.3862 - val_loss: 2.4089\n",
      "Epoch 422/1000\n",
      "160/160 [==============================] - 0s 199us/step - loss: 2.3839 - val_loss: 2.4061\n",
      "Epoch 423/1000\n",
      "160/160 [==============================] - 0s 223us/step - loss: 2.3828 - val_loss: 2.4017\n",
      "Epoch 424/1000\n",
      "160/160 [==============================] - 0s 265us/step - loss: 2.3789 - val_loss: 2.3993\n",
      "Epoch 425/1000\n",
      "160/160 [==============================] - 0s 231us/step - loss: 2.3777 - val_loss: 2.3964\n",
      "Epoch 426/1000\n",
      "160/160 [==============================] - 0s 228us/step - loss: 2.3789 - val_loss: 2.3972\n",
      "Epoch 427/1000\n",
      "160/160 [==============================] - 0s 232us/step - loss: 2.3723 - val_loss: 2.3930\n",
      "Epoch 428/1000\n",
      "160/160 [==============================] - 0s 205us/step - loss: 2.3700 - val_loss: 2.3906\n",
      "Epoch 429/1000\n",
      "160/160 [==============================] - 0s 190us/step - loss: 2.3755 - val_loss: 2.3841\n",
      "Epoch 430/1000\n",
      "160/160 [==============================] - 0s 219us/step - loss: 2.3653 - val_loss: 2.3806\n",
      "Epoch 431/1000\n",
      "160/160 [==============================] - 0s 228us/step - loss: 2.3680 - val_loss: 2.3804\n",
      "Epoch 432/1000\n",
      "160/160 [==============================] - 0s 158us/step - loss: 2.3601 - val_loss: 2.3779\n",
      "Epoch 433/1000\n",
      "160/160 [==============================] - 0s 195us/step - loss: 2.3570 - val_loss: 2.3725\n",
      "Epoch 434/1000\n",
      "160/160 [==============================] - 0s 212us/step - loss: 2.3562 - val_loss: 2.3678\n",
      "Epoch 435/1000\n",
      "160/160 [==============================] - 0s 218us/step - loss: 2.3519 - val_loss: 2.3654\n",
      "Epoch 436/1000\n",
      "160/160 [==============================] - 0s 204us/step - loss: 2.3495 - val_loss: 2.3642\n",
      "Epoch 437/1000\n",
      "160/160 [==============================] - 0s 196us/step - loss: 2.3470 - val_loss: 2.3617\n",
      "Epoch 438/1000\n",
      "160/160 [==============================] - 0s 191us/step - loss: 2.3446 - val_loss: 2.3600\n",
      "Epoch 439/1000\n",
      "160/160 [==============================] - 0s 191us/step - loss: 2.3421 - val_loss: 2.3572\n",
      "Epoch 440/1000\n",
      "160/160 [==============================] - 0s 208us/step - loss: 2.3400 - val_loss: 2.3519\n",
      "Epoch 441/1000\n",
      "160/160 [==============================] - 0s 218us/step - loss: 2.3389 - val_loss: 2.3474\n",
      "Epoch 442/1000\n",
      "160/160 [==============================] - 0s 263us/step - loss: 2.3350 - val_loss: 2.3449\n",
      "Epoch 443/1000\n",
      "160/160 [==============================] - 0s 196us/step - loss: 2.3339 - val_loss: 2.3433\n",
      "Epoch 444/1000\n",
      "160/160 [==============================] - 0s 185us/step - loss: 2.3309 - val_loss: 2.3392\n",
      "Epoch 445/1000\n",
      "160/160 [==============================] - 0s 218us/step - loss: 2.3280 - val_loss: 2.3356\n",
      "Epoch 446/1000\n",
      "160/160 [==============================] - 0s 183us/step - loss: 2.3255 - val_loss: 2.3305\n",
      "Epoch 447/1000\n",
      "160/160 [==============================] - 0s 206us/step - loss: 2.3315 - val_loss: 2.3261\n",
      "Epoch 448/1000\n",
      "160/160 [==============================] - 0s 171us/step - loss: 2.3218 - val_loss: 2.3230\n",
      "Epoch 449/1000\n",
      "160/160 [==============================] - 0s 203us/step - loss: 2.3176 - val_loss: 2.3219\n",
      "Epoch 450/1000\n",
      "160/160 [==============================] - 0s 234us/step - loss: 2.3156 - val_loss: 2.3210\n",
      "Epoch 451/1000\n",
      "160/160 [==============================] - 0s 211us/step - loss: 2.3136 - val_loss: 2.3189\n",
      "Epoch 452/1000\n",
      "160/160 [==============================] - 0s 172us/step - loss: 2.3119 - val_loss: 2.3142\n",
      "Epoch 453/1000\n",
      "160/160 [==============================] - 0s 190us/step - loss: 2.3119 - val_loss: 2.3132\n",
      "Epoch 454/1000\n",
      "160/160 [==============================] - 0s 208us/step - loss: 2.3062 - val_loss: 2.3093\n",
      "Epoch 455/1000\n",
      "160/160 [==============================] - 0s 184us/step - loss: 2.3037 - val_loss: 2.3061\n",
      "Epoch 456/1000\n",
      "160/160 [==============================] - 0s 201us/step - loss: 2.3023 - val_loss: 2.3045\n",
      "Epoch 457/1000\n",
      "160/160 [==============================] - 0s 223us/step - loss: 2.2997 - val_loss: 2.3022\n",
      "Epoch 458/1000\n",
      "160/160 [==============================] - 0s 205us/step - loss: 2.2988 - val_loss: 2.3021\n",
      "Epoch 459/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 207us/step - loss: 2.2953 - val_loss: 2.2970\n",
      "Epoch 460/1000\n",
      "160/160 [==============================] - 0s 190us/step - loss: 2.2920 - val_loss: 2.2935\n",
      "Epoch 461/1000\n",
      "160/160 [==============================] - 0s 203us/step - loss: 2.2904 - val_loss: 2.2888\n",
      "Epoch 462/1000\n",
      "160/160 [==============================] - 0s 230us/step - loss: 2.2874 - val_loss: 2.2856\n",
      "Epoch 463/1000\n",
      "160/160 [==============================] - 0s 203us/step - loss: 2.2850 - val_loss: 2.2820\n",
      "Epoch 464/1000\n",
      "160/160 [==============================] - 0s 197us/step - loss: 2.2847 - val_loss: 2.2790\n",
      "Epoch 465/1000\n",
      "160/160 [==============================] - 0s 195us/step - loss: 2.2803 - val_loss: 2.2744\n",
      "Epoch 466/1000\n",
      "160/160 [==============================] - 0s 198us/step - loss: 2.2786 - val_loss: 2.2718\n",
      "Epoch 467/1000\n",
      "160/160 [==============================] - 0s 170us/step - loss: 2.2769 - val_loss: 2.2661\n",
      "Epoch 468/1000\n",
      "160/160 [==============================] - 0s 190us/step - loss: 2.2738 - val_loss: 2.2626\n",
      "Epoch 469/1000\n",
      "160/160 [==============================] - 0s 201us/step - loss: 2.2707 - val_loss: 2.2606\n",
      "Epoch 470/1000\n",
      "160/160 [==============================] - 0s 192us/step - loss: 2.2680 - val_loss: 2.2591\n",
      "Epoch 471/1000\n",
      "160/160 [==============================] - 0s 167us/step - loss: 2.2661 - val_loss: 2.2551\n",
      "Epoch 472/1000\n",
      "160/160 [==============================] - 0s 204us/step - loss: 2.2655 - val_loss: 2.2530\n",
      "Epoch 473/1000\n",
      "160/160 [==============================] - 0s 233us/step - loss: 2.2615 - val_loss: 2.2465\n",
      "Epoch 474/1000\n",
      "160/160 [==============================] - 0s 162us/step - loss: 2.2589 - val_loss: 2.2424\n",
      "Epoch 475/1000\n",
      "160/160 [==============================] - 0s 176us/step - loss: 2.2579 - val_loss: 2.2391\n",
      "Epoch 476/1000\n",
      "160/160 [==============================] - 0s 166us/step - loss: 2.2548 - val_loss: 2.2377\n",
      "Epoch 477/1000\n",
      "160/160 [==============================] - 0s 177us/step - loss: 2.2519 - val_loss: 2.2383\n",
      "Epoch 478/1000\n",
      "160/160 [==============================] - 0s 203us/step - loss: 2.2492 - val_loss: 2.2375\n",
      "Epoch 479/1000\n",
      "160/160 [==============================] - 0s 221us/step - loss: 2.2473 - val_loss: 2.2344\n",
      "Epoch 480/1000\n",
      "160/160 [==============================] - 0s 202us/step - loss: 2.2456 - val_loss: 2.2321\n",
      "Epoch 481/1000\n",
      "160/160 [==============================] - 0s 237us/step - loss: 2.2413 - val_loss: 2.2248\n",
      "Epoch 482/1000\n",
      "160/160 [==============================] - 0s 193us/step - loss: 2.2395 - val_loss: 2.2184\n",
      "Epoch 483/1000\n",
      "160/160 [==============================] - 0s 185us/step - loss: 2.2437 - val_loss: 2.2131\n",
      "Epoch 484/1000\n",
      "160/160 [==============================] - 0s 857us/step - loss: 2.2329 - val_loss: 2.2099\n",
      "Epoch 485/1000\n",
      "160/160 [==============================] - 0s 194us/step - loss: 2.2346 - val_loss: 2.2127\n",
      "Epoch 486/1000\n",
      "160/160 [==============================] - 0s 216us/step - loss: 2.2317 - val_loss: 2.2026\n",
      "Epoch 487/1000\n",
      "160/160 [==============================] - 0s 208us/step - loss: 2.2244 - val_loss: 2.1983\n",
      "Epoch 488/1000\n",
      "160/160 [==============================] - 0s 214us/step - loss: 2.2219 - val_loss: 2.1926\n",
      "Epoch 489/1000\n",
      "160/160 [==============================] - 0s 233us/step - loss: 2.2213 - val_loss: 2.1866\n",
      "Epoch 490/1000\n",
      "160/160 [==============================] - 0s 201us/step - loss: 2.2169 - val_loss: 2.1829\n",
      "Epoch 491/1000\n",
      "160/160 [==============================] - 0s 215us/step - loss: 2.2132 - val_loss: 2.1799\n",
      "Epoch 492/1000\n",
      "160/160 [==============================] - 0s 190us/step - loss: 2.2090 - val_loss: 2.1761\n",
      "Epoch 493/1000\n",
      "160/160 [==============================] - 0s 188us/step - loss: 2.2080 - val_loss: 2.1717\n",
      "Epoch 494/1000\n",
      "160/160 [==============================] - 0s 208us/step - loss: 2.2049 - val_loss: 2.1655\n",
      "Epoch 495/1000\n",
      "160/160 [==============================] - 0s 224us/step - loss: 2.2029 - val_loss: 2.1580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/Keras-2.2.4-py3.5.egg/keras/callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.102107). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 496/1000\n",
      "160/160 [==============================] - 0s 239us/step - loss: 2.2023 - val_loss: 2.1554\n",
      "Epoch 497/1000\n",
      "160/160 [==============================] - 0s 222us/step - loss: 2.2038 - val_loss: 2.1561\n",
      "Epoch 498/1000\n",
      "160/160 [==============================] - 0s 213us/step - loss: 2.1950 - val_loss: 2.1553\n",
      "Epoch 499/1000\n",
      "160/160 [==============================] - 0s 206us/step - loss: 2.1917 - val_loss: 2.1496\n",
      "Epoch 500/1000\n",
      "160/160 [==============================] - 0s 222us/step - loss: 2.1919 - val_loss: 2.1453\n",
      "Epoch 501/1000\n",
      "160/160 [==============================] - 0s 215us/step - loss: 2.1863 - val_loss: 2.1398\n",
      "Epoch 502/1000\n",
      "160/160 [==============================] - 0s 211us/step - loss: 2.1835 - val_loss: 2.1352\n",
      "Epoch 503/1000\n",
      "160/160 [==============================] - 0s 202us/step - loss: 2.1803 - val_loss: 2.1362\n",
      "Epoch 504/1000\n",
      "160/160 [==============================] - 0s 237us/step - loss: 2.1750 - val_loss: 2.1325\n",
      "Epoch 505/1000\n",
      "160/160 [==============================] - 0s 192us/step - loss: 2.1730 - val_loss: 2.1255\n",
      "Epoch 506/1000\n",
      "160/160 [==============================] - 0s 221us/step - loss: 2.1692 - val_loss: 2.1192\n",
      "Epoch 507/1000\n",
      "160/160 [==============================] - 0s 216us/step - loss: 2.1663 - val_loss: 2.1133\n",
      "Epoch 508/1000\n",
      "160/160 [==============================] - 0s 199us/step - loss: 2.1638 - val_loss: 2.1096\n",
      "Epoch 509/1000\n",
      "160/160 [==============================] - 0s 178us/step - loss: 2.1609 - val_loss: 2.1072\n",
      "Epoch 510/1000\n",
      "160/160 [==============================] - 0s 232us/step - loss: 2.1639 - val_loss: 2.1020\n",
      "Epoch 511/1000\n",
      "160/160 [==============================] - 0s 199us/step - loss: 2.1553 - val_loss: 2.1013\n",
      "Epoch 512/1000\n",
      "160/160 [==============================] - 0s 214us/step - loss: 2.1521 - val_loss: 2.0990\n",
      "Epoch 513/1000\n",
      "160/160 [==============================] - 0s 197us/step - loss: 2.1499 - val_loss: 2.0964\n",
      "Epoch 514/1000\n",
      "160/160 [==============================] - 0s 195us/step - loss: 2.1472 - val_loss: 2.0952\n",
      "Epoch 515/1000\n",
      "160/160 [==============================] - 0s 219us/step - loss: 2.1493 - val_loss: 2.0885\n",
      "Epoch 516/1000\n",
      "160/160 [==============================] - 0s 182us/step - loss: 2.1416 - val_loss: 2.0866\n",
      "Epoch 517/1000\n",
      "160/160 [==============================] - 0s 210us/step - loss: 2.1379 - val_loss: 2.0859\n",
      "Epoch 518/1000\n",
      "160/160 [==============================] - 0s 255us/step - loss: 2.1351 - val_loss: 2.0835\n",
      "Epoch 519/1000\n",
      "160/160 [==============================] - 0s 199us/step - loss: 2.1331 - val_loss: 2.0784\n",
      "Epoch 520/1000\n",
      "160/160 [==============================] - 0s 191us/step - loss: 2.1292 - val_loss: 2.0751\n",
      "Epoch 521/1000\n",
      "160/160 [==============================] - 0s 194us/step - loss: 2.1248 - val_loss: 2.0711\n",
      "Epoch 522/1000\n",
      "160/160 [==============================] - 0s 207us/step - loss: 2.1236 - val_loss: 2.0694\n",
      "Epoch 523/1000\n",
      "160/160 [==============================] - 0s 180us/step - loss: 2.1157 - val_loss: 2.0629\n",
      "Epoch 524/1000\n",
      "160/160 [==============================] - ETA: 0s - loss: 2.074 - 0s 189us/step - loss: 2.1174 - val_loss: 2.0603\n",
      "Epoch 525/1000\n",
      "160/160 [==============================] - 0s 185us/step - loss: 2.1097 - val_loss: 2.0573\n",
      "Epoch 526/1000\n",
      "160/160 [==============================] - 0s 198us/step - loss: 2.1108 - val_loss: 2.0592\n",
      "Epoch 527/1000\n",
      "160/160 [==============================] - 0s 219us/step - loss: 2.1032 - val_loss: 2.0506\n",
      "Epoch 528/1000\n",
      "160/160 [==============================] - 0s 203us/step - loss: 2.0964 - val_loss: 2.0444\n",
      "Epoch 529/1000\n",
      "160/160 [==============================] - 0s 201us/step - loss: 2.0916 - val_loss: 2.0408\n",
      "Epoch 530/1000\n",
      "160/160 [==============================] - 0s 207us/step - loss: 2.0889 - val_loss: 2.0351\n",
      "Epoch 531/1000\n",
      "160/160 [==============================] - 0s 211us/step - loss: 2.0849 - val_loss: 2.0324\n",
      "Epoch 532/1000\n",
      "160/160 [==============================] - 0s 216us/step - loss: 2.0825 - val_loss: 2.0306\n",
      "Epoch 533/1000\n",
      "160/160 [==============================] - 0s 215us/step - loss: 2.0793 - val_loss: 2.0269\n",
      "Epoch 534/1000\n",
      "160/160 [==============================] - 0s 225us/step - loss: 2.0729 - val_loss: 2.0222\n",
      "Epoch 535/1000\n",
      "160/160 [==============================] - 0s 228us/step - loss: 2.0730 - val_loss: 2.0177\n",
      "Epoch 536/1000\n",
      "160/160 [==============================] - 0s 229us/step - loss: 2.0670 - val_loss: 2.0170\n",
      "Epoch 537/1000\n",
      "160/160 [==============================] - 0s 254us/step - loss: 2.0622 - val_loss: 2.0186\n",
      "Epoch 538/1000\n",
      "160/160 [==============================] - 0s 240us/step - loss: 2.0600 - val_loss: 2.0195\n",
      "Epoch 539/1000\n",
      "160/160 [==============================] - 0s 303us/step - loss: 2.0551 - val_loss: 2.0128\n",
      "Epoch 540/1000\n",
      "160/160 [==============================] - 0s 247us/step - loss: 2.0503 - val_loss: 2.0108\n",
      "Epoch 541/1000\n",
      "160/160 [==============================] - 0s 233us/step - loss: 2.0525 - val_loss: 2.0059\n",
      "Epoch 542/1000\n",
      "160/160 [==============================] - 0s 207us/step - loss: 2.0412 - val_loss: 2.0009\n",
      "Epoch 543/1000\n",
      "160/160 [==============================] - 0s 199us/step - loss: 2.0350 - val_loss: 1.9992\n",
      "Epoch 544/1000\n",
      "160/160 [==============================] - 0s 196us/step - loss: 2.0309 - val_loss: 1.9968\n",
      "Epoch 545/1000\n",
      "160/160 [==============================] - 0s 189us/step - loss: 2.0278 - val_loss: 1.9926\n",
      "Epoch 546/1000\n",
      "160/160 [==============================] - 0s 537us/step - loss: 2.0251 - val_loss: 1.9863\n",
      "Epoch 547/1000\n",
      "160/160 [==============================] - 0s 180us/step - loss: 2.0197 - val_loss: 1.9796\n",
      "Epoch 548/1000\n",
      "160/160 [==============================] - 0s 181us/step - loss: 2.0199 - val_loss: 1.9767\n",
      "Epoch 549/1000\n",
      "160/160 [==============================] - 0s 174us/step - loss: 2.0125 - val_loss: 1.9701\n",
      "Epoch 550/1000\n",
      "160/160 [==============================] - ETA: 0s - loss: 1.567 - 0s 194us/step - loss: 2.0082 - val_loss: 1.9659\n",
      "Epoch 551/1000\n",
      "160/160 [==============================] - 0s 196us/step - loss: 2.0077 - val_loss: 1.9655\n",
      "Epoch 552/1000\n",
      "160/160 [==============================] - 0s 181us/step - loss: 1.9995 - val_loss: 1.9606\n",
      "Epoch 553/1000\n",
      "160/160 [==============================] - 0s 184us/step - loss: 1.9946 - val_loss: 1.9601\n",
      "Epoch 554/1000\n",
      "160/160 [==============================] - 0s 206us/step - loss: 1.9881 - val_loss: 1.9590\n",
      "Epoch 555/1000\n",
      "160/160 [==============================] - 0s 204us/step - loss: 1.9827 - val_loss: 1.9574\n",
      "Epoch 556/1000\n",
      "160/160 [==============================] - 0s 211us/step - loss: 1.9785 - val_loss: 1.9599\n",
      "Epoch 557/1000\n",
      "160/160 [==============================] - 0s 193us/step - loss: 1.9704 - val_loss: 1.9597\n",
      "Epoch 558/1000\n",
      "160/160 [==============================] - 0s 212us/step - loss: 1.9678 - val_loss: 1.9563\n",
      "Epoch 559/1000\n",
      "160/160 [==============================] - 0s 202us/step - loss: 1.9606 - val_loss: 1.9531\n",
      "Epoch 560/1000\n",
      "160/160 [==============================] - 0s 198us/step - loss: 1.9556 - val_loss: 1.9476\n",
      "Epoch 561/1000\n",
      "160/160 [==============================] - 0s 199us/step - loss: 1.9501 - val_loss: 1.9400\n",
      "Epoch 562/1000\n",
      "160/160 [==============================] - 0s 195us/step - loss: 1.9489 - val_loss: 1.9315\n",
      "Epoch 563/1000\n",
      "160/160 [==============================] - 0s 199us/step - loss: 1.9486 - val_loss: 1.9264\n",
      "Epoch 564/1000\n",
      "160/160 [==============================] - 0s 195us/step - loss: 1.9406 - val_loss: 1.9177\n",
      "Epoch 565/1000\n",
      "160/160 [==============================] - 0s 197us/step - loss: 1.9355 - val_loss: 1.9131\n",
      "Epoch 566/1000\n",
      "160/160 [==============================] - 0s 210us/step - loss: 1.9323 - val_loss: 1.9120\n",
      "Epoch 567/1000\n",
      "160/160 [==============================] - 0s 213us/step - loss: 1.9337 - val_loss: 1.9092\n",
      "Epoch 568/1000\n",
      "160/160 [==============================] - 0s 220us/step - loss: 1.9256 - val_loss: 1.9086\n",
      "Epoch 569/1000\n",
      "160/160 [==============================] - 0s 213us/step - loss: 1.9219 - val_loss: 1.9057\n",
      "Epoch 570/1000\n",
      "160/160 [==============================] - 0s 209us/step - loss: 1.9185 - val_loss: 1.9026\n",
      "Epoch 571/1000\n",
      "160/160 [==============================] - 0s 226us/step - loss: 1.9161 - val_loss: 1.9026\n",
      "Epoch 572/1000\n",
      "160/160 [==============================] - 0s 232us/step - loss: 1.9132 - val_loss: 1.8996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 573/1000\n",
      "160/160 [==============================] - 0s 185us/step - loss: 1.9123 - val_loss: 1.8957\n",
      "Epoch 574/1000\n",
      "160/160 [==============================] - 0s 200us/step - loss: 1.9035 - val_loss: 1.8923\n",
      "Epoch 575/1000\n",
      "160/160 [==============================] - 0s 195us/step - loss: 1.9018 - val_loss: 1.8842\n",
      "Epoch 576/1000\n",
      "160/160 [==============================] - 0s 190us/step - loss: 1.8960 - val_loss: 1.8797\n",
      "Epoch 577/1000\n",
      "160/160 [==============================] - 0s 203us/step - loss: 1.8956 - val_loss: 1.8761\n",
      "Epoch 578/1000\n",
      "160/160 [==============================] - 0s 194us/step - loss: 1.8923 - val_loss: 1.8723\n",
      "Epoch 579/1000\n",
      "160/160 [==============================] - 0s 218us/step - loss: 1.8872 - val_loss: 1.8682\n",
      "Epoch 580/1000\n",
      "160/160 [==============================] - 0s 250us/step - loss: 1.8839 - val_loss: 1.8652\n",
      "Epoch 581/1000\n",
      "160/160 [==============================] - 0s 219us/step - loss: 1.8827 - val_loss: 1.8616\n",
      "Epoch 582/1000\n",
      "160/160 [==============================] - 0s 216us/step - loss: 1.8783 - val_loss: 1.8583\n",
      "Epoch 583/1000\n",
      "160/160 [==============================] - 0s 219us/step - loss: 1.8744 - val_loss: 1.8541\n",
      "Epoch 584/1000\n",
      "160/160 [==============================] - 0s 219us/step - loss: 1.8714 - val_loss: 1.8488\n",
      "Epoch 585/1000\n",
      "160/160 [==============================] - 0s 221us/step - loss: 1.8712 - val_loss: 1.8450\n",
      "Epoch 586/1000\n",
      "160/160 [==============================] - 0s 223us/step - loss: 1.8670 - val_loss: 1.8404\n",
      "Epoch 587/1000\n",
      "160/160 [==============================] - 0s 208us/step - loss: 1.8638 - val_loss: 1.8376\n",
      "Epoch 588/1000\n",
      "160/160 [==============================] - 0s 196us/step - loss: 1.8594 - val_loss: 1.8327\n",
      "Epoch 589/1000\n",
      "160/160 [==============================] - 0s 213us/step - loss: 1.8567 - val_loss: 1.8304\n",
      "Epoch 590/1000\n",
      "160/160 [==============================] - 0s 217us/step - loss: 1.8537 - val_loss: 1.8261\n",
      "Epoch 591/1000\n",
      "160/160 [==============================] - 0s 206us/step - loss: 1.8522 - val_loss: 1.8225\n",
      "Epoch 592/1000\n",
      "160/160 [==============================] - 0s 228us/step - loss: 1.8478 - val_loss: 1.8204\n",
      "Epoch 593/1000\n",
      "160/160 [==============================] - 0s 206us/step - loss: 1.8444 - val_loss: 1.8163\n",
      "Epoch 594/1000\n",
      "160/160 [==============================] - 0s 220us/step - loss: 1.8415 - val_loss: 1.8131\n",
      "Epoch 595/1000\n",
      "160/160 [==============================] - 0s 208us/step - loss: 1.8379 - val_loss: 1.8105\n",
      "Epoch 596/1000\n",
      "160/160 [==============================] - 0s 208us/step - loss: 1.8358 - val_loss: 1.8066\n",
      "Epoch 597/1000\n",
      "160/160 [==============================] - 0s 188us/step - loss: 1.8330 - val_loss: 1.8042\n",
      "Epoch 598/1000\n",
      "160/160 [==============================] - 0s 208us/step - loss: 1.8297 - val_loss: 1.8001\n",
      "Epoch 599/1000\n",
      "160/160 [==============================] - 0s 195us/step - loss: 1.8297 - val_loss: 1.7982\n",
      "Epoch 600/1000\n",
      "160/160 [==============================] - 0s 195us/step - loss: 1.8237 - val_loss: 1.7949\n",
      "Epoch 601/1000\n",
      "160/160 [==============================] - 0s 195us/step - loss: 1.8197 - val_loss: 1.7933\n",
      "Epoch 602/1000\n",
      "160/160 [==============================] - 0s 198us/step - loss: 1.8183 - val_loss: 1.7922\n",
      "Epoch 603/1000\n",
      "160/160 [==============================] - 0s 203us/step - loss: 1.8173 - val_loss: 1.7876\n",
      "Epoch 604/1000\n",
      "160/160 [==============================] - 0s 196us/step - loss: 1.8111 - val_loss: 1.7831\n",
      "Epoch 605/1000\n",
      "160/160 [==============================] - 0s 215us/step - loss: 1.8116 - val_loss: 1.7819\n",
      "Epoch 606/1000\n",
      "160/160 [==============================] - 0s 193us/step - loss: 1.8101 - val_loss: 1.7737\n",
      "Epoch 607/1000\n",
      "160/160 [==============================] - 0s 195us/step - loss: 1.8138 - val_loss: 1.7676\n",
      "Epoch 608/1000\n",
      "160/160 [==============================] - 0s 464us/step - loss: 1.8005 - val_loss: 1.7631\n",
      "Epoch 609/1000\n",
      "160/160 [==============================] - 0s 277us/step - loss: 1.7957 - val_loss: 1.7611\n",
      "Epoch 610/1000\n",
      "160/160 [==============================] - 0s 182us/step - loss: 1.7954 - val_loss: 1.7583\n",
      "Epoch 611/1000\n",
      "160/160 [==============================] - 0s 201us/step - loss: 1.7903 - val_loss: 1.7554\n",
      "Epoch 612/1000\n",
      "160/160 [==============================] - 0s 195us/step - loss: 1.7878 - val_loss: 1.7519\n",
      "Epoch 613/1000\n",
      "160/160 [==============================] - 0s 200us/step - loss: 1.7841 - val_loss: 1.7473\n",
      "Epoch 614/1000\n",
      "160/160 [==============================] - 0s 185us/step - loss: 1.7852 - val_loss: 1.7432\n",
      "Epoch 615/1000\n",
      "160/160 [==============================] - 0s 219us/step - loss: 1.7798 - val_loss: 1.7387\n",
      "Epoch 616/1000\n",
      "160/160 [==============================] - 0s 178us/step - loss: 1.7759 - val_loss: 1.7340\n",
      "Epoch 617/1000\n",
      "160/160 [==============================] - 0s 201us/step - loss: 1.7733 - val_loss: 1.7294\n",
      "Epoch 618/1000\n",
      "160/160 [==============================] - 0s 200us/step - loss: 1.7758 - val_loss: 1.7266\n",
      "Epoch 619/1000\n",
      "160/160 [==============================] - 0s 217us/step - loss: 1.7687 - val_loss: 1.7234\n",
      "Epoch 620/1000\n",
      "160/160 [==============================] - 0s 183us/step - loss: 1.7657 - val_loss: 1.7218\n",
      "Epoch 621/1000\n",
      "160/160 [==============================] - 0s 196us/step - loss: 1.7636 - val_loss: 1.7177\n",
      "Epoch 622/1000\n",
      "160/160 [==============================] - 0s 217us/step - loss: 1.7595 - val_loss: 1.7158\n",
      "Epoch 623/1000\n",
      "160/160 [==============================] - 0s 235us/step - loss: 1.7562 - val_loss: 1.7122\n",
      "Epoch 624/1000\n",
      "160/160 [==============================] - 0s 217us/step - loss: 1.7543 - val_loss: 1.7088\n",
      "Epoch 625/1000\n",
      "160/160 [==============================] - 0s 216us/step - loss: 1.7531 - val_loss: 1.7042\n",
      "Epoch 626/1000\n",
      "160/160 [==============================] - 0s 216us/step - loss: 1.7483 - val_loss: 1.7000\n",
      "Epoch 627/1000\n",
      "160/160 [==============================] - 0s 210us/step - loss: 1.7456 - val_loss: 1.6971\n",
      "Epoch 628/1000\n",
      "160/160 [==============================] - 0s 233us/step - loss: 1.7452 - val_loss: 1.6934\n",
      "Epoch 629/1000\n",
      "160/160 [==============================] - 0s 187us/step - loss: 1.7407 - val_loss: 1.6909\n",
      "Epoch 630/1000\n",
      "160/160 [==============================] - 0s 201us/step - loss: 1.7371 - val_loss: 1.6874\n",
      "Epoch 631/1000\n",
      "160/160 [==============================] - 0s 215us/step - loss: 1.7359 - val_loss: 1.6856\n",
      "Epoch 632/1000\n",
      "160/160 [==============================] - 0s 201us/step - loss: 1.7329 - val_loss: 1.6809\n",
      "Epoch 633/1000\n",
      "160/160 [==============================] - 0s 224us/step - loss: 1.7295 - val_loss: 1.6773\n",
      "Epoch 634/1000\n",
      "160/160 [==============================] - 0s 210us/step - loss: 1.7277 - val_loss: 1.6747\n",
      "Epoch 635/1000\n",
      "160/160 [==============================] - 0s 203us/step - loss: 1.7375 - val_loss: 1.6702\n",
      "Epoch 636/1000\n",
      "160/160 [==============================] - 0s 189us/step - loss: 1.7217 - val_loss: 1.6658\n",
      "Epoch 637/1000\n",
      "160/160 [==============================] - 0s 185us/step - loss: 1.7197 - val_loss: 1.6683\n",
      "Epoch 638/1000\n",
      "160/160 [==============================] - 0s 208us/step - loss: 1.7215 - val_loss: 1.6640\n",
      "Epoch 639/1000\n",
      "160/160 [==============================] - 0s 215us/step - loss: 1.7145 - val_loss: 1.6567\n",
      "Epoch 640/1000\n",
      "160/160 [==============================] - 0s 186us/step - loss: 1.7205 - val_loss: 1.6592\n",
      "Epoch 641/1000\n",
      "160/160 [==============================] - 0s 180us/step - loss: 1.7145 - val_loss: 1.6515\n",
      "Epoch 642/1000\n",
      "160/160 [==============================] - 0s 171us/step - loss: 1.7067 - val_loss: 1.6499\n",
      "Epoch 643/1000\n",
      "160/160 [==============================] - 0s 210us/step - loss: 1.7026 - val_loss: 1.6508\n",
      "Epoch 644/1000\n",
      "160/160 [==============================] - 0s 203us/step - loss: 1.7037 - val_loss: 1.6461\n",
      "Epoch 645/1000\n",
      "160/160 [==============================] - 0s 206us/step - loss: 1.6986 - val_loss: 1.6399\n",
      "Epoch 646/1000\n",
      "160/160 [==============================] - 0s 183us/step - loss: 1.7001 - val_loss: 1.6376\n",
      "Epoch 647/1000\n",
      "160/160 [==============================] - 0s 201us/step - loss: 1.6992 - val_loss: 1.6353\n",
      "Epoch 648/1000\n",
      "160/160 [==============================] - 0s 202us/step - loss: 1.6914 - val_loss: 1.6301\n",
      "Epoch 649/1000\n",
      "160/160 [==============================] - 0s 195us/step - loss: 1.6867 - val_loss: 1.6315\n",
      "Epoch 650/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 177us/step - loss: 1.6873 - val_loss: 1.6281\n",
      "Epoch 651/1000\n",
      "160/160 [==============================] - 0s 192us/step - loss: 1.6845 - val_loss: 1.6209\n",
      "Epoch 652/1000\n",
      "160/160 [==============================] - 0s 174us/step - loss: 1.6794 - val_loss: 1.6163\n",
      "Epoch 653/1000\n",
      "160/160 [==============================] - 0s 175us/step - loss: 1.6819 - val_loss: 1.6141\n",
      "Epoch 654/1000\n",
      "160/160 [==============================] - 0s 198us/step - loss: 1.6765 - val_loss: 1.6106\n",
      "Epoch 655/1000\n",
      "160/160 [==============================] - 0s 187us/step - loss: 1.6712 - val_loss: 1.6100\n",
      "Epoch 656/1000\n",
      "160/160 [==============================] - 0s 183us/step - loss: 1.6702 - val_loss: 1.6081\n",
      "Epoch 657/1000\n",
      "160/160 [==============================] - 0s 194us/step - loss: 1.6676 - val_loss: 1.6047\n",
      "Epoch 658/1000\n",
      "160/160 [==============================] - 0s 193us/step - loss: 1.6638 - val_loss: 1.6000\n",
      "Epoch 659/1000\n",
      "160/160 [==============================] - 0s 199us/step - loss: 1.6666 - val_loss: 1.5974\n",
      "Epoch 660/1000\n",
      "160/160 [==============================] - 0s 206us/step - loss: 1.6606 - val_loss: 1.5921\n",
      "Epoch 661/1000\n",
      "160/160 [==============================] - 0s 185us/step - loss: 1.6560 - val_loss: 1.5903\n",
      "Epoch 662/1000\n",
      "160/160 [==============================] - 0s 204us/step - loss: 1.6570 - val_loss: 1.5853\n",
      "Epoch 663/1000\n",
      "160/160 [==============================] - 0s 203us/step - loss: 1.6515 - val_loss: 1.5833\n",
      "Epoch 664/1000\n",
      "160/160 [==============================] - 0s 190us/step - loss: 1.6500 - val_loss: 1.5789\n",
      "Epoch 665/1000\n",
      "160/160 [==============================] - 0s 197us/step - loss: 1.6483 - val_loss: 1.5771\n",
      "Epoch 666/1000\n",
      "160/160 [==============================] - 0s 173us/step - loss: 1.6442 - val_loss: 1.5764\n",
      "Epoch 667/1000\n",
      "160/160 [==============================] - 0s 184us/step - loss: 1.6407 - val_loss: 1.5745\n",
      "Epoch 668/1000\n",
      "160/160 [==============================] - 0s 218us/step - loss: 1.6409 - val_loss: 1.5720\n",
      "Epoch 669/1000\n",
      "160/160 [==============================] - 0s 222us/step - loss: 1.6372 - val_loss: 1.5669\n",
      "Epoch 670/1000\n",
      "160/160 [==============================] - 0s 324us/step - loss: 1.6341 - val_loss: 1.5620\n",
      "Epoch 671/1000\n",
      "160/160 [==============================] - 0s 213us/step - loss: 1.6343 - val_loss: 1.5575\n",
      "Epoch 672/1000\n",
      "160/160 [==============================] - 0s 183us/step - loss: 1.6350 - val_loss: 1.5560\n",
      "Epoch 673/1000\n",
      "160/160 [==============================] - 0s 194us/step - loss: 1.6289 - val_loss: 1.5583\n",
      "Epoch 674/1000\n",
      "160/160 [==============================] - 0s 186us/step - loss: 1.6261 - val_loss: 1.5576\n",
      "Epoch 675/1000\n",
      "160/160 [==============================] - 0s 226us/step - loss: 1.6285 - val_loss: 1.5486\n",
      "Epoch 676/1000\n",
      "160/160 [==============================] - 0s 216us/step - loss: 1.6183 - val_loss: 1.5460\n",
      "Epoch 677/1000\n",
      "160/160 [==============================] - 0s 226us/step - loss: 1.6164 - val_loss: 1.5446\n",
      "Epoch 678/1000\n",
      "160/160 [==============================] - 0s 208us/step - loss: 1.6162 - val_loss: 1.5437\n",
      "Epoch 679/1000\n",
      "160/160 [==============================] - 0s 233us/step - loss: 1.6117 - val_loss: 1.5394\n",
      "Epoch 680/1000\n",
      "160/160 [==============================] - 0s 191us/step - loss: 1.6108 - val_loss: 1.5366\n",
      "Epoch 681/1000\n",
      "160/160 [==============================] - 0s 208us/step - loss: 1.6069 - val_loss: 1.5329\n",
      "Epoch 682/1000\n",
      "160/160 [==============================] - 0s 216us/step - loss: 1.6039 - val_loss: 1.5314\n",
      "Epoch 683/1000\n",
      "160/160 [==============================] - 0s 201us/step - loss: 1.6016 - val_loss: 1.5300\n",
      "Epoch 684/1000\n",
      "160/160 [==============================] - 0s 193us/step - loss: 1.6014 - val_loss: 1.5260\n",
      "Epoch 685/1000\n",
      "160/160 [==============================] - 0s 200us/step - loss: 1.5986 - val_loss: 1.5183\n",
      "Epoch 686/1000\n",
      "160/160 [==============================] - 0s 203us/step - loss: 1.5951 - val_loss: 1.5169\n",
      "Epoch 687/1000\n",
      "160/160 [==============================] - 0s 205us/step - loss: 1.6038 - val_loss: 1.5157\n",
      "Epoch 688/1000\n",
      "160/160 [==============================] - 0s 208us/step - loss: 1.5894 - val_loss: 1.5108\n",
      "Epoch 689/1000\n",
      "160/160 [==============================] - 0s 249us/step - loss: 1.5873 - val_loss: 1.5085\n",
      "Epoch 690/1000\n",
      "160/160 [==============================] - 0s 235us/step - loss: 1.5871 - val_loss: 1.5073\n",
      "Epoch 691/1000\n",
      "160/160 [==============================] - 0s 264us/step - loss: 1.5816 - val_loss: 1.5044\n",
      "Epoch 692/1000\n",
      "160/160 [==============================] - 0s 222us/step - loss: 1.5823 - val_loss: 1.5033\n",
      "Epoch 693/1000\n",
      "160/160 [==============================] - 0s 224us/step - loss: 1.5764 - val_loss: 1.4999\n",
      "Epoch 694/1000\n",
      "160/160 [==============================] - 0s 208us/step - loss: 1.5798 - val_loss: 1.4971\n",
      "Epoch 695/1000\n",
      "160/160 [==============================] - 0s 222us/step - loss: 1.5738 - val_loss: 1.4950\n",
      "Epoch 696/1000\n",
      "160/160 [==============================] - 0s 227us/step - loss: 1.5781 - val_loss: 1.4962\n",
      "Epoch 697/1000\n",
      "160/160 [==============================] - 0s 236us/step - loss: 1.5695 - val_loss: 1.4897\n",
      "Epoch 698/1000\n",
      "160/160 [==============================] - 0s 242us/step - loss: 1.5648 - val_loss: 1.4855\n",
      "Epoch 699/1000\n",
      "160/160 [==============================] - 0s 266us/step - loss: 1.5625 - val_loss: 1.4835\n",
      "Epoch 700/1000\n",
      "160/160 [==============================] - 0s 239us/step - loss: 1.5651 - val_loss: 1.4801\n",
      "Epoch 701/1000\n",
      "160/160 [==============================] - 0s 255us/step - loss: 1.5584 - val_loss: 1.4771\n",
      "Epoch 702/1000\n",
      "160/160 [==============================] - 0s 230us/step - loss: 1.5602 - val_loss: 1.4787\n",
      "Epoch 703/1000\n",
      "160/160 [==============================] - 0s 211us/step - loss: 1.5563 - val_loss: 1.4735\n",
      "Epoch 704/1000\n",
      "160/160 [==============================] - 0s 240us/step - loss: 1.5520 - val_loss: 1.4696\n",
      "Epoch 705/1000\n",
      "160/160 [==============================] - 0s 209us/step - loss: 1.5498 - val_loss: 1.4665\n",
      "Epoch 706/1000\n",
      "160/160 [==============================] - 0s 222us/step - loss: 1.5473 - val_loss: 1.4643\n",
      "Epoch 707/1000\n",
      "160/160 [==============================] - 0s 240us/step - loss: 1.5442 - val_loss: 1.4621\n",
      "Epoch 708/1000\n",
      "160/160 [==============================] - 0s 235us/step - loss: 1.5430 - val_loss: 1.4598\n",
      "Epoch 709/1000\n",
      "160/160 [==============================] - 0s 220us/step - loss: 1.5390 - val_loss: 1.4566\n",
      "Epoch 710/1000\n",
      "160/160 [==============================] - 0s 221us/step - loss: 1.5376 - val_loss: 1.4544\n",
      "Epoch 711/1000\n",
      "160/160 [==============================] - 0s 215us/step - loss: 1.5355 - val_loss: 1.4500\n",
      "Epoch 712/1000\n",
      "160/160 [==============================] - 0s 222us/step - loss: 1.5333 - val_loss: 1.4488\n",
      "Epoch 713/1000\n",
      "160/160 [==============================] - 0s 199us/step - loss: 1.5295 - val_loss: 1.4461\n",
      "Epoch 714/1000\n",
      "160/160 [==============================] - 0s 204us/step - loss: 1.5273 - val_loss: 1.4416\n",
      "Epoch 715/1000\n",
      "160/160 [==============================] - 0s 204us/step - loss: 1.5263 - val_loss: 1.4386\n",
      "Epoch 716/1000\n",
      "160/160 [==============================] - 0s 203us/step - loss: 1.5235 - val_loss: 1.4349\n",
      "Epoch 717/1000\n",
      "160/160 [==============================] - 0s 211us/step - loss: 1.5200 - val_loss: 1.4326\n",
      "Epoch 718/1000\n",
      "160/160 [==============================] - 0s 227us/step - loss: 1.5179 - val_loss: 1.4302\n",
      "Epoch 719/1000\n",
      "160/160 [==============================] - 0s 236us/step - loss: 1.5157 - val_loss: 1.4267\n",
      "Epoch 720/1000\n",
      "160/160 [==============================] - 0s 214us/step - loss: 1.5173 - val_loss: 1.4236\n",
      "Epoch 721/1000\n",
      "160/160 [==============================] - 0s 244us/step - loss: 1.5121 - val_loss: 1.4228\n",
      "Epoch 722/1000\n",
      "160/160 [==============================] - 0s 243us/step - loss: 1.5104 - val_loss: 1.4232\n",
      "Epoch 723/1000\n",
      "160/160 [==============================] - 0s 248us/step - loss: 1.5088 - val_loss: 1.4218\n",
      "Epoch 724/1000\n",
      "160/160 [==============================] - 0s 262us/step - loss: 1.5051 - val_loss: 1.4174\n",
      "Epoch 725/1000\n",
      "160/160 [==============================] - 0s 250us/step - loss: 1.5013 - val_loss: 1.4153\n",
      "Epoch 726/1000\n",
      "160/160 [==============================] - 0s 258us/step - loss: 1.5062 - val_loss: 1.4118\n",
      "Epoch 727/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 260us/step - loss: 1.4994 - val_loss: 1.4083\n",
      "Epoch 728/1000\n",
      "160/160 [==============================] - 0s 250us/step - loss: 1.4944 - val_loss: 1.4074\n",
      "Epoch 729/1000\n",
      "160/160 [==============================] - 0s 245us/step - loss: 1.4963 - val_loss: 1.4082\n",
      "Epoch 730/1000\n",
      "160/160 [==============================] - 0s 230us/step - loss: 1.4912 - val_loss: 1.4014\n",
      "Epoch 731/1000\n",
      "160/160 [==============================] - 0s 220us/step - loss: 1.4868 - val_loss: 1.3999\n",
      "Epoch 732/1000\n",
      "160/160 [==============================] - 0s 241us/step - loss: 1.4904 - val_loss: 1.4003\n",
      "Epoch 733/1000\n",
      "160/160 [==============================] - 0s 254us/step - loss: 1.4874 - val_loss: 1.3979\n",
      "Epoch 734/1000\n",
      "160/160 [==============================] - 0s 255us/step - loss: 1.4832 - val_loss: 1.3971\n",
      "Epoch 735/1000\n",
      "160/160 [==============================] - 0s 269us/step - loss: 1.4790 - val_loss: 1.3937\n",
      "Epoch 736/1000\n",
      "160/160 [==============================] - 0s 241us/step - loss: 1.4771 - val_loss: 1.3914\n",
      "Epoch 737/1000\n",
      "160/160 [==============================] - 0s 281us/step - loss: 1.4755 - val_loss: 1.3837\n",
      "Epoch 738/1000\n",
      "160/160 [==============================] - 0s 226us/step - loss: 1.4765 - val_loss: 1.3803\n",
      "Epoch 739/1000\n",
      "160/160 [==============================] - 0s 233us/step - loss: 1.4700 - val_loss: 1.3771\n",
      "Epoch 740/1000\n",
      "160/160 [==============================] - 0s 217us/step - loss: 1.4663 - val_loss: 1.3762\n",
      "Epoch 741/1000\n",
      "160/160 [==============================] - 0s 258us/step - loss: 1.4652 - val_loss: 1.3741\n",
      "Epoch 742/1000\n",
      "160/160 [==============================] - 0s 244us/step - loss: 1.4645 - val_loss: 1.3696\n",
      "Epoch 743/1000\n",
      "160/160 [==============================] - 0s 221us/step - loss: 1.4608 - val_loss: 1.3663\n",
      "Epoch 744/1000\n",
      "160/160 [==============================] - 0s 212us/step - loss: 1.4621 - val_loss: 1.3659\n",
      "Epoch 745/1000\n",
      "160/160 [==============================] - 0s 198us/step - loss: 1.4590 - val_loss: 1.3621\n",
      "Epoch 746/1000\n",
      "160/160 [==============================] - 0s 198us/step - loss: 1.4553 - val_loss: 1.3616\n",
      "Epoch 747/1000\n",
      "160/160 [==============================] - 0s 209us/step - loss: 1.4519 - val_loss: 1.3609\n",
      "Epoch 748/1000\n",
      "160/160 [==============================] - 0s 213us/step - loss: 1.4501 - val_loss: 1.3628\n",
      "Epoch 749/1000\n",
      "160/160 [==============================] - 0s 207us/step - loss: 1.4486 - val_loss: 1.3596\n",
      "Epoch 750/1000\n",
      "160/160 [==============================] - 0s 225us/step - loss: 1.4450 - val_loss: 1.3528\n",
      "Epoch 751/1000\n",
      "160/160 [==============================] - 0s 237us/step - loss: 1.4427 - val_loss: 1.3493\n",
      "Epoch 752/1000\n",
      "160/160 [==============================] - 0s 217us/step - loss: 1.4415 - val_loss: 1.3447\n",
      "Epoch 753/1000\n",
      "160/160 [==============================] - 0s 223us/step - loss: 1.4422 - val_loss: 1.3407\n",
      "Epoch 754/1000\n",
      "160/160 [==============================] - 0s 254us/step - loss: 1.4343 - val_loss: 1.3431\n",
      "Epoch 755/1000\n",
      "160/160 [==============================] - 0s 210us/step - loss: 1.4346 - val_loss: 1.3419\n",
      "Epoch 756/1000\n",
      "160/160 [==============================] - 0s 232us/step - loss: 1.4336 - val_loss: 1.3355\n",
      "Epoch 757/1000\n",
      "160/160 [==============================] - 0s 220us/step - loss: 1.4284 - val_loss: 1.3328\n",
      "Epoch 758/1000\n",
      "160/160 [==============================] - 0s 219us/step - loss: 1.4288 - val_loss: 1.3293\n",
      "Epoch 759/1000\n",
      "160/160 [==============================] - 0s 232us/step - loss: 1.4316 - val_loss: 1.3259\n",
      "Epoch 760/1000\n",
      "160/160 [==============================] - 0s 261us/step - loss: 1.4232 - val_loss: 1.3258\n",
      "Epoch 761/1000\n",
      "160/160 [==============================] - 0s 203us/step - loss: 1.4285 - val_loss: 1.3304\n",
      "Epoch 762/1000\n",
      "160/160 [==============================] - 0s 234us/step - loss: 1.4228 - val_loss: 1.3246\n",
      "Epoch 763/1000\n",
      "160/160 [==============================] - 0s 235us/step - loss: 1.4149 - val_loss: 1.3210\n",
      "Epoch 764/1000\n",
      "160/160 [==============================] - 0s 239us/step - loss: 1.4204 - val_loss: 1.3216\n",
      "Epoch 765/1000\n",
      "160/160 [==============================] - 0s 207us/step - loss: 1.4170 - val_loss: 1.3155\n",
      "Epoch 766/1000\n",
      "160/160 [==============================] - 0s 214us/step - loss: 1.4111 - val_loss: 1.3198\n",
      "Epoch 767/1000\n",
      "160/160 [==============================] - 0s 234us/step - loss: 1.4111 - val_loss: 1.3156\n",
      "Epoch 768/1000\n",
      "160/160 [==============================] - 0s 202us/step - loss: 1.4122 - val_loss: 1.3068\n",
      "Epoch 769/1000\n",
      "160/160 [==============================] - 0s 196us/step - loss: 1.4055 - val_loss: 1.3040\n",
      "Epoch 770/1000\n",
      "160/160 [==============================] - 0s 202us/step - loss: 1.4010 - val_loss: 1.3023\n",
      "Epoch 771/1000\n",
      "160/160 [==============================] - 0s 227us/step - loss: 1.4030 - val_loss: 1.3030\n",
      "Epoch 772/1000\n",
      "160/160 [==============================] - 0s 209us/step - loss: 1.3968 - val_loss: 1.3003\n",
      "Epoch 773/1000\n",
      "160/160 [==============================] - 0s 191us/step - loss: 1.3951 - val_loss: 1.2960\n",
      "Epoch 774/1000\n",
      "160/160 [==============================] - 0s 188us/step - loss: 1.3927 - val_loss: 1.2947\n",
      "Epoch 775/1000\n",
      "160/160 [==============================] - 0s 237us/step - loss: 1.3973 - val_loss: 1.2899\n",
      "Epoch 776/1000\n",
      "160/160 [==============================] - 0s 226us/step - loss: 1.3865 - val_loss: 1.2884\n",
      "Epoch 777/1000\n",
      "160/160 [==============================] - 0s 219us/step - loss: 1.3843 - val_loss: 1.2877\n",
      "Epoch 778/1000\n",
      "160/160 [==============================] - 0s 213us/step - loss: 1.3855 - val_loss: 1.2820\n",
      "Epoch 779/1000\n",
      "160/160 [==============================] - 0s 201us/step - loss: 1.3801 - val_loss: 1.2806\n",
      "Epoch 780/1000\n",
      "160/160 [==============================] - 0s 193us/step - loss: 1.3804 - val_loss: 1.2782\n",
      "Epoch 781/1000\n",
      "160/160 [==============================] - 0s 205us/step - loss: 1.3767 - val_loss: 1.2756\n",
      "Epoch 782/1000\n",
      "160/160 [==============================] - 0s 207us/step - loss: 1.3741 - val_loss: 1.2730\n",
      "Epoch 783/1000\n",
      "160/160 [==============================] - 0s 187us/step - loss: 1.3724 - val_loss: 1.2722\n",
      "Epoch 784/1000\n",
      "160/160 [==============================] - 0s 176us/step - loss: 1.3715 - val_loss: 1.2697\n",
      "Epoch 785/1000\n",
      "160/160 [==============================] - 0s 200us/step - loss: 1.3674 - val_loss: 1.2691\n",
      "Epoch 786/1000\n",
      "160/160 [==============================] - 0s 195us/step - loss: 1.3662 - val_loss: 1.2667\n",
      "Epoch 787/1000\n",
      "160/160 [==============================] - 0s 197us/step - loss: 1.3657 - val_loss: 1.2652\n",
      "Epoch 788/1000\n",
      "160/160 [==============================] - 0s 195us/step - loss: 1.3607 - val_loss: 1.2598\n",
      "Epoch 789/1000\n",
      "160/160 [==============================] - 0s 191us/step - loss: 1.3598 - val_loss: 1.2574\n",
      "Epoch 790/1000\n",
      "160/160 [==============================] - 0s 194us/step - loss: 1.3597 - val_loss: 1.2564\n",
      "Epoch 791/1000\n",
      "160/160 [==============================] - 0s 211us/step - loss: 1.3549 - val_loss: 1.2539\n",
      "Epoch 792/1000\n",
      "160/160 [==============================] - 0s 207us/step - loss: 1.3547 - val_loss: 1.2511\n",
      "Epoch 793/1000\n",
      "160/160 [==============================] - 0s 205us/step - loss: 1.3506 - val_loss: 1.2491\n",
      "Epoch 794/1000\n",
      "160/160 [==============================] - 0s 197us/step - loss: 1.3499 - val_loss: 1.2456\n",
      "Epoch 795/1000\n",
      "160/160 [==============================] - 0s 213us/step - loss: 1.3459 - val_loss: 1.2444\n",
      "Epoch 796/1000\n",
      "160/160 [==============================] - 0s 223us/step - loss: 1.3478 - val_loss: 1.2430\n",
      "Epoch 797/1000\n",
      "160/160 [==============================] - 0s 220us/step - loss: 1.3439 - val_loss: 1.2410\n",
      "Epoch 798/1000\n",
      "160/160 [==============================] - 0s 221us/step - loss: 1.3394 - val_loss: 1.2395\n",
      "Epoch 799/1000\n",
      "160/160 [==============================] - 0s 207us/step - loss: 1.3381 - val_loss: 1.2384\n",
      "Epoch 800/1000\n",
      "160/160 [==============================] - 0s 202us/step - loss: 1.3364 - val_loss: 1.2350\n",
      "Epoch 801/1000\n",
      "160/160 [==============================] - 0s 227us/step - loss: 1.3329 - val_loss: 1.2310\n",
      "Epoch 802/1000\n",
      "160/160 [==============================] - 0s 198us/step - loss: 1.3356 - val_loss: 1.2270\n",
      "Epoch 803/1000\n",
      "160/160 [==============================] - 0s 186us/step - loss: 1.3296 - val_loss: 1.2252\n",
      "Epoch 804/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 204us/step - loss: 1.3272 - val_loss: 1.2229\n",
      "Epoch 805/1000\n",
      "160/160 [==============================] - 0s 189us/step - loss: 1.3267 - val_loss: 1.2226\n",
      "Epoch 806/1000\n",
      "160/160 [==============================] - 0s 212us/step - loss: 1.3232 - val_loss: 1.2173\n",
      "Epoch 807/1000\n",
      "160/160 [==============================] - 0s 206us/step - loss: 1.3247 - val_loss: 1.2171\n",
      "Epoch 808/1000\n",
      "160/160 [==============================] - 0s 191us/step - loss: 1.3212 - val_loss: 1.2142\n",
      "Epoch 809/1000\n",
      "160/160 [==============================] - 0s 202us/step - loss: 1.3157 - val_loss: 1.2152\n",
      "Epoch 810/1000\n",
      "160/160 [==============================] - 0s 194us/step - loss: 1.3150 - val_loss: 1.2171\n",
      "Epoch 811/1000\n",
      "160/160 [==============================] - 0s 208us/step - loss: 1.3144 - val_loss: 1.2102\n",
      "Epoch 812/1000\n",
      "160/160 [==============================] - 0s 201us/step - loss: 1.3111 - val_loss: 1.2057\n",
      "Epoch 813/1000\n",
      "160/160 [==============================] - 0s 225us/step - loss: 1.3084 - val_loss: 1.2031\n",
      "Epoch 814/1000\n",
      "160/160 [==============================] - 0s 228us/step - loss: 1.3069 - val_loss: 1.2001\n",
      "Epoch 815/1000\n",
      "160/160 [==============================] - 0s 211us/step - loss: 1.3047 - val_loss: 1.1986\n",
      "Epoch 816/1000\n",
      "160/160 [==============================] - 0s 203us/step - loss: 1.3013 - val_loss: 1.1979\n",
      "Epoch 817/1000\n",
      "160/160 [==============================] - 0s 192us/step - loss: 1.3022 - val_loss: 1.1969\n",
      "Epoch 818/1000\n",
      "160/160 [==============================] - 0s 208us/step - loss: 1.2983 - val_loss: 1.1897\n",
      "Epoch 819/1000\n",
      "160/160 [==============================] - 0s 217us/step - loss: 1.2961 - val_loss: 1.1885\n",
      "Epoch 820/1000\n",
      "160/160 [==============================] - 0s 203us/step - loss: 1.3010 - val_loss: 1.1864\n",
      "Epoch 821/1000\n",
      "160/160 [==============================] - 0s 216us/step - loss: 1.2920 - val_loss: 1.1865\n",
      "Epoch 822/1000\n",
      "160/160 [==============================] - 0s 209us/step - loss: 1.2906 - val_loss: 1.1910\n",
      "Epoch 823/1000\n",
      "160/160 [==============================] - 0s 220us/step - loss: 1.2912 - val_loss: 1.1848\n",
      "Epoch 824/1000\n",
      "160/160 [==============================] - 0s 259us/step - loss: 1.2890 - val_loss: 1.1777\n",
      "Epoch 825/1000\n",
      "160/160 [==============================] - 0s 205us/step - loss: 1.2869 - val_loss: 1.1750\n",
      "Epoch 826/1000\n",
      "160/160 [==============================] - 0s 191us/step - loss: 1.2839 - val_loss: 1.1709\n",
      "Epoch 827/1000\n",
      "160/160 [==============================] - 0s 202us/step - loss: 1.2841 - val_loss: 1.1726\n",
      "Epoch 828/1000\n",
      "160/160 [==============================] - 0s 193us/step - loss: 1.2783 - val_loss: 1.1705\n",
      "Epoch 829/1000\n",
      "160/160 [==============================] - 0s 194us/step - loss: 1.2750 - val_loss: 1.1677\n",
      "Epoch 830/1000\n",
      "160/160 [==============================] - 0s 182us/step - loss: 1.2737 - val_loss: 1.1645\n",
      "Epoch 831/1000\n",
      "160/160 [==============================] - 0s 192us/step - loss: 1.2732 - val_loss: 1.1643\n",
      "Epoch 832/1000\n",
      "160/160 [==============================] - 0s 224us/step - loss: 1.2736 - val_loss: 1.1611\n",
      "Epoch 833/1000\n",
      "160/160 [==============================] - 0s 205us/step - loss: 1.2713 - val_loss: 1.1631\n",
      "Epoch 834/1000\n",
      "160/160 [==============================] - 0s 178us/step - loss: 1.2700 - val_loss: 1.1563\n",
      "Epoch 835/1000\n",
      "160/160 [==============================] - 0s 176us/step - loss: 1.2655 - val_loss: 1.1554\n",
      "Epoch 836/1000\n",
      "160/160 [==============================] - 0s 185us/step - loss: 1.2629 - val_loss: 1.1520\n",
      "Epoch 837/1000\n",
      "160/160 [==============================] - 0s 213us/step - loss: 1.2627 - val_loss: 1.1485\n",
      "Epoch 838/1000\n",
      "160/160 [==============================] - 0s 204us/step - loss: 1.2589 - val_loss: 1.1458\n",
      "Epoch 839/1000\n",
      "160/160 [==============================] - 0s 236us/step - loss: 1.2571 - val_loss: 1.1509\n",
      "Epoch 840/1000\n",
      "160/160 [==============================] - 0s 210us/step - loss: 1.2592 - val_loss: 1.1454\n",
      "Epoch 841/1000\n",
      "160/160 [==============================] - 0s 201us/step - loss: 1.2525 - val_loss: 1.1369\n",
      "Epoch 842/1000\n",
      "160/160 [==============================] - 0s 246us/step - loss: 1.2524 - val_loss: 1.1376\n",
      "Epoch 843/1000\n",
      "160/160 [==============================] - 0s 166us/step - loss: 1.2516 - val_loss: 1.1331\n",
      "Epoch 844/1000\n",
      "160/160 [==============================] - 0s 193us/step - loss: 1.2450 - val_loss: 1.1376\n",
      "Epoch 845/1000\n",
      "160/160 [==============================] - 0s 185us/step - loss: 1.2464 - val_loss: 1.1419\n",
      "Epoch 846/1000\n",
      "160/160 [==============================] - 0s 213us/step - loss: 1.2460 - val_loss: 1.1314\n",
      "Epoch 847/1000\n",
      "160/160 [==============================] - 0s 207us/step - loss: 1.2390 - val_loss: 1.1282\n",
      "Epoch 848/1000\n",
      "160/160 [==============================] - 0s 201us/step - loss: 1.2414 - val_loss: 1.1253\n",
      "Epoch 849/1000\n",
      "160/160 [==============================] - 0s 206us/step - loss: 1.2391 - val_loss: 1.1221\n",
      "Epoch 850/1000\n",
      "160/160 [==============================] - ETA: 0s - loss: 0.984 - 0s 191us/step - loss: 1.2363 - val_loss: 1.1225\n",
      "Epoch 851/1000\n",
      "160/160 [==============================] - 0s 194us/step - loss: 1.2431 - val_loss: 1.1200\n",
      "Epoch 852/1000\n",
      "160/160 [==============================] - 0s 189us/step - loss: 1.2322 - val_loss: 1.1143\n",
      "Epoch 853/1000\n",
      "160/160 [==============================] - 0s 182us/step - loss: 1.2311 - val_loss: 1.1146\n",
      "Epoch 854/1000\n",
      "160/160 [==============================] - 0s 178us/step - loss: 1.2289 - val_loss: 1.1154\n",
      "Epoch 855/1000\n",
      "160/160 [==============================] - 0s 186us/step - loss: 1.2245 - val_loss: 1.1183\n",
      "Epoch 856/1000\n",
      "160/160 [==============================] - 0s 212us/step - loss: 1.2237 - val_loss: 1.1151\n",
      "Epoch 857/1000\n",
      "160/160 [==============================] - 0s 195us/step - loss: 1.2205 - val_loss: 1.1082\n",
      "Epoch 858/1000\n",
      "160/160 [==============================] - 0s 216us/step - loss: 1.2174 - val_loss: 1.1037\n",
      "Epoch 859/1000\n",
      "160/160 [==============================] - 0s 219us/step - loss: 1.2179 - val_loss: 1.0988\n",
      "Epoch 860/1000\n",
      "160/160 [==============================] - 0s 205us/step - loss: 1.2150 - val_loss: 1.0983\n",
      "Epoch 861/1000\n",
      "160/160 [==============================] - 0s 210us/step - loss: 1.2120 - val_loss: 1.0980\n",
      "Epoch 862/1000\n",
      "160/160 [==============================] - 0s 220us/step - loss: 1.2147 - val_loss: 1.0935\n",
      "Epoch 863/1000\n",
      "160/160 [==============================] - 0s 210us/step - loss: 1.2089 - val_loss: 1.0930\n",
      "Epoch 864/1000\n",
      "160/160 [==============================] - 0s 211us/step - loss: 1.2065 - val_loss: 1.0951\n",
      "Epoch 865/1000\n",
      "160/160 [==============================] - 0s 220us/step - loss: 1.2062 - val_loss: 1.0894\n",
      "Epoch 866/1000\n",
      "160/160 [==============================] - 0s 216us/step - loss: 1.2023 - val_loss: 1.0874\n",
      "Epoch 867/1000\n",
      "160/160 [==============================] - 0s 211us/step - loss: 1.2002 - val_loss: 1.0847\n",
      "Epoch 868/1000\n",
      "160/160 [==============================] - 0s 207us/step - loss: 1.2036 - val_loss: 1.0850\n",
      "Epoch 869/1000\n",
      "160/160 [==============================] - 0s 207us/step - loss: 1.1974 - val_loss: 1.0821\n",
      "Epoch 870/1000\n",
      "160/160 [==============================] - 0s 232us/step - loss: 1.1960 - val_loss: 1.0777\n",
      "Epoch 871/1000\n",
      "160/160 [==============================] - 0s 198us/step - loss: 1.1934 - val_loss: 1.0761\n",
      "Epoch 872/1000\n",
      "160/160 [==============================] - 0s 193us/step - loss: 1.1946 - val_loss: 1.0777\n",
      "Epoch 873/1000\n",
      "160/160 [==============================] - 0s 234us/step - loss: 1.1908 - val_loss: 1.0750\n",
      "Epoch 874/1000\n",
      "160/160 [==============================] - 0s 214us/step - loss: 1.1868 - val_loss: 1.0697\n",
      "Epoch 875/1000\n",
      "160/160 [==============================] - 0s 213us/step - loss: 1.1902 - val_loss: 1.0677\n",
      "Epoch 876/1000\n",
      "160/160 [==============================] - 0s 205us/step - loss: 1.1906 - val_loss: 1.0667\n",
      "Epoch 877/1000\n",
      "160/160 [==============================] - 0s 197us/step - loss: 1.1834 - val_loss: 1.0718\n",
      "Epoch 878/1000\n",
      "160/160 [==============================] - 0s 204us/step - loss: 1.1843 - val_loss: 1.0675\n",
      "Epoch 879/1000\n",
      "160/160 [==============================] - 0s 236us/step - loss: 1.1818 - val_loss: 1.0578\n",
      "Epoch 880/1000\n",
      "160/160 [==============================] - 0s 204us/step - loss: 1.1875 - val_loss: 1.0574\n",
      "Epoch 881/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 236us/step - loss: 1.1770 - val_loss: 1.0538\n",
      "Epoch 882/1000\n",
      "160/160 [==============================] - 0s 230us/step - loss: 1.1873 - val_loss: 1.0676\n",
      "Epoch 883/1000\n",
      "160/160 [==============================] - 0s 244us/step - loss: 1.1775 - val_loss: 1.0547\n",
      "Epoch 884/1000\n",
      "160/160 [==============================] - 0s 257us/step - loss: 1.1872 - val_loss: 1.0529\n",
      "Epoch 885/1000\n",
      "160/160 [==============================] - 0s 249us/step - loss: 1.1736 - val_loss: 1.0472\n",
      "Epoch 886/1000\n",
      "160/160 [==============================] - 0s 188us/step - loss: 1.1649 - val_loss: 1.0491\n",
      "Epoch 887/1000\n",
      "160/160 [==============================] - 0s 208us/step - loss: 1.1660 - val_loss: 1.0531\n",
      "Epoch 888/1000\n",
      "160/160 [==============================] - 0s 201us/step - loss: 1.1663 - val_loss: 1.0438\n",
      "Epoch 889/1000\n",
      "160/160 [==============================] - 0s 219us/step - loss: 1.1604 - val_loss: 1.0391\n",
      "Epoch 890/1000\n",
      "160/160 [==============================] - 0s 198us/step - loss: 1.1591 - val_loss: 1.0364\n",
      "Epoch 891/1000\n",
      "160/160 [==============================] - 0s 192us/step - loss: 1.1580 - val_loss: 1.0374\n",
      "Epoch 892/1000\n",
      "160/160 [==============================] - 0s 233us/step - loss: 1.1568 - val_loss: 1.0382\n",
      "Epoch 893/1000\n",
      "160/160 [==============================] - 0s 211us/step - loss: 1.1545 - val_loss: 1.0330\n",
      "Epoch 894/1000\n",
      "160/160 [==============================] - 0s 195us/step - loss: 1.1559 - val_loss: 1.0294\n",
      "Epoch 895/1000\n",
      "160/160 [==============================] - 0s 203us/step - loss: 1.1505 - val_loss: 1.0275\n",
      "Epoch 896/1000\n",
      "160/160 [==============================] - 0s 218us/step - loss: 1.1466 - val_loss: 1.0280\n",
      "Epoch 897/1000\n",
      "160/160 [==============================] - 0s 208us/step - loss: 1.1468 - val_loss: 1.0269\n",
      "Epoch 898/1000\n",
      "160/160 [==============================] - 0s 200us/step - loss: 1.1464 - val_loss: 1.0219\n",
      "Epoch 899/1000\n",
      "160/160 [==============================] - ETA: 0s - loss: 1.292 - 0s 238us/step - loss: 1.1416 - val_loss: 1.0199\n",
      "Epoch 900/1000\n",
      "160/160 [==============================] - 0s 205us/step - loss: 1.1420 - val_loss: 1.0185\n",
      "Epoch 901/1000\n",
      "160/160 [==============================] - 0s 211us/step - loss: 1.1406 - val_loss: 1.0176\n",
      "Epoch 902/1000\n",
      "160/160 [==============================] - 0s 247us/step - loss: 1.1405 - val_loss: 1.0200\n",
      "Epoch 903/1000\n",
      "160/160 [==============================] - 0s 211us/step - loss: 1.1390 - val_loss: 1.0128\n",
      "Epoch 904/1000\n",
      "160/160 [==============================] - 0s 227us/step - loss: 1.1327 - val_loss: 1.0096\n",
      "Epoch 905/1000\n",
      "160/160 [==============================] - 0s 214us/step - loss: 1.1315 - val_loss: 1.0068\n",
      "Epoch 906/1000\n",
      "160/160 [==============================] - 0s 210us/step - loss: 1.1350 - val_loss: 1.0084\n",
      "Epoch 907/1000\n",
      "160/160 [==============================] - 0s 210us/step - loss: 1.1281 - val_loss: 1.0022\n",
      "Epoch 908/1000\n",
      "160/160 [==============================] - 0s 210us/step - loss: 1.1274 - val_loss: 0.9983\n",
      "Epoch 909/1000\n",
      "160/160 [==============================] - 0s 223us/step - loss: 1.1285 - val_loss: 0.9991\n",
      "Epoch 910/1000\n",
      "160/160 [==============================] - 0s 208us/step - loss: 1.1233 - val_loss: 0.9957\n",
      "Epoch 911/1000\n",
      "160/160 [==============================] - 0s 218us/step - loss: 1.1205 - val_loss: 0.9946\n",
      "Epoch 912/1000\n",
      "160/160 [==============================] - 0s 201us/step - loss: 1.1220 - val_loss: 0.9949\n",
      "Epoch 913/1000\n",
      "160/160 [==============================] - 0s 281us/step - loss: 1.1194 - val_loss: 0.9897\n",
      "Epoch 914/1000\n",
      "160/160 [==============================] - 0s 246us/step - loss: 1.1164 - val_loss: 0.9895\n",
      "Epoch 915/1000\n",
      "160/160 [==============================] - 0s 200us/step - loss: 1.1162 - val_loss: 0.9884\n",
      "Epoch 916/1000\n",
      "160/160 [==============================] - 0s 215us/step - loss: 1.1158 - val_loss: 0.9921\n",
      "Epoch 917/1000\n",
      "160/160 [==============================] - 0s 242us/step - loss: 1.1125 - val_loss: 0.9841\n",
      "Epoch 918/1000\n",
      "160/160 [==============================] - 0s 212us/step - loss: 1.1107 - val_loss: 0.9780\n",
      "Epoch 919/1000\n",
      "160/160 [==============================] - 0s 220us/step - loss: 1.1079 - val_loss: 0.9767\n",
      "Epoch 920/1000\n",
      "160/160 [==============================] - 0s 201us/step - loss: 1.1061 - val_loss: 0.9751\n",
      "Epoch 921/1000\n",
      "160/160 [==============================] - 0s 187us/step - loss: 1.1058 - val_loss: 0.9726\n",
      "Epoch 922/1000\n",
      "160/160 [==============================] - 0s 184us/step - loss: 1.1022 - val_loss: 0.9742\n",
      "Epoch 923/1000\n",
      "160/160 [==============================] - 0s 188us/step - loss: 1.1075 - val_loss: 0.9713\n",
      "Epoch 924/1000\n",
      "160/160 [==============================] - 0s 178us/step - loss: 1.0980 - val_loss: 0.9722\n",
      "Epoch 925/1000\n",
      "160/160 [==============================] - 0s 193us/step - loss: 1.0979 - val_loss: 0.9707\n",
      "Epoch 926/1000\n",
      "160/160 [==============================] - 0s 194us/step - loss: 1.0970 - val_loss: 0.9695\n",
      "Epoch 927/1000\n",
      "160/160 [==============================] - 0s 188us/step - loss: 1.0950 - val_loss: 0.9658\n",
      "Epoch 928/1000\n",
      "160/160 [==============================] - 0s 199us/step - loss: 1.0919 - val_loss: 0.9651\n",
      "Epoch 929/1000\n",
      "160/160 [==============================] - 0s 191us/step - loss: 1.0992 - val_loss: 0.9636\n",
      "Epoch 930/1000\n",
      "160/160 [==============================] - 0s 184us/step - loss: 1.0893 - val_loss: 0.9659\n",
      "Epoch 931/1000\n",
      "160/160 [==============================] - 0s 190us/step - loss: 1.0897 - val_loss: 0.9690\n",
      "Epoch 932/1000\n",
      "160/160 [==============================] - 0s 203us/step - loss: 1.0910 - val_loss: 0.9599\n",
      "Epoch 933/1000\n",
      "160/160 [==============================] - 0s 204us/step - loss: 1.0841 - val_loss: 0.9569\n",
      "Epoch 934/1000\n",
      "160/160 [==============================] - 0s 195us/step - loss: 1.0883 - val_loss: 0.9567\n",
      "Epoch 935/1000\n",
      "160/160 [==============================] - 0s 214us/step - loss: 1.0823 - val_loss: 0.9553\n",
      "Epoch 936/1000\n",
      "160/160 [==============================] - 0s 229us/step - loss: 1.0804 - val_loss: 0.9629\n",
      "Epoch 937/1000\n",
      "160/160 [==============================] - ETA: 0s - loss: 1.002 - 0s 230us/step - loss: 1.0815 - val_loss: 0.9544\n",
      "Epoch 938/1000\n",
      "160/160 [==============================] - 0s 224us/step - loss: 1.0785 - val_loss: 0.9491\n",
      "Epoch 939/1000\n",
      "160/160 [==============================] - 0s 199us/step - loss: 1.0757 - val_loss: 0.9474\n",
      "Epoch 940/1000\n",
      "160/160 [==============================] - 0s 179us/step - loss: 1.0739 - val_loss: 0.9466\n",
      "Epoch 941/1000\n",
      "160/160 [==============================] - 0s 201us/step - loss: 1.0711 - val_loss: 0.9439\n",
      "Epoch 942/1000\n",
      "160/160 [==============================] - 0s 199us/step - loss: 1.0691 - val_loss: 0.9426\n",
      "Epoch 943/1000\n",
      "160/160 [==============================] - 0s 199us/step - loss: 1.0682 - val_loss: 0.9371\n",
      "Epoch 944/1000\n",
      "160/160 [==============================] - 0s 203us/step - loss: 1.0679 - val_loss: 0.9323\n",
      "Epoch 945/1000\n",
      "160/160 [==============================] - 0s 234us/step - loss: 1.0664 - val_loss: 0.9334\n",
      "Epoch 946/1000\n",
      "160/160 [==============================] - 0s 217us/step - loss: 1.0630 - val_loss: 0.9329\n",
      "Epoch 947/1000\n",
      "160/160 [==============================] - 0s 207us/step - loss: 1.0607 - val_loss: 0.9325\n",
      "Epoch 948/1000\n",
      "160/160 [==============================] - 0s 218us/step - loss: 1.0614 - val_loss: 0.9332\n",
      "Epoch 949/1000\n",
      "160/160 [==============================] - 0s 205us/step - loss: 1.0630 - val_loss: 0.9286\n",
      "Epoch 950/1000\n",
      "160/160 [==============================] - 0s 191us/step - loss: 1.0575 - val_loss: 0.9286\n",
      "Epoch 951/1000\n",
      "160/160 [==============================] - 0s 190us/step - loss: 1.0551 - val_loss: 0.9292\n",
      "Epoch 952/1000\n",
      "160/160 [==============================] - 0s 222us/step - loss: 1.0536 - val_loss: 0.9251\n",
      "Epoch 953/1000\n",
      "160/160 [==============================] - 0s 193us/step - loss: 1.0591 - val_loss: 0.9175\n",
      "Epoch 954/1000\n",
      "160/160 [==============================] - 0s 195us/step - loss: 1.0637 - val_loss: 0.9151\n",
      "Epoch 955/1000\n",
      "160/160 [==============================] - 0s 207us/step - loss: 1.0477 - val_loss: 0.9237\n",
      "Epoch 956/1000\n",
      "160/160 [==============================] - 0s 178us/step - loss: 1.0510 - val_loss: 0.9232\n",
      "Epoch 957/1000\n",
      "160/160 [==============================] - 0s 184us/step - loss: 1.0481 - val_loss: 0.9121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 958/1000\n",
      "160/160 [==============================] - 0s 185us/step - loss: 1.0521 - val_loss: 0.9131\n",
      "Epoch 959/1000\n",
      "160/160 [==============================] - 0s 195us/step - loss: 1.0471 - val_loss: 0.9119\n",
      "Epoch 960/1000\n",
      "160/160 [==============================] - 0s 206us/step - loss: 1.0477 - val_loss: 0.9260\n",
      "Epoch 961/1000\n",
      "160/160 [==============================] - 0s 205us/step - loss: 1.0446 - val_loss: 0.9154\n",
      "Epoch 962/1000\n",
      "160/160 [==============================] - 0s 196us/step - loss: 1.0381 - val_loss: 0.9087\n",
      "Epoch 963/1000\n",
      "160/160 [==============================] - 0s 214us/step - loss: 1.0513 - val_loss: 0.9092\n",
      "Epoch 964/1000\n",
      "160/160 [==============================] - 0s 193us/step - loss: 1.0377 - val_loss: 0.9105\n",
      "Epoch 965/1000\n",
      "160/160 [==============================] - 0s 197us/step - loss: 1.0337 - val_loss: 0.9113\n",
      "Epoch 966/1000\n",
      "160/160 [==============================] - 0s 191us/step - loss: 1.0338 - val_loss: 0.9038\n",
      "Epoch 967/1000\n",
      "160/160 [==============================] - 0s 195us/step - loss: 1.0379 - val_loss: 0.8977\n",
      "Epoch 968/1000\n",
      "160/160 [==============================] - 0s 185us/step - loss: 1.0298 - val_loss: 0.8946\n",
      "Epoch 969/1000\n",
      "160/160 [==============================] - 0s 202us/step - loss: 1.0306 - val_loss: 0.8984\n",
      "Epoch 970/1000\n",
      "160/160 [==============================] - 0s 189us/step - loss: 1.0261 - val_loss: 0.8924\n",
      "Epoch 971/1000\n",
      "160/160 [==============================] - 0s 210us/step - loss: 1.0261 - val_loss: 0.8905\n",
      "Epoch 972/1000\n",
      "160/160 [==============================] - 0s 180us/step - loss: 1.0227 - val_loss: 0.8895\n",
      "Epoch 973/1000\n",
      "160/160 [==============================] - 0s 208us/step - loss: 1.0203 - val_loss: 0.8925\n",
      "Epoch 974/1000\n",
      "160/160 [==============================] - 0s 202us/step - loss: 1.0203 - val_loss: 0.8924\n",
      "Epoch 975/1000\n",
      "160/160 [==============================] - 0s 207us/step - loss: 1.0189 - val_loss: 0.8848\n",
      "Epoch 976/1000\n",
      "160/160 [==============================] - 0s 212us/step - loss: 1.0229 - val_loss: 0.8838\n",
      "Epoch 977/1000\n",
      "160/160 [==============================] - 0s 238us/step - loss: 1.0157 - val_loss: 0.8834\n",
      "Epoch 978/1000\n",
      "160/160 [==============================] - 0s 231us/step - loss: 1.0163 - val_loss: 0.8819\n",
      "Epoch 979/1000\n",
      "160/160 [==============================] - 0s 210us/step - loss: 1.0217 - val_loss: 0.8907\n",
      "Epoch 980/1000\n",
      "160/160 [==============================] - 0s 188us/step - loss: 1.0140 - val_loss: 0.8846\n",
      "Epoch 981/1000\n",
      "160/160 [==============================] - 0s 231us/step - loss: 1.0132 - val_loss: 0.8795\n",
      "Epoch 982/1000\n",
      "160/160 [==============================] - 0s 226us/step - loss: 1.0107 - val_loss: 0.8784\n",
      "Epoch 983/1000\n",
      "160/160 [==============================] - 0s 197us/step - loss: 1.0048 - val_loss: 0.8808\n",
      "Epoch 984/1000\n",
      "160/160 [==============================] - 0s 206us/step - loss: 1.0053 - val_loss: 0.8871\n",
      "Epoch 985/1000\n",
      "160/160 [==============================] - 0s 223us/step - loss: 1.0054 - val_loss: 0.8777\n",
      "Epoch 986/1000\n",
      "160/160 [==============================] - 0s 252us/step - loss: 1.0009 - val_loss: 0.8731\n",
      "Epoch 987/1000\n",
      "160/160 [==============================] - 0s 226us/step - loss: 1.0038 - val_loss: 0.8745\n",
      "Epoch 988/1000\n",
      "160/160 [==============================] - 0s 213us/step - loss: 1.0023 - val_loss: 0.8720\n",
      "Epoch 989/1000\n",
      "160/160 [==============================] - 0s 231us/step - loss: 1.0030 - val_loss: 0.8796\n",
      "Epoch 990/1000\n",
      "160/160 [==============================] - 0s 193us/step - loss: 0.9952 - val_loss: 0.8718\n",
      "Epoch 991/1000\n",
      "160/160 [==============================] - 0s 219us/step - loss: 0.9904 - val_loss: 0.8723\n",
      "Epoch 992/1000\n",
      "160/160 [==============================] - 0s 239us/step - loss: 0.9926 - val_loss: 0.8705\n",
      "Epoch 993/1000\n",
      "160/160 [==============================] - 0s 237us/step - loss: 0.9953 - val_loss: 0.8708\n",
      "Epoch 994/1000\n",
      "160/160 [==============================] - 0s 275us/step - loss: 1.0010 - val_loss: 0.8721\n",
      "Epoch 995/1000\n",
      "160/160 [==============================] - 0s 249us/step - loss: 0.9869 - val_loss: 0.8642\n",
      "Epoch 996/1000\n",
      "160/160 [==============================] - 0s 237us/step - loss: 0.9904 - val_loss: 0.8657\n",
      "Epoch 997/1000\n",
      "160/160 [==============================] - 0s 283us/step - loss: 0.9910 - val_loss: 0.8616\n",
      "Epoch 998/1000\n",
      "160/160 [==============================] - 0s 221us/step - loss: 0.9829 - val_loss: 0.8606\n",
      "Epoch 999/1000\n",
      "160/160 [==============================] - 0s 221us/step - loss: 0.9810 - val_loss: 0.8591\n",
      "Epoch 1000/1000\n",
      "160/160 [==============================] - 0s 183us/step - loss: 0.9823 - val_loss: 0.8538\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(x_train,y_train,batch_size=80,epochs=1000,validation_data=(x_val,y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从输出的数据可以看出：一开始，loss的值非常大，而随着训练不断的进行，loss在逐渐减小。loss是我们预先设定的损失函数计算得到的损失值,val_loss是测试集的损失值，数字越小，说明模型的识别精度越好。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**知识链接**\n",
    "\n",
    "history是keras的一个专用对象——History类。History类对象包含两个属性，分别为epoch和history。epoch为训练轮数，而history并不固定，由编译模型（compile）时的参数决定。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': [2323.8714599609375, 2095.5718383789062, 1907.81298828125, 1715.9583129882812, 1540.694091796875, 1374.264892578125, 1227.6837158203125, 1089.5366516113281, 966.9758605957031, 862.3857421875, 763.4689025878906, 677.3984375, 607.1590881347656, 547.5772552490234, 488.45751953125, 442.74070739746094, 404.39637756347656, 373.260498046875, 347.7055969238281, 326.11509704589844, 308.02052307128906, 295.4550476074219, 282.4314422607422, 272.6534881591797, 264.2041473388672, 255.93704986572266, 249.28530883789062, 242.765625, 235.99964141845703, 230.54686737060547, 223.86456298828125, 217.66085052490234, 211.7111358642578, 205.2708282470703, 198.9144287109375, 192.65232849121094, 186.177978515625, 180.2510757446289, 173.58940887451172, 167.49867248535156, 161.7417755126953, 155.7442169189453, 150.35115814208984, 144.62525177001953, 139.06281661987305, 133.81660842895508, 128.53779220581055, 123.48653411865234, 119.25614166259766, 114.4974250793457, 110.13480758666992, 105.80297088623047, 101.44502258300781, 97.54337310791016, 93.61747360229492, 90.06002044677734, 85.89470672607422, 82.35893630981445, 78.51789093017578, 74.52167892456055, 70.61062049865723, 66.83334922790527, 63.26635932922363, 59.795921325683594, 56.74880027770996, 53.8331241607666, 50.988224029541016, 48.71487808227539, 46.2835693359375, 44.06263732910156, 41.94076919555664, 39.702842712402344, 37.583839416503906, 35.7786865234375, 34.09123992919922, 32.65235710144043, 31.345267295837402, 30.03256893157959, 28.844850540161133, 27.64246940612793, 26.504496574401855, 25.535590171813965, 24.61140727996826, 23.7291202545166, 22.897125244140625, 22.057588577270508, 21.29724884033203, 20.55666208267212, 19.733259201049805, 19.09465789794922, 18.388235092163086, 17.806649208068848, 17.132308959960938, 16.576707363128662, 16.02303123474121, 15.505296230316162, 15.028429985046387, 14.57589864730835, 14.140393733978271, 13.750872611999512, 13.344100952148438, 12.9748215675354, 12.65773868560791, 12.317254543304443, 12.011085987091064, 11.722087860107422, 11.446345329284668, 11.190612316131592, 10.934599876403809, 10.691932678222656, 10.462914943695068, 10.230137825012207, 10.033384799957275, 9.804161071777344, 9.612905979156494, 9.430222988128662, 9.258477687835693, 9.08875584602356, 8.904613494873047, 8.740696430206299, 8.576882362365723, 8.431217670440674, 8.291897058486938, 8.13245940208435, 7.970168590545654, 7.8454225063323975, 7.70489501953125, 7.585035562515259, 7.451300382614136, 7.3183135986328125, 7.188527345657349, 7.072339057922363, 6.9493324756622314, 6.830933094024658, 6.738856315612793, 6.617651700973511, 6.506066799163818, 6.384579658508301, 6.284619092941284, 6.198696136474609, 6.085673809051514, 5.991688251495361, 5.887115240097046, 5.7962751388549805, 5.709609508514404, 5.61443018913269, 5.531418561935425, 5.45109224319458, 5.3663411140441895, 5.293813228607178, 5.211749792098999, 5.132227182388306, 5.06001353263855, 4.987438678741455, 4.92409610748291, 4.851989507675171, 4.7812806367874146, 4.721318960189819, 4.656044006347656, 4.595577001571655, 4.5424981117248535, 4.478276014328003, 4.421962022781372, 4.372259974479675, 4.315531611442566, 4.264826655387878, 4.21987521648407, 4.170260548591614, 4.13292384147644, 4.075703740119934, 4.036056399345398, 3.9876441955566406, 3.9444061517715454, 3.906960964202881, 3.868579626083374, 3.8342626094818115, 3.7856398820877075, 3.755779981613159, 3.7175018787384033, 3.6887290477752686, 3.6532219648361206, 3.623509645462036, 3.598169445991516, 3.5659698247909546, 3.536221146583557, 3.514543652534485, 3.4884685277938843, 3.467466711997986, 3.4423367977142334, 3.4231057167053223, 3.4040756225585938, 3.383711576461792, 3.35947847366333, 3.3407782316207886, 3.326568365097046, 3.305346965789795, 3.2879083156585693, 3.2709513902664185, 3.255126476287842, 3.2430447340011597, 3.226808547973633, 3.2135318517684937, 3.2084113359451294, 3.1887035369873047, 3.1787084341049194, 3.168383002281189, 3.1566500663757324, 3.1547420024871826, 3.1417189836502075, 3.1290395259857178, 3.1229079961776733, 3.1130350828170776, 3.1050920486450195, 3.0984749794006348, 3.0948272943496704, 3.0838342905044556, 3.077288269996643, 3.0699158906936646, 3.062002658843994, 3.0574196577072144, 3.051217555999756, 3.0499370098114014, 3.0413841009140015, 3.0332798957824707, 3.027125358581543, 3.0219738483428955, 3.0169438123703003, 3.010222911834717, 3.007474899291992, 3.0007882118225098, 2.9974578619003296, 2.988407611846924, 2.9887996912002563, 2.9786934852600098, 2.9726210832595825, 2.969804286956787, 2.9640660285949707, 2.9583135843276978, 2.9562180042266846, 2.95279598236084, 2.944224238395691, 2.9386746883392334, 2.9336687326431274, 2.928567409515381, 2.924732208251953, 2.9199297428131104, 2.913849353790283, 2.909730911254883, 2.9063069820404053, 2.901921510696411, 2.895685076713562, 2.8922899961471558, 2.8875967264175415, 2.883803129196167, 2.8792941570281982, 2.874744415283203, 2.870397210121155, 2.867383599281311, 2.8629772663116455, 2.8587714433670044, 2.8551111221313477, 2.8512803316116333, 2.8467252254486084, 2.8431442975997925, 2.838794469833374, 2.8355523347854614, 2.83491051197052, 2.827345371246338, 2.823324680328369, 2.8210976123809814, 2.816192865371704, 2.8123241662979126, 2.809870958328247, 2.805692672729492, 2.803504228591919, 2.8023102283477783, 2.796460270881653, 2.79150652885437, 2.789318561553955, 2.788713216781616, 2.7857954502105713, 2.7896496057510376, 2.7754483222961426, 2.7720234394073486, 2.7729086875915527, 2.7666983604431152, 2.7618002891540527, 2.757807731628418, 2.7575230598449707, 2.753868579864502, 2.7499481439590454, 2.745255470275879, 2.7422086000442505, 2.7401317358016968, 2.736808180809021, 2.732795238494873, 2.7338355779647827, 2.727527141571045, 2.7254199981689453, 2.719526529312134, 2.716105580329895, 2.7133724689483643, 2.711653232574463, 2.7069746255874634, 2.7035181522369385, 2.7017446756362915, 2.697125792503357, 2.6948760747909546, 2.6911216974258423, 2.6882288455963135, 2.685038685798645, 2.6848360300064087, 2.678073763847351, 2.6748173236846924, 2.673000454902649, 2.673025131225586, 2.666099429130554, 2.6647719144821167, 2.663153648376465, 2.6642889976501465, 2.655792474746704, 2.6515631675720215, 2.6498706340789795, 2.649521827697754, 2.6423083543777466, 2.6414761543273926, 2.636327862739563, 2.636060118675232, 2.6315011978149414, 2.627668619155884, 2.626801013946533, 2.624856948852539, 2.620305299758911, 2.6163012981414795, 2.6129144430160522, 2.6102617979049683, 2.6081165075302124, 2.604327440261841, 2.6007957458496094, 2.599219858646393, 2.5963919162750244, 2.592729330062866, 2.5901670455932617, 2.5872803926467896, 2.5854897499084473, 2.5838427543640137, 2.578647255897522, 2.5758755207061768, 2.5753531455993652, 2.5729087591171265, 2.570643424987793, 2.5649717450141907, 2.566354751586914, 2.5594656467437744, 2.5585495233535767, 2.5576177835464478, 2.5532715916633606, 2.548987030982971, 2.547000288963318, 2.54472553730011, 2.543174624443054, 2.5408657789230347, 2.5396400690078735, 2.5375449657440186, 2.5324472188949585, 2.530205011367798, 2.530567169189453, 2.5232765674591064, 2.52131724357605, 2.5259920358657837, 2.5206480026245117, 2.5152111053466797, 2.5130311250686646, 2.5102308988571167, 2.5096981525421143, 2.5130022764205933, 2.510088086128235, 2.500128149986267, 2.498051404953003, 2.496571660041809, 2.4914010763168335, 2.4908053874969482, 2.4854007959365845, 2.4833003282546997, 2.481271743774414, 2.4794954657554626, 2.4750757217407227, 2.473135471343994, 2.469730854034424, 2.468345284461975, 2.4647459983825684, 2.4632433652877808, 2.462624430656433, 2.4670666456222534, 2.457628846168518, 2.449965476989746, 2.4471726417541504, 2.4491759538650513, 2.4448437690734863, 2.440295934677124, 2.4396380186080933, 2.4351089000701904, 2.4311174154281616, 2.4296149015426636, 2.4276498556137085, 2.431881904602051, 2.421581745147705, 2.420931339263916, 2.4169758558273315, 2.4150192737579346, 2.4143964052200317, 2.411843419075012, 2.4065057039260864, 2.405132293701172, 2.4032872915267944, 2.399052143096924, 2.3965686559677124, 2.3932725191116333, 2.3927417993545532, 2.3896435499191284, 2.3861676454544067, 2.3839070796966553, 2.3828141689300537, 2.3789072036743164, 2.377651572227478, 2.3788869380950928, 2.372280478477478, 2.3699578046798706, 2.375513017177582, 2.3653106689453125, 2.3680429458618164, 2.360132932662964, 2.357006549835205, 2.356224775314331, 2.3519232273101807, 2.3495126962661743, 2.3470219373703003, 2.3446457386016846, 2.342089891433716, 2.3400421142578125, 2.338882803916931, 2.335044026374817, 2.3339075446128845, 2.3308541774749756, 2.3280328512191772, 2.325460433959961, 2.3315229415893555, 2.3217591047286987, 2.317566156387329, 2.3155943155288696, 2.3136098384857178, 2.311916470527649, 2.3118690252304077, 2.306247115135193, 2.3037118911743164, 2.3023457527160645, 2.2996543645858765, 2.298838973045349, 2.2952675819396973, 2.2919782400131226, 2.290449798107147, 2.2874045372009277, 2.2849963903427124, 2.2847074270248413, 2.280336618423462, 2.278567314147949, 2.2769086360931396, 2.273841917514801, 2.2706673741340637, 2.2680312395095825, 2.266085624694824, 2.265475630760193, 2.261487126350403, 2.2588744163513184, 2.257898449897766, 2.254806101322174, 2.2518622875213623, 2.249189019203186, 2.2473079562187195, 2.2456371188163757, 2.241327166557312, 2.2394962310791016, 2.2437185049057007, 2.232875943183899, 2.2345772981643677, 2.231655180454254, 2.224433481693268, 2.2219040393829346, 2.22125244140625, 2.216911792755127, 2.213249087333679, 2.2089896202087402, 2.207975149154663, 2.20486843585968, 2.202875316143036, 2.202269434928894, 2.2038044929504395, 2.1950267553329468, 2.191704273223877, 2.1919186115264893, 2.186285376548767, 2.18345046043396, 2.1803141832351685, 2.174962341785431, 2.1730260848999023, 2.1691657304763794, 2.166330933570862, 2.163829743862152, 2.1608696579933167, 2.1638704538345337, 2.1552820205688477, 2.152073562145233, 2.1498841643333435, 2.1472281217575073, 2.1492875814437866, 2.141577363014221, 2.1378663182258606, 2.135057806968689, 2.1331161856651306, 2.1292243003845215, 2.1248079538345337, 2.1236246824264526, 2.115688681602478, 2.1173689365386963, 2.1096567511558533, 2.11083722114563, 2.1032156944274902, 2.0963600873947144, 2.0916295647621155, 2.0888951420783997, 2.0848525762557983, 2.0824817419052124, 2.0792579650878906, 2.0729100704193115, 2.073031425476074, 2.0670047998428345, 2.062166392803192, 2.0600074529647827, 2.0551376342773438, 2.0502803325653076, 2.0524749159812927, 2.041222095489502, 2.034971594810486, 2.030910313129425, 2.027759611606598, 2.025147795677185, 2.019705355167389, 2.0199210047721863, 2.0124824047088623, 2.008211612701416, 2.0077051520347595, 1.9995449781417847, 1.994641125202179, 1.9880762696266174, 1.982707142829895, 1.9785178899765015, 1.970363199710846, 1.9677865505218506, 1.9606024026870728, 1.955645203590393, 1.9500704407691956, 1.9488518238067627, 1.9486325979232788, 1.9405600428581238, 1.9355000257492065, 1.9323023557662964, 1.9336810111999512, 1.9256304502487183, 1.9219188690185547, 1.9185079336166382, 1.9160537719726562, 1.9132426381111145, 1.9123388528823853, 1.903500735759735, 1.9017903804779053, 1.8960450887680054, 1.8955784440040588, 1.8922927379608154, 1.8871567845344543, 1.8838707208633423, 1.882677674293518, 1.8782787322998047, 1.8743590116500854, 1.871442973613739, 1.871152937412262, 1.8669732809066772, 1.8637937307357788, 1.8594226241111755, 1.856656014919281, 1.8537089824676514, 1.852191686630249, 1.8478071689605713, 1.8443681001663208, 1.841461181640625, 1.837877869606018, 1.8358148336410522, 1.8329958319664001, 1.8296566009521484, 1.8296734690666199, 1.8237025737762451, 1.8196606636047363, 1.8183485865592957, 1.8172576427459717, 1.8111360669136047, 1.8116174340248108, 1.8100645542144775, 1.8138424754142761, 1.8005273342132568, 1.7957100868225098, 1.7954208254814148, 1.7902705073356628, 1.7878247499465942, 1.7841066122055054, 1.7851577401161194, 1.7798498272895813, 1.7758764624595642, 1.7732551097869873, 1.7757527232170105, 1.7686594724655151, 1.7656614780426025, 1.763614296913147, 1.7595219016075134, 1.7561969757080078, 1.754259169101715, 1.7530763745307922, 1.7482845187187195, 1.7456263899803162, 1.7452067136764526, 1.74065363407135, 1.737145483493805, 1.7358811497688293, 1.732907474040985, 1.7295472025871277, 1.7277438640594482, 1.7375131249427795, 1.721667766571045, 1.719669759273529, 1.7215063571929932, 1.714479684829712, 1.7205167412757874, 1.7144684195518494, 1.7067060470581055, 1.70260089635849, 1.703744649887085, 1.6986027956008911, 1.7000976204872131, 1.699209451675415, 1.6914211511611938, 1.6866509318351746, 1.6873137950897217, 1.6844874620437622, 1.6793870329856873, 1.681868314743042, 1.6765393614768982, 1.6712422370910645, 1.6702035069465637, 1.667570948600769, 1.6637600660324097, 1.6665834188461304, 1.6605918407440186, 1.6560348272323608, 1.6570202112197876, 1.6514539122581482, 1.650028109550476, 1.6482683420181274, 1.6441648602485657, 1.6407251358032227, 1.6409100890159607, 1.6371594667434692, 1.6341086030006409, 1.6343132853507996, 1.6349668502807617, 1.628854751586914, 1.6261079907417297, 1.6285313367843628, 1.6183390617370605, 1.6163923144340515, 1.6162204146385193, 1.6116614937782288, 1.6108325719833374, 1.606916606426239, 1.603869915008545, 1.6015616655349731, 1.60135418176651, 1.5985770225524902, 1.595052719116211, 1.6037954688072205, 1.589396595954895, 1.587256371974945, 1.5871360301971436, 1.581625759601593, 1.5822736024856567, 1.5763680338859558, 1.5798284411430359, 1.5738343596458435, 1.5780984461307526, 1.56947523355484, 1.5648204684257507, 1.5625133514404297, 1.565083622932434, 1.558381736278534, 1.5602276921272278, 1.5562768578529358, 1.5519677996635437, 1.5498012900352478, 1.5473003387451172, 1.5441980957984924, 1.5429613590240479, 1.5389971733093262, 1.537580132484436, 1.5355210304260254, 1.5332524180412292, 1.5294806957244873, 1.5272995829582214, 1.5263117551803589, 1.5235201716423035, 1.519967257976532, 1.5179446935653687, 1.5156956315040588, 1.5172513127326965, 1.512111485004425, 1.5103960037231445, 1.5087583661079407, 1.5050575733184814, 1.501311182975769, 1.506235957145691, 1.4994496703147888, 1.4943541884422302, 1.496263325214386, 1.4912017583847046, 1.4868276715278625, 1.4903501272201538, 1.4874238967895508, 1.483202040195465, 1.4790299534797668, 1.477104663848877, 1.4754530191421509, 1.4765109419822693, 1.4700406789779663, 1.466282844543457, 1.46516752243042, 1.4644771814346313, 1.4607896208763123, 1.4620658159255981, 1.458970308303833, 1.4552934169769287, 1.451885461807251, 1.450065016746521, 1.448590874671936, 1.4450078010559082, 1.4426631927490234, 1.4414671659469604, 1.4421855211257935, 1.4343435764312744, 1.434648036956787, 1.4336068630218506, 1.4284198880195618, 1.4288326501846313, 1.4315937757492065, 1.4232183694839478, 1.4285116791725159, 1.4227968454360962, 1.4148930311203003, 1.4203956127166748, 1.4170066714286804, 1.4110835194587708, 1.411054253578186, 1.412151277065277, 1.4054691791534424, 1.4009659886360168, 1.4030251502990723, 1.3967559337615967, 1.3950786590576172, 1.3926897048950195, 1.3973180651664734, 1.3864786624908447, 1.3843342065811157, 1.3855475187301636, 1.3801189064979553, 1.3804090023040771, 1.3767136931419373, 1.374131441116333, 1.3724139332771301, 1.3714597821235657, 1.3674463629722595, 1.3662367463111877, 1.365654706954956, 1.3606733083724976, 1.3598332405090332, 1.359743356704712, 1.3549211621284485, 1.3547111749649048, 1.3505840301513672, 1.3499290943145752, 1.3459043502807617, 1.3477745652198792, 1.3438670635223389, 1.3393508791923523, 1.3381143808364868, 1.3364295363426208, 1.3328824043273926, 1.3355922102928162, 1.3296338319778442, 1.3272391557693481, 1.326728641986847, 1.3231525421142578, 1.3247165083885193, 1.3211968541145325, 1.3156639337539673, 1.315035879611969, 1.3144022226333618, 1.3111304640769958, 1.308436095714569, 1.3068840503692627, 1.3047042489051819, 1.3012970089912415, 1.3022115230560303, 1.2982975840568542, 1.2961041927337646, 1.3009527325630188, 1.2919699847698212, 1.2905813455581665, 1.2911661267280579, 1.289004147052765, 1.2869312763214111, 1.283851146697998, 1.2841355204582214, 1.2783098220825195, 1.274958312511444, 1.2737159132957458, 1.2732215523719788, 1.2736273407936096, 1.2713373303413391, 1.2699748277664185, 1.2654724717140198, 1.2629372477531433, 1.2627386450767517, 1.2588828802108765, 1.257112741470337, 1.2592197060585022, 1.252513349056244, 1.2524296045303345, 1.2515785694122314, 1.2449739575386047, 1.2463740110397339, 1.2459917068481445, 1.2389707565307617, 1.241374135017395, 1.2391213476657867, 1.2363284528255463, 1.2431337535381317, 1.2321862578392029, 1.2311275601387024, 1.2288926243782043, 1.2244526743888855, 1.223653495311737, 1.2204627394676208, 1.2174421548843384, 1.2178632616996765, 1.2149702906608582, 1.2120071053504944, 1.214704990386963, 1.2089283466339111, 1.2064507603645325, 1.2062050104141235, 1.2022654712200165, 1.2001734375953674, 1.203631430864334, 1.1974355578422546, 1.195968747138977, 1.193438172340393, 1.1945735216140747, 1.1908027529716492, 1.1868197917938232, 1.1901747584342957, 1.190630853176117, 1.1833546161651611, 1.1842522025108337, 1.1817740499973297, 1.1875411570072174, 1.177005648612976, 1.1873303949832916, 1.177466332912445, 1.187212586402893, 1.1735699772834778, 1.1649289727210999, 1.1660053133964539, 1.1663257777690887, 1.1603596210479736, 1.1590688824653625, 1.1579915881156921, 1.1568474173545837, 1.1545017957687378, 1.1558513641357422, 1.1504557728767395, 1.1465842723846436, 1.1467593312263489, 1.146405577659607, 1.1416182219982147, 1.142029583454132, 1.1405537724494934, 1.1405433416366577, 1.1389972269535065, 1.1327283382415771, 1.1314620971679688, 1.134972333908081, 1.128116488456726, 1.127351999282837, 1.1285279393196106, 1.123294472694397, 1.120507687330246, 1.1220489144325256, 1.1194127202033997, 1.1163907647132874, 1.116228997707367, 1.1158443093299866, 1.1124680042266846, 1.1106684803962708, 1.107879638671875, 1.106145203113556, 1.1057624816894531, 1.1022388637065887, 1.1075336337089539, 1.0980112850666046, 1.0979338884353638, 1.097021371126175, 1.0949991345405579, 1.0919248461723328, 1.0992156863212585, 1.0892512500286102, 1.089724361896515, 1.0910266637802124, 1.0840975046157837, 1.0882942080497742, 1.0822758078575134, 1.0803709626197815, 1.081454575061798, 1.0784866511821747, 1.0756814777851105, 1.0738817751407623, 1.0710503458976746, 1.069121539592743, 1.0681826770305634, 1.06789168715477, 1.0664026141166687, 1.062995970249176, 1.0607095062732697, 1.0614306330680847, 1.063039392232895, 1.0575258433818817, 1.0551009476184845, 1.0535942316055298, 1.0591270625591278, 1.0636719465255737, 1.047734260559082, 1.0510247349739075, 1.048055112361908, 1.0521055459976196, 1.0470740497112274, 1.047706514596939, 1.0446156561374664, 1.0381411015987396, 1.05134117603302, 1.0377109944820404, 1.0336889624595642, 1.0337510108947754, 1.0378762185573578, 1.029792159795761, 1.0305548310279846, 1.0260899662971497, 1.0260639190673828, 1.0227360129356384, 1.0203342139720917, 1.0203033685684204, 1.0189003646373749, 1.0229027271270752, 1.0156728029251099, 1.0163278579711914, 1.0216836631298065, 1.0140339136123657, 1.0132486820220947, 1.0107448995113373, 1.0047986507415771, 1.0053346157073975, 1.0053824484348297, 1.0009450316429138, 1.0037729144096375, 1.0022607445716858, 1.0030337870121002, 0.9951753914356232, 0.99039226770401, 0.9925822615623474, 0.9952899813652039, 1.0010226666927338, 0.9868993759155273, 0.9904371500015259, 0.9909951388835907, 0.9828968048095703, 0.9810250997543335, 0.9822503924369812], 'val_loss': [2612.506103515625, 2364.43408203125, 2123.1220703125, 1898.74609375, 1691.233154296875, 1501.209228515625, 1327.0849609375, 1170.2322998046875, 1029.5010986328125, 903.2527465820312, 792.0186157226562, 694.921630859375, 610.2282104492188, 537.14599609375, 476.7659606933594, 426.42071533203125, 384.38311767578125, 349.6092834472656, 321.22607421875, 298.04888916015625, 279.46392822265625, 264.30657958984375, 252.0885772705078, 242.1785125732422, 233.9741668701172, 227.1423797607422, 221.20639038085938, 215.9398193359375, 211.193359375, 206.6848907470703, 202.4250030517578, 198.3107147216797, 194.21893310546875, 190.13003540039062, 186.0368194580078, 181.85276794433594, 177.60113525390625, 173.38319396972656, 168.8687286376953, 164.44436645507812, 159.95046997070312, 155.4261932373047, 150.89797973632812, 145.72776794433594, 140.53289794921875, 135.5393829345703, 130.7773895263672, 126.12590026855469, 121.54789733886719, 117.1258544921875, 112.6694564819336, 108.36429595947266, 104.19218444824219, 100.11399841308594, 96.18070983886719, 92.37477111816406, 88.717529296875, 85.00831604003906, 80.6152114868164, 76.1377944946289, 71.43959045410156, 66.70020294189453, 61.733177185058594, 57.386085510253906, 54.300392150878906, 52.20412063598633, 50.97929763793945, 49.544822692871094, 47.677650451660156, 45.58555221557617, 43.18048858642578, 40.782745361328125, 38.611114501953125, 36.67963790893555, 35.01714324951172, 33.471946716308594, 32.131126403808594, 30.8798770904541, 29.73431968688965, 28.619705200195312, 27.536550521850586, 26.459569931030273, 25.44425392150879, 24.403215408325195, 23.439884185791016, 22.453197479248047, 21.5220890045166, 20.615901947021484, 19.768871307373047, 18.96962547302246, 18.28421401977539, 17.650819778442383, 17.03002166748047, 16.493135452270508, 15.939518928527832, 15.444005012512207, 14.967262268066406, 14.531454086303711, 14.117141723632812, 13.712884902954102, 13.342353820800781, 12.98248291015625, 12.656103134155273, 12.336875915527344, 12.045588493347168, 11.758649826049805, 11.513763427734375, 11.251197814941406, 11.021223068237305, 10.794374465942383, 10.591691017150879, 10.400270462036133, 10.197021484375, 10.001344680786133, 9.821892738342285, 9.665773391723633, 9.5072021484375, 9.356465339660645, 9.210380554199219, 9.062555313110352, 8.924659729003906, 8.788125991821289, 8.648664474487305, 8.539045333862305, 8.419065475463867, 8.298653602600098, 8.182348251342773, 8.082474708557129, 7.957869529724121, 7.834424018859863, 7.717828273773193, 7.604938507080078, 7.480188846588135, 7.374563694000244, 7.262840270996094, 7.159680366516113, 7.055028438568115, 6.962237358093262, 6.870344638824463, 6.780653476715088, 6.687281608581543, 6.593101501464844, 6.500048637390137, 6.4086503982543945, 6.316003322601318, 6.2288994789123535, 6.14525842666626, 6.071978569030762, 6.0007734298706055, 5.937374591827393, 5.868237495422363, 5.806366920471191, 5.7488274574279785, 5.6909708976745605, 5.636615753173828, 5.576892852783203, 5.520453453063965, 5.46408748626709, 5.415305137634277, 5.359313011169434, 5.314414024353027, 5.2620086669921875, 5.213435173034668, 5.17556619644165, 5.125554084777832, 5.0749192237854, 5.024956703186035, 4.970012664794922, 4.919609546661377, 4.874920845031738, 4.831878662109375, 4.7978620529174805, 4.762495994567871, 4.727712631225586, 4.688094615936279, 4.642591953277588, 4.6072773933410645, 4.574069976806641, 4.5404534339904785, 4.506742477416992, 4.475714683532715, 4.445850849151611, 4.414769172668457, 4.38736629486084, 4.35610294342041, 4.322197437286377, 4.292422294616699, 4.265346527099609, 4.240296840667725, 4.215188980102539, 4.189755916595459, 4.156159400939941, 4.129906177520752, 4.101481914520264, 4.0749101638793945, 4.051799774169922, 4.025789737701416, 3.9999098777770996, 3.9782767295837402, 3.9579429626464844, 3.938089370727539, 3.920396327972412, 3.900634765625, 3.8852286338806152, 3.871474027633667, 3.8546741008758545, 3.8390960693359375, 3.8254554271698, 3.8054261207580566, 3.78833270072937, 3.7707324028015137, 3.7558913230895996, 3.7428345680236816, 3.7292518615722656, 3.7197253704071045, 3.7073123455047607, 3.696267604827881, 3.6861891746520996, 3.676548719406128, 3.6660244464874268, 3.657761335372925, 3.6478729248046875, 3.6412558555603027, 3.6304497718811035, 3.6187081336975098, 3.6093509197235107, 3.600114345550537, 3.5878517627716064, 3.5793235301971436, 3.5670509338378906, 3.5591468811035156, 3.5511550903320312, 3.5436489582061768, 3.5324158668518066, 3.5236973762512207, 3.5143790245056152, 3.506652355194092, 3.500666856765747, 3.4988136291503906, 3.4952971935272217, 3.480017900466919, 3.4666836261749268, 3.4569802284240723, 3.4483559131622314, 3.4398257732391357, 3.435082197189331, 3.429727077484131, 3.425497055053711, 3.417705535888672, 3.406613826751709, 3.396388530731201, 3.386620283126831, 3.377962827682495, 3.3708579540252686, 3.364394426345825, 3.3562710285186768, 3.3471932411193848, 3.3358986377716064, 3.326709032058716, 3.3177618980407715, 3.3117241859436035, 3.3039698600769043, 3.2939159870147705, 3.2834200859069824, 3.2746505737304688, 3.2688632011413574, 3.2641422748565674, 3.2552170753479004, 3.248775005340576, 3.2419419288635254, 3.236701250076294, 3.231149196624756, 3.224543809890747, 3.215198040008545, 3.2047533988952637, 3.1989314556121826, 3.1899971961975098, 3.1839327812194824, 3.1790051460266113, 3.1683554649353027, 3.1626343727111816, 3.1500484943389893, 3.1415979862213135, 3.135559558868408, 3.1319680213928223, 3.1242451667785645, 3.1128180027008057, 3.1027679443359375, 3.0926575660705566, 3.085752010345459, 3.078993082046509, 3.074326276779175, 3.0713658332824707, 3.0658326148986816, 3.055893659591675, 3.046767234802246, 3.03813099861145, 3.031761646270752, 3.0269687175750732, 3.0186500549316406, 3.010106325149536, 3.0019497871398926, 2.996079206466675, 2.9894330501556396, 2.983396053314209, 2.978623390197754, 2.970581531524658, 2.9636101722717285, 2.958811044692993, 2.952974319458008, 2.946547269821167, 2.939459800720215, 2.9321794509887695, 2.9248085021972656, 2.9203994274139404, 2.914485454559326, 2.906797409057617, 2.900381088256836, 2.8962349891662598, 2.8911423683166504, 2.8867745399475098, 2.883756160736084, 2.879619836807251, 2.873051166534424, 2.867568016052246, 2.860379934310913, 2.8536081314086914, 2.848813772201538, 2.8454749584198, 2.8420419692993164, 2.836796998977661, 2.831059217453003, 2.8286209106445312, 2.8226776123046875, 2.81801438331604, 2.8119072914123535, 2.806532382965088, 2.800797700881958, 2.7956032752990723, 2.7915241718292236, 2.78617000579834, 2.7798378467559814, 2.775177001953125, 2.7700014114379883, 2.765476703643799, 2.7614338397979736, 2.7593226432800293, 2.7558627128601074, 2.7529642581939697, 2.7462146282196045, 2.7416369915008545, 2.7362492084503174, 2.7308006286621094, 2.727341651916504, 2.7231242656707764, 2.719816207885742, 2.717594861984253, 2.71364164352417, 2.7092511653900146, 2.7044129371643066, 2.700082778930664, 2.6950552463531494, 2.690429449081421, 2.6848251819610596, 2.6809167861938477, 2.674978733062744, 2.6709601879119873, 2.6671690940856934, 2.6637587547302246, 2.6624927520751953, 2.6555397510528564, 2.6508052349090576, 2.646692991256714, 2.639888048171997, 2.6349289417266846, 2.62937593460083, 2.6237874031066895, 2.6183547973632812, 2.6124751567840576, 2.605555772781372, 2.598909854888916, 2.593644618988037, 2.5878140926361084, 2.5826313495635986, 2.5777175426483154, 2.571699857711792, 2.565077066421509, 2.5605101585388184, 2.5548088550567627, 2.5462164878845215, 2.539743423461914, 2.533842086791992, 2.5258350372314453, 2.5200881958007812, 2.515514850616455, 2.5097289085388184, 2.5061943531036377, 2.5044219493865967, 2.4993903636932373, 2.4966132640838623, 2.4936983585357666, 2.487330198287964, 2.482430934906006, 2.478086233139038, 2.473008871078491, 2.470000743865967, 2.4636478424072266, 2.4594597816467285, 2.453793525695801, 2.448904514312744, 2.445388078689575, 2.4407310485839844, 2.4388198852539062, 2.4338221549987793, 2.4292235374450684, 2.4255900382995605, 2.4235482215881348, 2.4194085597991943, 2.4139695167541504, 2.4119083881378174, 2.4089348316192627, 2.4061279296875, 2.4017462730407715, 2.3993031978607178, 2.3963708877563477, 2.3971786499023438, 2.392965316772461, 2.3905982971191406, 2.3841347694396973, 2.3805718421936035, 2.380420207977295, 2.3779048919677734, 2.372514247894287, 2.367788314819336, 2.3653578758239746, 2.3641715049743652, 2.3616816997528076, 2.360010862350464, 2.3572094440460205, 2.3518688678741455, 2.347438097000122, 2.344871997833252, 2.343341588973999, 2.3391923904418945, 2.33555269241333, 2.3304615020751953, 2.3261337280273438, 2.322993278503418, 2.321948528289795, 2.320953369140625, 2.3188867568969727, 2.3142311573028564, 2.3131518363952637, 2.309304714202881, 2.3061161041259766, 2.304457187652588, 2.302170991897583, 2.3021130561828613, 2.2970266342163086, 2.293468952178955, 2.2888169288635254, 2.285604476928711, 2.2820277214050293, 2.2789711952209473, 2.2744033336639404, 2.2718091011047363, 2.266111373901367, 2.262598752975464, 2.2605807781219482, 2.2590510845184326, 2.255096912384033, 2.2529823780059814, 2.246474266052246, 2.24239182472229, 2.2390716075897217, 2.2377490997314453, 2.2382969856262207, 2.2374911308288574, 2.2343974113464355, 2.2320799827575684, 2.2248458862304688, 2.2184226512908936, 2.213113784790039, 2.2098774909973145, 2.2127304077148438, 2.202613592147827, 2.198309898376465, 2.1926369667053223, 2.186551570892334, 2.182931661605835, 2.1798698902130127, 2.176147937774658, 2.1716694831848145, 2.1655232906341553, 2.1580166816711426, 2.1554298400878906, 2.1560986042022705, 2.155330181121826, 2.1495752334594727, 2.1452879905700684, 2.139845848083496, 2.135213851928711, 2.1362082958221436, 2.132458209991455, 2.1255247592926025, 2.1192166805267334, 2.1133358478546143, 2.1095612049102783, 2.1071760654449463, 2.1020359992980957, 2.101264476776123, 2.099008083343506, 2.0964276790618896, 2.0951600074768066, 2.0884904861450195, 2.0866363048553467, 2.0859198570251465, 2.083548069000244, 2.078352689743042, 2.075117588043213, 2.0711207389831543, 2.0694007873535156, 2.062915086746216, 2.0602688789367676, 2.0572636127471924, 2.0592007637023926, 2.050581932067871, 2.044368267059326, 2.040828227996826, 2.0351169109344482, 2.0324223041534424, 2.0306358337402344, 2.026893138885498, 2.022193431854248, 2.0177042484283447, 2.017009973526001, 2.018582820892334, 2.019460678100586, 2.0128390789031982, 2.0107529163360596, 2.0058646202087402, 2.000920295715332, 1.9992386102676392, 1.9968059062957764, 1.9926116466522217, 1.9862744808197021, 1.9796288013458252, 1.9766530990600586, 1.9701331853866577, 1.9658901691436768, 1.9654979705810547, 1.960626244544983, 1.9600918292999268, 1.9589916467666626, 1.9573971033096313, 1.9599183797836304, 1.9597094058990479, 1.9562867879867554, 1.9531269073486328, 1.9476158618927002, 1.9399646520614624, 1.9315048456192017, 1.926414132118225, 1.91765558719635, 1.9130862951278687, 1.912025809288025, 1.909210205078125, 1.9085986614227295, 1.9056720733642578, 1.9026073217391968, 1.9026371240615845, 1.8996353149414062, 1.89572274684906, 1.8922672271728516, 1.8842480182647705, 1.8797401189804077, 1.8760693073272705, 1.8723201751708984, 1.8681972026824951, 1.8652217388153076, 1.8616451025009155, 1.8583190441131592, 1.8541412353515625, 1.848815679550171, 1.8449558019638062, 1.8404271602630615, 1.8376057147979736, 1.8327157497406006, 1.830440878868103, 1.8261375427246094, 1.822461724281311, 1.8203675746917725, 1.8163011074066162, 1.8130966424942017, 1.8105045557022095, 1.806606650352478, 1.8041702508926392, 1.8000609874725342, 1.7982139587402344, 1.7949196100234985, 1.7933286428451538, 1.7922004461288452, 1.787571668624878, 1.783084511756897, 1.7818851470947266, 1.7737395763397217, 1.7675654888153076, 1.7631019353866577, 1.7610504627227783, 1.758252739906311, 1.7553850412368774, 1.7519245147705078, 1.7472813129425049, 1.7432279586791992, 1.7387340068817139, 1.7339855432510376, 1.7294361591339111, 1.7266486883163452, 1.723394751548767, 1.7218433618545532, 1.7176809310913086, 1.715778112411499, 1.7122176885604858, 1.7087589502334595, 1.7041537761688232, 1.7000305652618408, 1.69709050655365, 1.6933670043945312, 1.6909058094024658, 1.6873966455459595, 1.6856199502944946, 1.680871605873108, 1.6772559881210327, 1.6747043132781982, 1.6701892614364624, 1.6657841205596924, 1.6682716608047485, 1.663987159729004, 1.656654953956604, 1.6591907739639282, 1.6514946222305298, 1.649929404258728, 1.6508220434188843, 1.6461429595947266, 1.6398931741714478, 1.6375758647918701, 1.635298490524292, 1.630069375038147, 1.6314942836761475, 1.6280796527862549, 1.6209230422973633, 1.6163263320922852, 1.6141389608383179, 1.6105668544769287, 1.6099860668182373, 1.608097791671753, 1.6046890020370483, 1.6000210046768188, 1.5973565578460693, 1.59214186668396, 1.590308427810669, 1.5853245258331299, 1.5833148956298828, 1.5788867473602295, 1.5770916938781738, 1.5763680934906006, 1.5744705200195312, 1.572035551071167, 1.5668973922729492, 1.5620012283325195, 1.5574610233306885, 1.55599045753479, 1.5582897663116455, 1.5576179027557373, 1.5486171245574951, 1.5460364818572998, 1.5446416139602661, 1.5437133312225342, 1.5394237041473389, 1.5365586280822754, 1.5329447984695435, 1.5313808917999268, 1.5299831628799438, 1.525984287261963, 1.5183212757110596, 1.5169076919555664, 1.5157325267791748, 1.510765790939331, 1.5084596872329712, 1.5072771310806274, 1.5043714046478271, 1.5032627582550049, 1.4999206066131592, 1.497077465057373, 1.4949895143508911, 1.4961845874786377, 1.4897278547286987, 1.4855047464370728, 1.4834976196289062, 1.480132818222046, 1.4770599603652954, 1.4786529541015625, 1.4734649658203125, 1.469578504562378, 1.466533899307251, 1.4642703533172607, 1.4621257781982422, 1.4598041772842407, 1.4566034078598022, 1.4543853998184204, 1.4500133991241455, 1.4488399028778076, 1.4460527896881104, 1.441554307937622, 1.4386489391326904, 1.4349428415298462, 1.4326144456863403, 1.4302363395690918, 1.426693320274353, 1.4235944747924805, 1.4228092432022095, 1.4231586456298828, 1.4217712879180908, 1.417427659034729, 1.4153053760528564, 1.4118382930755615, 1.4083172082901, 1.4074456691741943, 1.408233404159546, 1.4014499187469482, 1.3998931646347046, 1.4002758264541626, 1.3978605270385742, 1.397144079208374, 1.3937370777130127, 1.3913688659667969, 1.383726716041565, 1.3802632093429565, 1.3771021366119385, 1.3762178421020508, 1.3741099834442139, 1.3696472644805908, 1.3663173913955688, 1.3658610582351685, 1.3620703220367432, 1.3616176843643188, 1.3609483242034912, 1.3628274202346802, 1.359626293182373, 1.3527649641036987, 1.3492568731307983, 1.3447011709213257, 1.340714454650879, 1.3430516719818115, 1.3419221639633179, 1.335481882095337, 1.332794189453125, 1.3292970657348633, 1.3259449005126953, 1.3257988691329956, 1.3304328918457031, 1.3246458768844604, 1.3210129737854004, 1.3216395378112793, 1.3154513835906982, 1.3197832107543945, 1.3155932426452637, 1.3068382740020752, 1.3039785623550415, 1.3023449182510376, 1.3030173778533936, 1.3002737760543823, 1.296012282371521, 1.2947114706039429, 1.289853811264038, 1.288414716720581, 1.287695288658142, 1.281996488571167, 1.2805532217025757, 1.2782392501831055, 1.275633692741394, 1.2730417251586914, 1.2722278833389282, 1.2697474956512451, 1.2691409587860107, 1.2667348384857178, 1.265214204788208, 1.2597521543502808, 1.257407546043396, 1.2563692331314087, 1.2539091110229492, 1.2511476278305054, 1.2490936517715454, 1.2455775737762451, 1.2444493770599365, 1.243046522140503, 1.240952968597412, 1.2394635677337646, 1.238385796546936, 1.2350327968597412, 1.2309868335723877, 1.2269562482833862, 1.2251962423324585, 1.2228665351867676, 1.222599983215332, 1.217267394065857, 1.2170915603637695, 1.2142480611801147, 1.2151926755905151, 1.2171218395233154, 1.2102376222610474, 1.2056500911712646, 1.2030580043792725, 1.200057029724121, 1.1986262798309326, 1.1978718042373657, 1.1969417333602905, 1.1897354125976562, 1.1884562969207764, 1.1864022016525269, 1.1864980459213257, 1.1909515857696533, 1.184826374053955, 1.1776940822601318, 1.174964189529419, 1.1708838939666748, 1.1726032495498657, 1.1705316305160522, 1.1677030324935913, 1.1644567251205444, 1.1643272638320923, 1.1610808372497559, 1.1631336212158203, 1.1563045978546143, 1.155449390411377, 1.152048110961914, 1.1485220193862915, 1.145777940750122, 1.1508817672729492, 1.1454111337661743, 1.1368505954742432, 1.1376125812530518, 1.1331021785736084, 1.1375672817230225, 1.1419050693511963, 1.1314218044281006, 1.1282126903533936, 1.1253020763397217, 1.1221368312835693, 1.1225483417510986, 1.1200004816055298, 1.1143419742584229, 1.114606261253357, 1.1153932809829712, 1.1183441877365112, 1.1150614023208618, 1.108203411102295, 1.1037414073944092, 1.0987846851348877, 1.0983470678329468, 1.0980231761932373, 1.0935227870941162, 1.0930392742156982, 1.095066785812378, 1.0894439220428467, 1.0873783826828003, 1.0846774578094482, 1.0849746465682983, 1.0821491479873657, 1.0777208805084229, 1.0760996341705322, 1.077709436416626, 1.0749523639678955, 1.0696563720703125, 1.0677049160003662, 1.0666958093643188, 1.0718305110931396, 1.0675188302993774, 1.0578069686889648, 1.0574101209640503, 1.0538017749786377, 1.0675742626190186, 1.0547378063201904, 1.0529149770736694, 1.0472261905670166, 1.0490868091583252, 1.0530519485473633, 1.0437999963760376, 1.0390675067901611, 1.036354422569275, 1.0373903512954712, 1.0382463932037354, 1.0330259799957275, 1.029423713684082, 1.027478814125061, 1.0280258655548096, 1.0268990993499756, 1.021884560585022, 1.0199273824691772, 1.0184612274169922, 1.0176206827163696, 1.0199953317642212, 1.0128449201583862, 1.0095508098602295, 1.006793737411499, 1.008447527885437, 1.0022201538085938, 0.9982602000236511, 0.9990726709365845, 0.9956720471382141, 0.9945561289787292, 0.9949043393135071, 0.9896863698959351, 0.9894715547561646, 0.988368809223175, 0.9920849800109863, 0.9840715527534485, 0.9780084490776062, 0.9766653180122375, 0.9751163721084595, 0.9726384878158569, 0.9742380380630493, 0.9712621569633484, 0.972223162651062, 0.9707251787185669, 0.9695459604263306, 0.9657667279243469, 0.9650881886482239, 0.9636009931564331, 0.9659101366996765, 0.9689962267875671, 0.9599087834358215, 0.9569278955459595, 0.9566957354545593, 0.9553318023681641, 0.9628974199295044, 0.9544048309326172, 0.949088454246521, 0.9474004507064819, 0.9465928077697754, 0.9439166784286499, 0.9425836801528931, 0.937098503112793, 0.9322507977485657, 0.9333568811416626, 0.9329312443733215, 0.9325295686721802, 0.9331918954849243, 0.9286224246025085, 0.9286022186279297, 0.9292155504226685, 0.9251086115837097, 0.9174625277519226, 0.9151331186294556, 0.9237165451049805, 0.9231983423233032, 0.9121412038803101, 0.913111686706543, 0.9119192361831665, 0.9259843826293945, 0.9154269099235535, 0.9086549878120422, 0.9092413783073425, 0.9104512333869934, 0.9113296270370483, 0.9038301706314087, 0.8976762890815735, 0.8946442604064941, 0.8983680605888367, 0.8924365043640137, 0.890485942363739, 0.8894813656806946, 0.892491340637207, 0.8924331665039062, 0.8847742080688477, 0.8838192224502563, 0.8834125399589539, 0.8819023966789246, 0.8906837701797485, 0.8845885992050171, 0.8794683218002319, 0.8783790469169617, 0.880767822265625, 0.8870518803596497, 0.8776591420173645, 0.8730783462524414, 0.8744803667068481, 0.8720306158065796, 0.8795652389526367, 0.8718141317367554, 0.8723028898239136, 0.8704534769058228, 0.8708421587944031, 0.8721355199813843, 0.8642457723617554, 0.8656549453735352, 0.8616388440132141, 0.8606166839599609, 0.8591473698616028, 0.8537837862968445]}\n"
     ]
    }
   ],
   "source": [
    "print(history.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出history，我们可以发现history的值是字典类型（dict），key为`loss`和`val_loss`，`loss`的值为列表。用`history.history['loss'][0]`得到其中的某一个值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2323.8714599609375\n"
     ]
    }
   ],
   "source": [
    "print(history.history['loss'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将history中的数据通过matplotlib绘图表现出来，能够更加直观地看出loss的变化规律。因为在jupyter上调试，代码中加入%matplotlib inline命令，让图片输出在网页中。下面的图中，将训练过程中的训练集loss以及验证集loss全部输出，并打印出测试集的loss。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 0s 201us/step\n",
      "test_loss: 1.0507547855377197\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#将图片内嵌在交互窗口，而不是弹出一个图片窗口\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "t=len(history.history['loss'])\n",
    "plt.plot(range(t),history.history['loss'])\n",
    "plt.plot(range(t),history.history['val_loss'])\n",
    "print(\"test_loss:\",model.evaluate(x_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们如何评价一个模型训练是否成功？首先，训练过程中训练集loss要下降到一个较小的值，表示模型收敛较好，没有欠拟合；其次，测试集loss最后与训练集loss要尽可能相似，差距越小越好小，说明该模型没有过拟合。\n",
    "\n",
    "当一个神经网络模型成功训练出来后，便可以使用该模型进行预测了。通过pandas的DataFrame方法构造x_input，并使用模型的`predict`方法进行预测。这里的数据是根据`Advertising.csv`的前三条略加修改的，可以看看这个模型输出的结果与真实结果（sales列）是否一致。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[21.8746  ],\n",
       "       [10.844945],\n",
       "       [ 8.222065]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#用字典生成的DataFrame，需要指定一下列的次序\n",
    "d={'TV':[230,44,17],'radio':[37,39,45],'newspaper':[69,45,69]}\n",
    "x_input=pd.DataFrame(d,columns=['TV','radio','newspaper'])\n",
    "model.predict(x_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TV</th>\n",
       "      <th>radio</th>\n",
       "      <th>newspaper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>230</td>\n",
       "      <td>37</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44</td>\n",
       "      <td>39</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17</td>\n",
       "      <td>45</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    TV  radio  newspaper\n",
       "0  230     37         69\n",
       "1   44     39         45\n",
       "2   17     45         69"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用输出来的结果和前三条数据比较（原来数据中的`sales`分别为：22，10，9），看起来效果还是不错的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.保存模型\n",
    "\n",
    "训练出来的模型，可以保存。下次使用的时候载入，还可以继续训练。一般保存为h5格式，需要先安装h5py。\n",
    "\n",
    "命令如下：pip install h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./model/1-model-vv.h5')   # HDF5文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.应用模型\n",
    "\n",
    "使用keras.models的load_model语句载入模型，就可以直接用这个模型来做预测了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "import pandas as pd\n",
    "model = load_model('./model/1-model-vv.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这段代码将在`data`文件夹中生成一个名为`test.csv`的文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./data/test.csv\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./data/test.csv\n",
    "TV,radio,newspaper\n",
    "230.0,37.0,69.0\n",
    "44,39,45\n",
    "17,45,69\n",
    "283.1,42.1,66.1\n",
    "232.1,8.6,8.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[21.279892],\n",
       "       [10.489032],\n",
       "       [ 9.173268],\n",
       "       [24.312817],\n",
       "       [13.093816]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#读取数据，并且输出预测结果\n",
    "x_input=pd.read_csv('./data/test.csv')\n",
    "model.predict(x_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注：**可以导入模型后，继续训练，直到loss不会继续变小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
