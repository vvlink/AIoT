{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 神经网络和机器学习之鸢尾花分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "案例说明：鸢尾花(Iris)分类，使用全连接神经网络层。\n",
    "\n",
    "鸢(yuān)尾花分类相当于机器学习中的Helloworld问题。鸢尾花可以分为很多类，一般通过花萼长度、花萼宽度、花瓣长度、花瓣宽度进行区分。我们让机器来学习关于这个鸢尾花分类的一组数据，然后建立模型，训练。后面直接给出花的四个特征，让机器判断花的分类。\n",
    "\n",
    "案例选择了keras框架，需要先安装keras和tensorflow。虚谷号教育版已经预装必要的库，可以直接使用。\n",
    "\n",
    "本案例已经提供了训练好的模型，文件名称为：2-model-vv.h5。如果想直接测试模型，请跳到“导入模型”环节，输入数据开始识别。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.环境搭建\n",
    "\n",
    "下面是安装命令：\n",
    "\n",
    "pip install keras\n",
    "\n",
    "pip install -i https://pypi.tuna.tsinghua.edu.cn/simple tensorflow\n",
    "\n",
    "建议选择清华源，速度将快很多。参考命令如下：\n",
    "\n",
    "pip install -i https://pypi.tuna.tsinghua.edu.cn/simple tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.数据准备\n",
    "\n",
    "\n",
    "鸢尾花分类数据集在`data`中，文件名称为`iris.csv`。数据分为5列，前4列为花萼长度，花萼宽度，花瓣长度，花瓣宽度等4个用于识别鸢尾花的属性，第5列为鸢尾花的类别（包括Setosa，Versicolour，Virginica三类）。\n",
    "\n",
    "这个数据集可以从UCI数据集上直接下载，具体地址为：http://archive.ics.uci.edu/ml/datasets/Iris\n",
    "打开页面后点击Datafolder就可以下载到本地磁盘上，默认格式为逗号分隔的文本文件。\n",
    "\n",
    "也可以直接从sklearn包里datasets里导入，语法为：from sklearn.datasets import load_iris。\n",
    "如果从本地磁盘上读入该数据集，可以采用pandas包里的read_excel或者read_csv方法，也可以利用python里面的csv包来处理。\n",
    "\n",
    "开始导入数据集吧。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data=pd.read_csv('./data/iris.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "该问题属于比较典型的多分类问题，因此在训练数据预处理中，首先对分类结果标签\"Species\"进行独热编码化。所谓独热编码(One-Hot)，是指用0/1构成的数组来表示一种情况，比如在鸢尾花分类中，顺序编码可以用0、1、2来表示不同的鸢尾花品种，而独热编码可以用[1,0,0]表示setosa，用[0,1,0]表示versicolor,用[0,0,1]表示virginica。独热编码相对于顺序编码避免了神经网络把没有数值大小意义的数据错误的理解为有数值意义。比如如果用顺序编码来表示鸢尾花品种，神经网络会错误的认为2表示的品种与0表示的品种之间的差距比较大，而与1表示的品种差距比较小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.get_dummies(data,columns=['Species']) #把种类(列名称为“Species”)进行独热编码\n",
    "x=data[['Sepal.Length', 'Sepal.Width', 'Petal.Length','Petal.Width']]\n",
    "y=data.iloc[:,-3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此时x与y的形状分别是(150,3)和(150,1)，即x具有150行、4列，y具有150行、1列。其中x是输入的数据(鸢尾花的属性)，y是输出的结果(鸢尾花的类别)。\n",
    "输出来看一下，你会发现x与y的行数一定是相同的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sepal.Length</th>\n",
       "      <th>Sepal.Width</th>\n",
       "      <th>Petal.Length</th>\n",
       "      <th>Petal.Width</th>\n",
       "      <th>Species_Iris-setosa</th>\n",
       "      <th>Species_Iris-versicolor</th>\n",
       "      <th>Species_Iris-virginica</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sepal.Length  Sepal.Width  Petal.Length  Petal.Width  Species_Iris-setosa  \\\n",
       "0           5.1          3.5           1.4          0.2                    1   \n",
       "1           4.9          3.0           1.4          0.2                    1   \n",
       "2           4.7          3.2           1.3          0.2                    1   \n",
       "3           4.6          3.1           1.5          0.2                    1   \n",
       "4           5.0          3.6           1.4          0.2                    1   \n",
       "\n",
       "   Species_Iris-versicolor  Species_Iris-virginica  \n",
       "0                        0                       0  \n",
       "1                        0                       0  \n",
       "2                        0                       0  \n",
       "3                        0                       0  \n",
       "4                        0                       0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 输出data的数据开头5行\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sepal.Length</th>\n",
       "      <th>Sepal.Width</th>\n",
       "      <th>Petal.Length</th>\n",
       "      <th>Petal.Width</th>\n",
       "      <th>Species_Iris-setosa</th>\n",
       "      <th>Species_Iris-versicolor</th>\n",
       "      <th>Species_Iris-virginica</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Sepal.Length  Sepal.Width  Petal.Length  Petal.Width  \\\n",
       "145           6.7          3.0           5.2          2.3   \n",
       "146           6.3          2.5           5.0          1.9   \n",
       "147           6.5          3.0           5.2          2.0   \n",
       "148           6.2          3.4           5.4          2.3   \n",
       "149           5.9          3.0           5.1          1.8   \n",
       "\n",
       "     Species_Iris-setosa  Species_Iris-versicolor  Species_Iris-virginica  \n",
       "145                    0                        0                       1  \n",
       "146                    0                        0                       1  \n",
       "147                    0                        0                       1  \n",
       "148                    0                        0                       1  \n",
       "149                    0                        0                       1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 输出data的数据最后5行\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sepal.Length</th>\n",
       "      <th>Sepal.Width</th>\n",
       "      <th>Petal.Length</th>\n",
       "      <th>Petal.Width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n",
       "0           5.1          3.5           1.4          0.2\n",
       "1           4.9          3.0           1.4          0.2\n",
       "2           4.7          3.2           1.3          0.2\n",
       "3           4.6          3.1           1.5          0.2\n",
       "4           5.0          3.6           1.4          0.2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 输出x数据的最后5行\n",
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 150)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 输出x、y的长度，肯定是一致的。\n",
    "len(x),len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.建立模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多分类问题是二分类问题的扩展。当分类数大于2时，就是多分类问题。比如把笔分成铅笔、圆珠笔、钢笔等等，就是多分类问题。多分类问题需要神经网络将最后一层神经元个数设置为与分类数目相同以输出一个数组，这个数组的长度就是分类数目，数组中每个数值对应在不同类别上的可能性。一般的，多分类问题通过softmax函数激活，损失函数使用类别交叉熵损失(categorical_crossentropy)。\n",
    "\n",
    "keras支持很多类型的神经网络层，这里使用add方法添加2个全连接神经网络层（Dense层）。\n",
    "第一层通过input_dim参数指定接收输入数据的维度为4（鸢尾花的属性），units=8表示将这个4维数据全连接到8个神经元，activation定义了激活函数为relu。第二层神经元，也就是最后一层神经元的个数设置要和分类的数目相同，所以设置为3，激活函数为softmax。\n",
    "\n",
    "代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=keras.models.Sequential()\n",
    "model.add(layers.Dense(units=8, input_dim=4, activation='relu'))\n",
    "model.add(layers.Dense(units=3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义好模型的层之后，需要对模型进行编译，同时指定训练模型所需要的优化器以及损失的估算方法。在keras中，可以通过optimizer参数来指定优化器。这里选择了adam。loss定义了损失函数为category_crossentropy。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后对模型进行训练，一下代码利用现有数据x和y对模型进行训练500次，epochs表示训练轮次，batch_size表示每次有多少行数据参与训练，最后把整个训练过程记录到history中。程序运行后，在控制台会打印出每轮次的训练情况。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "1/1 [==============================] - 15s 15s/step - loss: 5.0220\n",
      "Epoch 2/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 4.9722\n",
      "Epoch 3/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 4.9225\n",
      "Epoch 4/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 4.8728\n",
      "Epoch 5/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4.8231\n",
      "Epoch 6/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 4.7735\n",
      "Epoch 7/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 4.7240\n",
      "Epoch 8/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 4.6746\n",
      "Epoch 9/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 4.6253\n",
      "Epoch 10/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 4.5762\n",
      "Epoch 11/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 4.5271\n",
      "Epoch 12/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 4.4783\n",
      "Epoch 13/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 4.4295\n",
      "Epoch 14/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 4.3809\n",
      "Epoch 15/500\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 4.3323\n",
      "Epoch 16/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4.2839\n",
      "Epoch 17/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 4.2355\n",
      "Epoch 18/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4.1874\n",
      "Epoch 19/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 4.1394\n",
      "Epoch 20/500\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 4.0915\n",
      "Epoch 21/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 4.0439\n",
      "Epoch 22/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 3.9964\n",
      "Epoch 23/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 3.9489\n",
      "Epoch 24/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.9017\n",
      "Epoch 25/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.8546\n",
      "Epoch 26/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.8077\n",
      "Epoch 27/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.7609\n",
      "Epoch 28/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.7144\n",
      "Epoch 29/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 3.6680\n",
      "Epoch 30/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 3.6219\n",
      "Epoch 31/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 3.5760\n",
      "Epoch 32/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.5303\n",
      "Epoch 33/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.4848\n",
      "Epoch 34/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.4396\n",
      "Epoch 35/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.3946\n",
      "Epoch 36/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.3499\n",
      "Epoch 37/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.3055\n",
      "Epoch 38/500\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 3.2614\n",
      "Epoch 39/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.2175\n",
      "Epoch 40/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.1740\n",
      "Epoch 41/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 3.1307\n",
      "Epoch 42/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.0878\n",
      "Epoch 43/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.0452\n",
      "Epoch 44/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.0030\n",
      "Epoch 45/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.9611\n",
      "Epoch 46/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.9196\n",
      "Epoch 47/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.8785\n",
      "Epoch 48/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.8377\n",
      "Epoch 49/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.7974\n",
      "Epoch 50/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.7573\n",
      "Epoch 51/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.7175\n",
      "Epoch 52/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.678 - 0s 12ms/step - loss: 2.6780\n",
      "Epoch 53/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.6389\n",
      "Epoch 54/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.6001\n",
      "Epoch 55/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.5617\n",
      "Epoch 56/500\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 2.5238\n",
      "Epoch 57/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 2.4863\n",
      "Epoch 58/500\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 2.4491\n",
      "Epoch 59/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.4122\n",
      "Epoch 60/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.3758\n",
      "Epoch 61/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.3397\n",
      "Epoch 62/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.3041\n",
      "Epoch 63/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.2690\n",
      "Epoch 64/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.2343\n",
      "Epoch 65/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.2000\n",
      "Epoch 66/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.1661\n",
      "Epoch 67/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.1326\n",
      "Epoch 68/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.0994\n",
      "Epoch 69/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.0666\n",
      "Epoch 70/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.0341\n",
      "Epoch 71/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.0021\n",
      "Epoch 72/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.9703\n",
      "Epoch 73/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.9390\n",
      "Epoch 74/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.9079\n",
      "Epoch 75/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.8773\n",
      "Epoch 76/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.8470\n",
      "Epoch 77/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.8171\n",
      "Epoch 78/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.7876\n",
      "Epoch 79/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.7585\n",
      "Epoch 80/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.7298\n",
      "Epoch 81/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.7015\n",
      "Epoch 82/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.6738\n",
      "Epoch 83/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.6465\n",
      "Epoch 84/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.6199\n",
      "Epoch 85/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.5938\n",
      "Epoch 86/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.5682\n",
      "Epoch 87/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.5431\n",
      "Epoch 88/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.5186\n",
      "Epoch 89/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.4946\n",
      "Epoch 90/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.4712\n",
      "Epoch 91/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.4484\n",
      "Epoch 92/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.4263\n",
      "Epoch 93/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.4048\n",
      "Epoch 94/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.3839\n",
      "Epoch 95/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.3638\n",
      "Epoch 96/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.3444\n",
      "Epoch 97/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.3258\n",
      "Epoch 98/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.3079\n",
      "Epoch 99/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.2909\n",
      "Epoch 100/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.2746\n",
      "Epoch 101/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.2592\n",
      "Epoch 102/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.2446\n",
      "Epoch 103/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.2308\n",
      "Epoch 104/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.2177\n",
      "Epoch 105/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.2054\n",
      "Epoch 106/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.1938\n",
      "Epoch 107/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1829\n",
      "Epoch 108/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1726\n",
      "Epoch 109/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.1631\n",
      "Epoch 110/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1541\n",
      "Epoch 111/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.1457\n",
      "Epoch 112/500\n",
      "1/1 [==============================] - 0s 330ms/step - loss: 1.1379\n",
      "Epoch 113/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.1306\n",
      "Epoch 114/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.1238\n",
      "Epoch 115/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.1175\n",
      "Epoch 116/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.1115\n",
      "Epoch 117/500\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.1059\n",
      "Epoch 118/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.1007\n",
      "Epoch 119/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 1.0957\n",
      "Epoch 120/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0911\n",
      "Epoch 121/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.0867\n",
      "Epoch 122/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.0824\n",
      "Epoch 123/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.0784\n",
      "Epoch 124/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0744\n",
      "Epoch 125/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.0706\n",
      "Epoch 126/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.0669\n",
      "Epoch 127/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.0633\n",
      "Epoch 128/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0597\n",
      "Epoch 129/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.0562\n",
      "Epoch 130/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0528\n",
      "Epoch 131/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0494\n",
      "Epoch 132/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.0460\n",
      "Epoch 133/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.0426\n",
      "Epoch 134/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0392\n",
      "Epoch 135/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0358\n",
      "Epoch 136/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.0325\n",
      "Epoch 137/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.0292\n",
      "Epoch 138/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0258\n",
      "Epoch 139/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.0225\n",
      "Epoch 140/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.0192\n",
      "Epoch 141/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0159\n",
      "Epoch 142/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.0126\n",
      "Epoch 143/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.0093\n",
      "Epoch 144/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0061\n",
      "Epoch 145/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.0028\n",
      "Epoch 146/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.9996\n",
      "Epoch 147/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.9964\n",
      "Epoch 148/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.9931\n",
      "Epoch 149/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.9900\n",
      "Epoch 150/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.9868\n",
      "Epoch 151/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.9836\n",
      "Epoch 152/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.9805\n",
      "Epoch 153/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.9774\n",
      "Epoch 154/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.9743\n",
      "Epoch 155/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.9713\n",
      "Epoch 156/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.9682\n",
      "Epoch 157/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.9652\n",
      "Epoch 158/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.9623\n",
      "Epoch 159/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.9593\n",
      "Epoch 160/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.9563\n",
      "Epoch 161/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.9534\n",
      "Epoch 162/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.9505\n",
      "Epoch 163/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.9476\n",
      "Epoch 164/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.9447\n",
      "Epoch 165/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.9419\n",
      "Epoch 166/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.9390\n",
      "Epoch 167/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.9362\n",
      "Epoch 168/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9334\n",
      "Epoch 169/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.9306\n",
      "Epoch 170/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.9279\n",
      "Epoch 171/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.9251\n",
      "Epoch 172/500\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 0.9224\n",
      "Epoch 173/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.9197\n",
      "Epoch 174/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.9170\n",
      "Epoch 175/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.9143\n",
      "Epoch 176/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.9117\n",
      "Epoch 177/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.9091\n",
      "Epoch 178/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.9065\n",
      "Epoch 179/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.9039\n",
      "Epoch 180/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.9013\n",
      "Epoch 181/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.8987\n",
      "Epoch 182/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.8962\n",
      "Epoch 183/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.8936\n",
      "Epoch 184/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.8911\n",
      "Epoch 185/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.8886\n",
      "Epoch 186/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.8861\n",
      "Epoch 187/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.8836\n",
      "Epoch 188/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.8812\n",
      "Epoch 189/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.8787\n",
      "Epoch 190/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.8763\n",
      "Epoch 191/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.8739\n",
      "Epoch 192/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.8715\n",
      "Epoch 193/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.8691\n",
      "Epoch 194/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.8667\n",
      "Epoch 195/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.8644\n",
      "Epoch 196/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.8620\n",
      "Epoch 197/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.8597\n",
      "Epoch 198/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.8574\n",
      "Epoch 199/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.8551\n",
      "Epoch 200/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.8528\n",
      "Epoch 201/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.8506\n",
      "Epoch 202/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.8483\n",
      "Epoch 203/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.8461\n",
      "Epoch 204/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.8439\n",
      "Epoch 205/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.8416\n",
      "Epoch 206/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.8394\n",
      "Epoch 207/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.8373\n",
      "Epoch 208/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.8351\n",
      "Epoch 209/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.8329\n",
      "Epoch 210/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.8308\n",
      "Epoch 211/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.8287\n",
      "Epoch 212/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.8265\n",
      "Epoch 213/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.8244\n",
      "Epoch 214/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.8223\n",
      "Epoch 215/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.8203\n",
      "Epoch 216/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.8182\n",
      "Epoch 217/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.8161\n",
      "Epoch 218/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.8141\n",
      "Epoch 219/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.8121\n",
      "Epoch 220/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.8100\n",
      "Epoch 221/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.8080\n",
      "Epoch 222/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.8060\n",
      "Epoch 223/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.8040\n",
      "Epoch 224/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.8021\n",
      "Epoch 225/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.8001\n",
      "Epoch 226/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7981\n",
      "Epoch 227/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7962\n",
      "Epoch 228/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.7943\n",
      "Epoch 229/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7924\n",
      "Epoch 230/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7905\n",
      "Epoch 231/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7886\n",
      "Epoch 232/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7867\n",
      "Epoch 233/500\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.7848\n",
      "Epoch 234/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7829\n",
      "Epoch 235/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7811\n",
      "Epoch 236/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.7792\n",
      "Epoch 237/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7774\n",
      "Epoch 238/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7756\n",
      "Epoch 239/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.7738\n",
      "Epoch 240/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7720\n",
      "Epoch 241/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.770 - 0s 9ms/step - loss: 0.7702\n",
      "Epoch 242/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7684\n",
      "Epoch 243/500\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.7666\n",
      "Epoch 244/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7649\n",
      "Epoch 245/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7631\n",
      "Epoch 246/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7614\n",
      "Epoch 247/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.7596\n",
      "Epoch 248/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.7579\n",
      "Epoch 249/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.7562\n",
      "Epoch 250/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7545\n",
      "Epoch 251/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7528\n",
      "Epoch 252/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.7511\n",
      "Epoch 253/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.7494\n",
      "Epoch 254/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.7478\n",
      "Epoch 255/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.7461\n",
      "Epoch 256/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.7445\n",
      "Epoch 257/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.7428\n",
      "Epoch 258/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7412\n",
      "Epoch 259/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.7396\n",
      "Epoch 260/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.7379\n",
      "Epoch 261/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.7363\n",
      "Epoch 262/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7347\n",
      "Epoch 263/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.7331\n",
      "Epoch 264/500\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.7316\n",
      "Epoch 265/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.7300\n",
      "Epoch 266/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.7284\n",
      "Epoch 267/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.7269\n",
      "Epoch 268/500\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.7253\n",
      "Epoch 269/500\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.7238\n",
      "Epoch 270/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.7222\n",
      "Epoch 271/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.7207\n",
      "Epoch 272/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7192\n",
      "Epoch 273/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.7177\n",
      "Epoch 274/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7162\n",
      "Epoch 275/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.7147\n",
      "Epoch 276/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.7132\n",
      "Epoch 277/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.7117\n",
      "Epoch 278/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.7102\n",
      "Epoch 279/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7088\n",
      "Epoch 280/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7073\n",
      "Epoch 281/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7059\n",
      "Epoch 282/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7044\n",
      "Epoch 283/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7030\n",
      "Epoch 284/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7016\n",
      "Epoch 285/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7001\n",
      "Epoch 286/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.6987\n",
      "Epoch 287/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.6973\n",
      "Epoch 288/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.6959\n",
      "Epoch 289/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.6945\n",
      "Epoch 290/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.6931\n",
      "Epoch 291/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6917\n",
      "Epoch 292/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.6904\n",
      "Epoch 293/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.6890\n",
      "Epoch 294/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.6876\n",
      "Epoch 295/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.6863\n",
      "Epoch 296/500\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.6849\n",
      "Epoch 297/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6836\n",
      "Epoch 298/500\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6823\n",
      "Epoch 299/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6809\n",
      "Epoch 300/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6796\n",
      "Epoch 301/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6783\n",
      "Epoch 302/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6770\n",
      "Epoch 303/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6757\n",
      "Epoch 304/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.6744\n",
      "Epoch 305/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.673 - 0s 5ms/step - loss: 0.6731\n",
      "Epoch 306/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6718\n",
      "Epoch 307/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6705\n",
      "Epoch 308/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.6692\n",
      "Epoch 309/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6680\n",
      "Epoch 310/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.6667\n",
      "Epoch 311/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6655\n",
      "Epoch 312/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6642\n",
      "Epoch 313/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6630\n",
      "Epoch 314/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6617\n",
      "Epoch 315/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.6605\n",
      "Epoch 316/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.6593\n",
      "Epoch 317/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.6580\n",
      "Epoch 318/500\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.6568\n",
      "Epoch 319/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6556\n",
      "Epoch 320/500\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.6544\n",
      "Epoch 321/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.6532\n",
      "Epoch 322/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6520\n",
      "Epoch 323/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6508\n",
      "Epoch 324/500\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6496\n",
      "Epoch 325/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6485\n",
      "Epoch 326/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.6473\n",
      "Epoch 327/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.6461\n",
      "Epoch 328/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6449\n",
      "Epoch 329/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6438\n",
      "Epoch 330/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6426\n",
      "Epoch 331/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6415\n",
      "Epoch 332/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6403\n",
      "Epoch 333/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6392\n",
      "Epoch 334/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6381\n",
      "Epoch 335/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6369\n",
      "Epoch 336/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6358\n",
      "Epoch 337/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6347\n",
      "Epoch 338/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6336\n",
      "Epoch 339/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6325\n",
      "Epoch 340/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.6314\n",
      "Epoch 341/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6303\n",
      "Epoch 342/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6292\n",
      "Epoch 343/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6281\n",
      "Epoch 344/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.6270\n",
      "Epoch 345/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6259\n",
      "Epoch 346/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6248\n",
      "Epoch 347/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6238\n",
      "Epoch 348/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.6227\n",
      "Epoch 349/500\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.6216\n",
      "Epoch 350/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.6206\n",
      "Epoch 351/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6195\n",
      "Epoch 352/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6185\n",
      "Epoch 353/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6174\n",
      "Epoch 354/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6164\n",
      "Epoch 355/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6153\n",
      "Epoch 356/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6143\n",
      "Epoch 357/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6133\n",
      "Epoch 358/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6122\n",
      "Epoch 359/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6112\n",
      "Epoch 360/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6102\n",
      "Epoch 361/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6092\n",
      "Epoch 362/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.6082\n",
      "Epoch 363/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.607 - 0s 7ms/step - loss: 0.6072\n",
      "Epoch 364/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6062\n",
      "Epoch 365/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.6052\n",
      "Epoch 366/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.6042\n",
      "Epoch 367/500\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6032\n",
      "Epoch 368/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.6022\n",
      "Epoch 369/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6012\n",
      "Epoch 370/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6002\n",
      "Epoch 371/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.5993\n",
      "Epoch 372/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5983\n",
      "Epoch 373/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.5973\n",
      "Epoch 374/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5964\n",
      "Epoch 375/500\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5954\n",
      "Epoch 376/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5945\n",
      "Epoch 377/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.5935\n",
      "Epoch 378/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5926\n",
      "Epoch 379/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5916\n",
      "Epoch 380/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.5907\n",
      "Epoch 381/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.5897\n",
      "Epoch 382/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5888\n",
      "Epoch 383/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5879\n",
      "Epoch 384/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.5870\n",
      "Epoch 385/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.5860\n",
      "Epoch 386/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.5851\n",
      "Epoch 387/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.5842\n",
      "Epoch 388/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5833\n",
      "Epoch 389/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5824\n",
      "Epoch 390/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.5815\n",
      "Epoch 391/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.5806\n",
      "Epoch 392/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.5797\n",
      "Epoch 393/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5788\n",
      "Epoch 394/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.5779\n",
      "Epoch 395/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5770\n",
      "Epoch 396/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5761\n",
      "Epoch 397/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.5752\n",
      "Epoch 398/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5743\n",
      "Epoch 399/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5735\n",
      "Epoch 400/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5726\n",
      "Epoch 401/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.5717\n",
      "Epoch 402/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.5708\n",
      "Epoch 403/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.5700\n",
      "Epoch 404/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.5691\n",
      "Epoch 405/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5683\n",
      "Epoch 406/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5674\n",
      "Epoch 407/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.5666\n",
      "Epoch 408/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5657\n",
      "Epoch 409/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.5649\n",
      "Epoch 410/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.5640\n",
      "Epoch 411/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.5632\n",
      "Epoch 412/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.5623\n",
      "Epoch 413/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.5615\n",
      "Epoch 414/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.5607\n",
      "Epoch 415/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5599\n",
      "Epoch 416/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5590\n",
      "Epoch 417/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5582\n",
      "Epoch 418/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.5574\n",
      "Epoch 419/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5566\n",
      "Epoch 420/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5558\n",
      "Epoch 421/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5549\n",
      "Epoch 422/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5541\n",
      "Epoch 423/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5533\n",
      "Epoch 424/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5525\n",
      "Epoch 425/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5517\n",
      "Epoch 426/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5509\n",
      "Epoch 427/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5501\n",
      "Epoch 428/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.5493\n",
      "Epoch 429/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5486\n",
      "Epoch 430/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.5478\n",
      "Epoch 431/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5470\n",
      "Epoch 432/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5462\n",
      "Epoch 433/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5454\n",
      "Epoch 434/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.5446\n",
      "Epoch 435/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.5439\n",
      "Epoch 436/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5431\n",
      "Epoch 437/500\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5423\n",
      "Epoch 438/500\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.5416\n",
      "Epoch 439/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5408\n",
      "Epoch 440/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.5400\n",
      "Epoch 441/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.5393\n",
      "Epoch 442/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5385\n",
      "Epoch 443/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.5378\n",
      "Epoch 444/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.5370\n",
      "Epoch 445/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5363\n",
      "Epoch 446/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.5355\n",
      "Epoch 447/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5348\n",
      "Epoch 448/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5340\n",
      "Epoch 449/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.5333\n",
      "Epoch 450/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.5326\n",
      "Epoch 451/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5318\n",
      "Epoch 452/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5311\n",
      "Epoch 453/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5304\n",
      "Epoch 454/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5296\n",
      "Epoch 455/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5289\n",
      "Epoch 456/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5282\n",
      "Epoch 457/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.5275\n",
      "Epoch 458/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5267\n",
      "Epoch 459/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5260\n",
      "Epoch 460/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5253\n",
      "Epoch 461/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5246\n",
      "Epoch 462/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.5239\n",
      "Epoch 463/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5232\n",
      "Epoch 464/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5225\n",
      "Epoch 465/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5218\n",
      "Epoch 466/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5211\n",
      "Epoch 467/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.5204\n",
      "Epoch 468/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5197\n",
      "Epoch 469/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5190\n",
      "Epoch 470/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5183\n",
      "Epoch 471/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5176\n",
      "Epoch 472/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5169\n",
      "Epoch 473/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5162\n",
      "Epoch 474/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.5156\n",
      "Epoch 475/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5149\n",
      "Epoch 476/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5142\n",
      "Epoch 477/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.5135\n",
      "Epoch 478/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.5128\n",
      "Epoch 479/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5122\n",
      "Epoch 480/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5115\n",
      "Epoch 481/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.5108\n",
      "Epoch 482/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.5102\n",
      "Epoch 483/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5095\n",
      "Epoch 484/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5088\n",
      "Epoch 485/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5082\n",
      "Epoch 486/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.5075\n",
      "Epoch 487/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5069\n",
      "Epoch 488/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.5062\n",
      "Epoch 489/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5056\n",
      "Epoch 490/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5049\n",
      "Epoch 491/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5043\n",
      "Epoch 492/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5036\n",
      "Epoch 493/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.5030\n",
      "Epoch 494/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5023\n",
      "Epoch 495/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5017\n",
      "Epoch 496/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5010\n",
      "Epoch 497/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5004\n",
      "Epoch 498/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4998\n",
      "Epoch 499/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4991\n",
      "Epoch 500/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4985\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(x,y,batch_size=150,epochs=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一开始loss非常大，而随着训练不断的进行，loss在逐渐减小。将history中的数据通过matplotlib绘图表现出来，就非常直观了。因为在jupyter上调试，我加入%matplotlib inline命令。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fdaf78e7eb8>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#将图片内嵌在交互窗口，而不是弹出一个图片窗口\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(500),history.history['loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们如何评价一个模型训练是否成功？首先，训练过程中训练集loss要下降到一个较小的值，表示模型收敛较好，没有欠拟合；其次，测试集loss最后与训练集loss要尽可能相似，差距越小越好小，说明该模型没有过拟合。\n",
    "\n",
    "模型成功训练出来后，便可以使用该模型对输入的鸢尾花数据，判断是属于哪一种类别了。这里选择一个最简单的部分，随机读取数据集中的几条数据，看看这个模型会输出什么结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以在数据集中随机选择几条，略作修改后进行测试。看看输出的结果对不对。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sepal.Length</th>\n",
       "      <th>Sepal.Width</th>\n",
       "      <th>Petal.Length</th>\n",
       "      <th>Petal.Width</th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.1</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.5</td>\n",
       "      <td>2.4</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.1</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sepal.Length  Sepal.Width  Petal.Length  Petal.Width          Species\n",
       "0           5.1          3.4           1.5          0.2      Iris-setosa\n",
       "1           6.1          2.8           4.0          1.3  Iris-versicolor\n",
       "2           5.5          3.5           1.3          0.2      Iris-setosa\n",
       "3           4.8          3.0           1.4          0.3      Iris-setosa\n",
       "4           5.5          2.4           3.8          1.1  Iris-versicolor"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv('./iris.csv')\n",
    "data=data.sample(frac=1).reset_index(drop=True)   #打乱数据的先后顺序\n",
    "x_input=data.iloc[:,0:-1]\n",
    "x_input=x_input[:5]\n",
    "data[:5].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.85509336, 0.10326692, 0.04163969],\n",
       "       [0.16691774, 0.46382022, 0.36926198],\n",
       "       [0.916014  , 0.06202741, 0.02195866],\n",
       "       [0.8281539 , 0.1237003 , 0.04814569],\n",
       "       [0.16936547, 0.4697294 , 0.36090508]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里的数据是按照“Species_Iris-setosa，Species_Iris-versicolor，Species_Iris-virginica”来排序的。模型输出的数据中，每一列都是估算，哪一列数据大，我们就选择哪一个分类结果。\n",
    "\n",
    "如果发现不准确，我们继续训练一下模型，即再运行几次，等到loss的值没有显著变化的时候，再来测试模型。一般来说，只要loss值到达0.1左右，识别效果就很不错了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.模型的保存和导入"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.1.模型保存**\n",
    "\n",
    "使用save可以保存训练好的模型，下次导入即可使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./model/model-5-28.h5')   # HDF5文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.2.模型导入**\n",
    "\n",
    "使用keras.models的load_model语句载入模型，就可以直接用这个模型来做预测了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "import pandas as pd\n",
    "model = load_model('./model/model-5-28.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.3.模型应用**\n",
    "\n",
    "输入数据，然后利用模型的predict方法得到预测结果。\n",
    "\n",
    "注：一些教程中常常看到的“predict_classes”已经弃用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "请输入数据，用“,”分开: 5.5,2.6,4.4,1.2\n"
     ]
    }
   ],
   "source": [
    "#请输入数据，如：5.5,2.6,4.4,1.2\n",
    "s=input(\"请输入数据，用“,”分开:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['5.5', '2.6', '4.4', '1.2']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将输入的数据分割为列表\n",
    "i_data=s.split(',')\n",
    "i_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5.5, 2.6, 4.4, 1.2]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 转换列表的所有元素为float类型\n",
    "i_data=list(map(float,i_data))\n",
    "i_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#生成一个空的DataFrame，将输入的列表添加为新行\n",
    "x=pd.DataFrame(columns=['Sepal.Length','Sepal.Width','Petal.Length','Petal.Width'])\n",
    "x.loc[0]=i_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sepal.Length</th>\n",
       "      <th>Sepal.Width</th>\n",
       "      <th>Petal.Length</th>\n",
       "      <th>Petal.Width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.5</td>\n",
       "      <td>2.6</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n",
       "0           5.5          2.6           4.4          1.2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.06928474, 0.45517582, 0.47553945]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 输入并预测\n",
    "model.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从结果可以看出，概率最大的是最后一类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#用“np.argmax”将直接输出概率最大的一项\n",
    "import numpy as np\n",
    "np.argmax(model.predict(x), axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.利用自带数据集\n",
    "\n",
    "这一部分的内容用于参考，可以和前面的方法对比一下。\n",
    "\n",
    "参考资料：https://keras.io/zh/\n",
    "\n",
    "**6.1.数据导入**\n",
    "\n",
    "sklearn自带了iris数据集，即导入sklearn.datasets。要从网络下载数据的，第一次使用，要等一会儿。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "['setosa' 'versicolor' 'virginica']\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "from sklearn.datasets import load_iris\n",
    "i_data = load_iris()\n",
    "print(i_data.feature_names)\n",
    "print(i_data.target_names)\n",
    "x = i_data.data\n",
    "y = i_data.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6.2.数据预处理**\n",
    "\n",
    "Scikit-Learn已经帮我们把类别编码成了数字，不过是一维数组（None, ）（样本的个数不固定，用None表示），而Keras多分类接受的类别输入是一个二维数组，是y的one-hot编码形式。one-hot编码，简单来讲，就是将原来由0开始的类别值转换成向量，比如3个类别0,1,2，那么类别向量长度为3，以原类别值作为位置索引，对应位置置为1，其它位置置为0，即类别0对应：[1, 0, 0]，类别1对应[0, 1, 0]，类别2对应[0, 0, 1]。全部转换后，y变为二维数组（None，3），可以打印前3行看看。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 3)\n",
      "[[1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "y = to_categorical(y, 3)\n",
    "print(y.shape)\n",
    "print(y[0:3, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6.3.定义模型（搭建神经网络）**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "model = Sequential()\n",
    "from keras.layers import Dense\n",
    "model.add(Dense(units=8, input_dim=4, activation='relu'))\n",
    "model.add(Dense(units=3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6.4.编译模型**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6.5.训练模型**\n",
    "\n",
    "这段代码可以多运行几次，你会发现loss多值会越来越小。到了0.05后，变化就不大了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "15/15 [==============================] - 8s 2ms/step - loss: 2.1136 - accuracy: 0.3469\n",
      "Epoch 2/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1.9307 - accuracy: 0.3190\n",
      "Epoch 3/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1.6638 - accuracy: 0.3339\n",
      "Epoch 4/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 1.4534 - accuracy: 0.2622\n",
      "Epoch 5/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 1.2676 - accuracy: 0.2192\n",
      "Epoch 6/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 1.0622 - accuracy: 0.2964\n",
      "Epoch 7/200\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.9714 - accuracy: 0.2557\n",
      "Epoch 8/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.8820 - accuracy: 0.2493\n",
      "Epoch 9/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.7919 - accuracy: 0.4591\n",
      "Epoch 10/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.7734 - accuracy: 0.6383\n",
      "Epoch 11/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.7471 - accuracy: 0.6250\n",
      "Epoch 12/200\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.7537 - accuracy: 0.5421\n",
      "Epoch 13/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.7434 - accuracy: 0.5574\n",
      "Epoch 14/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.7007 - accuracy: 0.6220\n",
      "Epoch 15/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.7134 - accuracy: 0.5670\n",
      "Epoch 16/200\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.7009 - accuracy: 0.6010\n",
      "Epoch 17/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.6572 - accuracy: 0.6254\n",
      "Epoch 18/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.6755 - accuracy: 0.5895\n",
      "Epoch 19/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.6354 - accuracy: 0.6222\n",
      "Epoch 20/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.6499 - accuracy: 0.6358\n",
      "Epoch 21/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.6680 - accuracy: 0.5638\n",
      "Epoch 22/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.6134 - accuracy: 0.6390\n",
      "Epoch 23/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.6283 - accuracy: 0.6601\n",
      "Epoch 24/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.6291 - accuracy: 0.6636\n",
      "Epoch 25/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.5953 - accuracy: 0.6692\n",
      "Epoch 26/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.5868 - accuracy: 0.7271\n",
      "Epoch 27/200\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.5764 - accuracy: 0.7032\n",
      "Epoch 28/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.5935 - accuracy: 0.6216\n",
      "Epoch 29/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.5599 - accuracy: 0.7191\n",
      "Epoch 30/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.6238 - accuracy: 0.6711\n",
      "Epoch 31/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.5454 - accuracy: 0.6744\n",
      "Epoch 32/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.5863 - accuracy: 0.6878\n",
      "Epoch 33/200\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.5540 - accuracy: 0.7135\n",
      "Epoch 34/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.5438 - accuracy: 0.7784\n",
      "Epoch 35/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.5736 - accuracy: 0.7076\n",
      "Epoch 36/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.5521 - accuracy: 0.8222\n",
      "Epoch 37/200\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.5193 - accuracy: 0.7750\n",
      "Epoch 38/200\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.5118 - accuracy: 0.7153\n",
      "Epoch 39/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.5165 - accuracy: 0.7738\n",
      "Epoch 40/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.5534 - accuracy: 0.6706\n",
      "Epoch 41/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.4611 - accuracy: 0.8305\n",
      "Epoch 42/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.5081 - accuracy: 0.8626\n",
      "Epoch 43/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.5147 - accuracy: 0.8829\n",
      "Epoch 44/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.4758 - accuracy: 0.7834\n",
      "Epoch 45/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.5150 - accuracy: 0.8054\n",
      "Epoch 46/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4719 - accuracy: 0.8812\n",
      "Epoch 47/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4708 - accuracy: 0.8499\n",
      "Epoch 48/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.4786 - accuracy: 0.8650\n",
      "Epoch 49/200\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.4284 - accuracy: 0.8922\n",
      "Epoch 50/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4326 - accuracy: 0.8891\n",
      "Epoch 51/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.4732 - accuracy: 0.8691\n",
      "Epoch 52/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4472 - accuracy: 0.8851\n",
      "Epoch 53/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4374 - accuracy: 0.9440\n",
      "Epoch 54/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4558 - accuracy: 0.8994\n",
      "Epoch 55/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4435 - accuracy: 0.9103\n",
      "Epoch 56/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4267 - accuracy: 0.9535\n",
      "Epoch 57/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4449 - accuracy: 0.9267\n",
      "Epoch 58/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4253 - accuracy: 0.8965\n",
      "Epoch 59/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.3944 - accuracy: 0.9489\n",
      "Epoch 60/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4076 - accuracy: 0.9462\n",
      "Epoch 61/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.3861 - accuracy: 0.9635\n",
      "Epoch 62/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3949 - accuracy: 0.9181\n",
      "Epoch 63/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.3862 - accuracy: 0.9699\n",
      "Epoch 64/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3895 - accuracy: 0.9629\n",
      "Epoch 65/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3864 - accuracy: 0.9662\n",
      "Epoch 66/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3697 - accuracy: 0.9786\n",
      "Epoch 67/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3732 - accuracy: 0.9579\n",
      "Epoch 68/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3538 - accuracy: 0.9408\n",
      "Epoch 69/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3291 - accuracy: 0.9800\n",
      "Epoch 70/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3833 - accuracy: 0.9487\n",
      "Epoch 71/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3725 - accuracy: 0.9581\n",
      "Epoch 72/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3672 - accuracy: 0.9514\n",
      "Epoch 73/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.3386 - accuracy: 0.9645\n",
      "Epoch 74/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.3249 - accuracy: 0.9648\n",
      "Epoch 75/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3528 - accuracy: 0.9403\n",
      "Epoch 76/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.3172 - accuracy: 0.9725\n",
      "Epoch 77/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3136 - accuracy: 0.9749\n",
      "Epoch 78/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.2944 - accuracy: 0.9696\n",
      "Epoch 79/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3043 - accuracy: 0.9836\n",
      "Epoch 80/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3115 - accuracy: 0.9904\n",
      "Epoch 81/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.3066 - accuracy: 0.9746\n",
      "Epoch 82/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.2904 - accuracy: 0.9823\n",
      "Epoch 83/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3181 - accuracy: 0.9544\n",
      "Epoch 84/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3042 - accuracy: 0.9729\n",
      "Epoch 85/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.2727 - accuracy: 0.9778\n",
      "Epoch 86/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.3049 - accuracy: 0.9733\n",
      "Epoch 87/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.2866 - accuracy: 0.9861\n",
      "Epoch 88/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.2916 - accuracy: 0.9780\n",
      "Epoch 89/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.2812 - accuracy: 0.9799\n",
      "Epoch 90/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.2816 - accuracy: 0.9739\n",
      "Epoch 91/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.2704 - accuracy: 0.9788\n",
      "Epoch 92/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.2710 - accuracy: 0.9843\n",
      "Epoch 93/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.2718 - accuracy: 0.9609\n",
      "Epoch 94/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.2739 - accuracy: 0.9692\n",
      "Epoch 95/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.2382 - accuracy: 0.9854\n",
      "Epoch 96/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.2498 - accuracy: 0.9580\n",
      "Epoch 97/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.2523 - accuracy: 0.9924\n",
      "Epoch 98/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.2483 - accuracy: 0.9752\n",
      "Epoch 99/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.2687 - accuracy: 0.9459\n",
      "Epoch 100/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.2494 - accuracy: 0.9878\n",
      "Epoch 101/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.2571 - accuracy: 0.9653\n",
      "Epoch 102/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.2625 - accuracy: 0.9788\n",
      "Epoch 103/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.2336 - accuracy: 0.9699\n",
      "Epoch 104/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2470 - accuracy: 0.9704\n",
      "Epoch 105/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.2357 - accuracy: 0.9566\n",
      "Epoch 106/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.2515 - accuracy: 0.9611\n",
      "Epoch 107/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.2184 - accuracy: 0.9884\n",
      "Epoch 108/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.2251 - accuracy: 0.9769\n",
      "Epoch 109/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.2131 - accuracy: 0.9883\n",
      "Epoch 110/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.2188 - accuracy: 0.9770\n",
      "Epoch 111/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.2145 - accuracy: 0.9686\n",
      "Epoch 112/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.2364 - accuracy: 0.9660\n",
      "Epoch 113/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.2327 - accuracy: 0.9451\n",
      "Epoch 114/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.2248 - accuracy: 0.9791\n",
      "Epoch 115/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.2173 - accuracy: 0.9674\n",
      "Epoch 116/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.2059 - accuracy: 0.9724\n",
      "Epoch 117/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.2063 - accuracy: 0.9653\n",
      "Epoch 118/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.1762 - accuracy: 0.9895\n",
      "Epoch 119/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.2011 - accuracy: 0.9716\n",
      "Epoch 120/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.1990 - accuracy: 0.9629\n",
      "Epoch 121/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.1905 - accuracy: 0.9868\n",
      "Epoch 122/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.1957 - accuracy: 0.9670\n",
      "Epoch 123/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.1920 - accuracy: 0.9767\n",
      "Epoch 124/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.1874 - accuracy: 0.9839\n",
      "Epoch 125/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.1969 - accuracy: 0.9866\n",
      "Epoch 126/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.1761 - accuracy: 0.9887\n",
      "Epoch 127/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.1858 - accuracy: 0.9714\n",
      "Epoch 128/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.1914 - accuracy: 0.9814\n",
      "Epoch 129/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.1663 - accuracy: 0.9894\n",
      "Epoch 130/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.1898 - accuracy: 0.9640\n",
      "Epoch 131/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.1752 - accuracy: 0.9701\n",
      "Epoch 132/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.1675 - accuracy: 0.9911\n",
      "Epoch 133/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.1890 - accuracy: 0.9716\n",
      "Epoch 134/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.1848 - accuracy: 0.9716\n",
      "Epoch 135/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.1787 - accuracy: 0.9789\n",
      "Epoch 136/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.1757 - accuracy: 0.9725\n",
      "Epoch 137/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.1747 - accuracy: 0.9814\n",
      "Epoch 138/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.1531 - accuracy: 0.9864\n",
      "Epoch 139/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.1620 - accuracy: 0.9832\n",
      "Epoch 140/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.1715 - accuracy: 0.9619\n",
      "Epoch 141/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.1738 - accuracy: 0.9787\n",
      "Epoch 142/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.1514 - accuracy: 0.9920\n",
      "Epoch 143/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.1645 - accuracy: 0.9837\n",
      "Epoch 144/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.1317 - accuracy: 0.9937\n",
      "Epoch 145/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.1689 - accuracy: 0.9582\n",
      "Epoch 146/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.1682 - accuracy: 0.9594\n",
      "Epoch 147/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.1593 - accuracy: 0.9724\n",
      "Epoch 148/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.1808 - accuracy: 0.9650\n",
      "Epoch 149/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.1664 - accuracy: 0.9625\n",
      "Epoch 150/200\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.1438 - accuracy: 0.9874\n",
      "Epoch 151/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.1488 - accuracy: 0.9741\n",
      "Epoch 152/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.1436 - accuracy: 0.9816\n",
      "Epoch 153/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.1403 - accuracy: 0.9828\n",
      "Epoch 154/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.1441 - accuracy: 0.9944\n",
      "Epoch 155/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.1523 - accuracy: 0.9861\n",
      "Epoch 156/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.1354 - accuracy: 0.9740\n",
      "Epoch 157/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.1491 - accuracy: 0.9946\n",
      "Epoch 158/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.1488 - accuracy: 0.9585\n",
      "Epoch 159/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.1399 - accuracy: 0.9843\n",
      "Epoch 160/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.1620 - accuracy: 0.9581\n",
      "Epoch 161/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.1489 - accuracy: 0.9769\n",
      "Epoch 162/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.1277 - accuracy: 0.9838\n",
      "Epoch 163/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.1293 - accuracy: 0.9864\n",
      "Epoch 164/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.1228 - accuracy: 0.9839\n",
      "Epoch 165/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.1233 - accuracy: 0.9939\n",
      "Epoch 166/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.1368 - accuracy: 0.9815\n",
      "Epoch 167/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.1359 - accuracy: 0.9919\n",
      "Epoch 168/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.1272 - accuracy: 0.9849\n",
      "Epoch 169/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.1367 - accuracy: 0.9699\n",
      "Epoch 170/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.1440 - accuracy: 0.9661\n",
      "Epoch 171/200\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.1236 - accuracy: 0.9805\n",
      "Epoch 172/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.1418 - accuracy: 0.9819\n",
      "Epoch 173/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.1594 - accuracy: 0.9531\n",
      "Epoch 174/200\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.1420 - accuracy: 0.9575\n",
      "Epoch 175/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.1196 - accuracy: 0.9800\n",
      "Epoch 176/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.1319 - accuracy: 0.9810\n",
      "Epoch 177/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.1224 - accuracy: 0.9799\n",
      "Epoch 178/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.1289 - accuracy: 0.9636\n",
      "Epoch 179/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.1062 - accuracy: 0.9793\n",
      "Epoch 180/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.1236 - accuracy: 0.9918\n",
      "Epoch 181/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.1171 - accuracy: 0.9915\n",
      "Epoch 182/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0956 - accuracy: 0.9900\n",
      "Epoch 183/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.1135 - accuracy: 0.9891\n",
      "Epoch 184/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.1056 - accuracy: 0.9878\n",
      "Epoch 185/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.1247 - accuracy: 0.9932\n",
      "Epoch 186/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.1060 - accuracy: 0.9904\n",
      "Epoch 187/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.1105 - accuracy: 0.9787\n",
      "Epoch 188/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.1180 - accuracy: 0.9720\n",
      "Epoch 189/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.1048 - accuracy: 0.9890\n",
      "Epoch 190/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0981 - accuracy: 0.9838\n",
      "Epoch 191/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.1124 - accuracy: 0.9923\n",
      "Epoch 192/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.1022 - accuracy: 0.9877\n",
      "Epoch 193/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.1200 - accuracy: 0.9702\n",
      "Epoch 194/200\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.1024 - accuracy: 0.9898\n",
      "Epoch 195/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.1116 - accuracy: 0.9940\n",
      "Epoch 196/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.1206 - accuracy: 0.9761\n",
      "Epoch 197/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0941 - accuracy: 0.9910\n",
      "Epoch 198/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.1179 - accuracy: 0.9593\n",
      "Epoch 199/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0907 - accuracy: 0.9856\n",
      "Epoch 200/200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.1147 - accuracy: 0.9725\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(x, y, epochs=200, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "History对象会被模型的fit方法返回。用dir(history)的方式，可以得到History对象的所有属性，用vars(history)能够看到所有的属性值。\n",
    "\n",
    "具体可以参考：https://keras.io/zh/callbacks/#history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.1244957447052,\n",
       " 1.7910242080688477,\n",
       " 1.5408719778060913,\n",
       " 1.3443976640701294,\n",
       " 1.1806373596191406,\n",
       " 1.0392695665359497,\n",
       " 0.937523603439331,\n",
       " 0.8605464696884155,\n",
       " 0.8044152855873108,\n",
       " 0.7702749371528625,\n",
       " 0.750398576259613,\n",
       " 0.7358033657073975,\n",
       " 0.7212567329406738,\n",
       " 0.7096283435821533,\n",
       " 0.6989922523498535,\n",
       " 0.6859288811683655,\n",
       " 0.6757642030715942,\n",
       " 0.6668028831481934,\n",
       " 0.659077525138855,\n",
       " 0.6491382718086243,\n",
       " 0.6414797902107239,\n",
       " 0.6334409713745117,\n",
       " 0.6259604096412659,\n",
       " 0.6181484460830688,\n",
       " 0.6115184426307678,\n",
       " 0.6047548651695251,\n",
       " 0.5974271297454834,\n",
       " 0.5902149677276611,\n",
       " 0.5845476984977722,\n",
       " 0.5769246816635132,\n",
       " 0.5710497498512268,\n",
       " 0.5641980171203613,\n",
       " 0.5563315153121948,\n",
       " 0.5511131286621094,\n",
       " 0.5455443859100342,\n",
       " 0.5356295108795166,\n",
       " 0.5294861793518066,\n",
       " 0.5241512656211853,\n",
       " 0.5162819623947144,\n",
       " 0.5101361870765686,\n",
       " 0.5040771961212158,\n",
       " 0.4975661635398865,\n",
       " 0.49116846919059753,\n",
       " 0.48496779799461365,\n",
       " 0.47947564721107483,\n",
       " 0.4742377698421478,\n",
       " 0.46606943011283875,\n",
       " 0.46097031235694885,\n",
       " 0.455351322889328,\n",
       " 0.45051926374435425,\n",
       " 0.4443660080432892,\n",
       " 0.4379578232765198,\n",
       " 0.43283751606941223,\n",
       " 0.4278043508529663,\n",
       " 0.4237062335014343,\n",
       " 0.41688457131385803,\n",
       " 0.41557908058166504,\n",
       " 0.4076264500617981,\n",
       " 0.40352967381477356,\n",
       " 0.397098183631897,\n",
       " 0.39563894271850586,\n",
       " 0.38634371757507324,\n",
       " 0.38413482904434204,\n",
       " 0.38078781962394714,\n",
       " 0.3725326657295227,\n",
       " 0.3695921003818512,\n",
       " 0.36994290351867676,\n",
       " 0.36130595207214355,\n",
       " 0.35519829392433167,\n",
       " 0.351481169462204,\n",
       " 0.3474884629249573,\n",
       " 0.3431875705718994,\n",
       " 0.339663028717041,\n",
       " 0.3339100182056427,\n",
       " 0.3306918442249298,\n",
       " 0.3261803686618805,\n",
       " 0.32347092032432556,\n",
       " 0.3185223937034607,\n",
       " 0.3141818344593048,\n",
       " 0.31082266569137573,\n",
       " 0.3075686991214752,\n",
       " 0.30488935112953186,\n",
       " 0.300876647233963,\n",
       " 0.29642319679260254,\n",
       " 0.2923813760280609,\n",
       " 0.2891921401023865,\n",
       " 0.2854543924331665,\n",
       " 0.28267771005630493,\n",
       " 0.2789277732372284,\n",
       " 0.27724939584732056,\n",
       " 0.2760482430458069,\n",
       " 0.2688429057598114,\n",
       " 0.2678097188472748,\n",
       " 0.2629905939102173,\n",
       " 0.2602810561656952,\n",
       " 0.26111552119255066,\n",
       " 0.2531181871891022,\n",
       " 0.25071024894714355,\n",
       " 0.2475566565990448,\n",
       " 0.24530918896198273,\n",
       " 0.24251307547092438,\n",
       " 0.2446824163198471,\n",
       " 0.2358071357011795,\n",
       " 0.2353125512599945,\n",
       " 0.2317223846912384,\n",
       " 0.22925740480422974,\n",
       " 0.2266225665807724,\n",
       " 0.22392502427101135,\n",
       " 0.22214893996715546,\n",
       " 0.21931739151477814,\n",
       " 0.21640093624591827,\n",
       " 0.215007483959198,\n",
       " 0.2127734124660492,\n",
       " 0.21237003803253174,\n",
       " 0.20768961310386658,\n",
       " 0.20705625414848328,\n",
       " 0.204072043299675,\n",
       " 0.2015121430158615,\n",
       " 0.19926446676254272,\n",
       " 0.1971583366394043,\n",
       " 0.19567860662937164,\n",
       " 0.1930556297302246,\n",
       " 0.1914551705121994,\n",
       " 0.19004619121551514,\n",
       " 0.18827413022518158,\n",
       " 0.18629755079746246,\n",
       " 0.18461528420448303,\n",
       " 0.18356674909591675,\n",
       " 0.18124455213546753,\n",
       " 0.17991553246974945,\n",
       " 0.1777973473072052,\n",
       " 0.17601735889911652,\n",
       " 0.1733025461435318,\n",
       " 0.1723809689283371,\n",
       " 0.1713208705186844,\n",
       " 0.17188142240047455,\n",
       " 0.1664956510066986,\n",
       " 0.16690336167812347,\n",
       " 0.16489297151565552,\n",
       " 0.16595977544784546,\n",
       " 0.16158577799797058,\n",
       " 0.16151945292949677,\n",
       " 0.1590929925441742,\n",
       " 0.15828214585781097,\n",
       " 0.15682707726955414,\n",
       " 0.15517055988311768,\n",
       " 0.15485109388828278,\n",
       " 0.1521909236907959,\n",
       " 0.1544930636882782,\n",
       " 0.15043975412845612,\n",
       " 0.1491573452949524,\n",
       " 0.14983020722866058,\n",
       " 0.15007327497005463,\n",
       " 0.14489342272281647,\n",
       " 0.14642566442489624,\n",
       " 0.14299830794334412,\n",
       " 0.14782319962978363,\n",
       " 0.14579907059669495,\n",
       " 0.1400640308856964,\n",
       " 0.1387968808412552,\n",
       " 0.13898156583309174,\n",
       " 0.13747020065784454,\n",
       " 0.1370438039302826,\n",
       " 0.13603006303310394,\n",
       " 0.1355964094400406,\n",
       " 0.1330452412366867,\n",
       " 0.1327187865972519,\n",
       " 0.13215342164039612,\n",
       " 0.1310901641845703,\n",
       " 0.13022488355636597,\n",
       " 0.12980608642101288,\n",
       " 0.1292547881603241,\n",
       " 0.1306368112564087,\n",
       " 0.12627382576465607,\n",
       " 0.12648344039916992,\n",
       " 0.12746955454349518,\n",
       " 0.12451108545064926,\n",
       " 0.12361398339271545,\n",
       " 0.1241597905755043,\n",
       " 0.12248953431844711,\n",
       " 0.12193933874368668,\n",
       " 0.12096025794744492,\n",
       " 0.12026972323656082,\n",
       " 0.12082459777593613,\n",
       " 0.11851044744253159,\n",
       " 0.11847534030675888,\n",
       " 0.11842699348926544,\n",
       " 0.11660421639680862,\n",
       " 0.11616530269384384,\n",
       " 0.11498340964317322,\n",
       " 0.11512988805770874,\n",
       " 0.11472196131944656,\n",
       " 0.11602090299129486,\n",
       " 0.11407206207513809,\n",
       " 0.11360029876232147,\n",
       " 0.11225325614213943,\n",
       " 0.11099763214588165,\n",
       " 0.11072210967540741,\n",
       " 0.1097995787858963,\n",
       " 0.10950173437595367]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 输出所有的loss\n",
    "history.history['loss']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6.6.评估模型**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 1s 2ms/step - loss: 0.1085 - accuracy: 0.9733\n",
      "0.9733333587646484\n"
     ]
    }
   ],
   "source": [
    "result = model.evaluate(x, y)\n",
    "print(result[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6.7.模型预测**\n",
    "\n",
    "predict输出概率矩阵，每一行对应预测值在三个类别上的概率，可以全部打印出来看一下。\n",
    "\n",
    "**注意**：这里采用的是科学计数法。哪个数字小，就说明哪个概率最大。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.93398428e-01 6.60160091e-03 1.65872327e-09]\n",
      " [9.83572662e-01 1.64272822e-02 1.57033888e-08]\n",
      " [9.89965439e-01 1.00345090e-02 7.35242001e-09]\n",
      " [9.79231715e-01 2.07683034e-02 3.34529595e-08]\n",
      " [9.94261265e-01 5.73877571e-03 1.37241840e-09]\n",
      " [9.93492901e-01 6.50704652e-03 1.99003858e-09]\n",
      " [9.91097093e-01 8.90288875e-03 9.05162079e-09]\n",
      " [9.89982903e-01 1.00171268e-02 4.26628466e-09]\n",
      " [9.76297736e-01 2.37020869e-02 6.32534949e-08]\n",
      " [9.82232332e-01 1.77677032e-02 1.08878613e-08]\n",
      " [9.94691312e-01 5.30866161e-03 6.56002308e-10]\n",
      " [9.84799087e-01 1.52009372e-02 1.19210224e-08]\n",
      " [9.82635260e-01 1.73647180e-02 1.28084006e-08]\n",
      " [9.89370584e-01 1.06294593e-02 1.03388897e-08]\n",
      " [9.98536348e-01 1.46364165e-03 3.17859419e-11]\n",
      " [9.98614669e-01 1.38534408e-03 5.85982790e-11]\n",
      " [9.97677743e-01 2.32221768e-03 3.12573134e-10]\n",
      " [9.93563294e-01 6.43668743e-03 2.44884046e-09]\n",
      " [9.93339241e-01 6.66072639e-03 9.60733271e-10]\n",
      " [9.95215178e-01 4.78476752e-03 1.16316634e-09]\n",
      " [9.84296143e-01 1.57037973e-02 5.73015546e-09]\n",
      " [9.94361758e-01 5.63827204e-03 2.59170174e-09]\n",
      " [9.97270525e-01 2.72950367e-03 6.23354590e-10]\n",
      " [9.77546930e-01 2.24529523e-02 6.55275940e-08]\n",
      " [9.60279584e-01 3.97202335e-02 7.00023719e-08]\n",
      " [9.71782148e-01 2.82178223e-02 3.70198805e-08]\n",
      " [9.85893250e-01 1.41067747e-02 1.92300149e-08]\n",
      " [9.92055357e-01 7.94463977e-03 2.01931072e-09]\n",
      " [9.92406905e-01 7.59316469e-03 2.00449768e-09]\n",
      " [9.77170825e-01 2.28291396e-02 3.26964376e-08]\n",
      " [9.73998964e-01 2.60010231e-02 3.88516455e-08]\n",
      " [9.91576493e-01 8.42357334e-03 4.41424008e-09]\n",
      " [9.97080266e-01 2.91973422e-03 1.43255491e-10]\n",
      " [9.98240948e-01 1.75908173e-03 5.64805806e-11]\n",
      " [9.82033193e-01 1.79668162e-02 1.72486008e-08]\n",
      " [9.92624044e-01 7.37601379e-03 2.86720403e-09]\n",
      " [9.95299041e-01 4.70093265e-03 5.36337752e-10]\n",
      " [9.93929982e-01 6.07001781e-03 1.12009957e-09]\n",
      " [9.84553993e-01 1.54459290e-02 2.71976255e-08]\n",
      " [9.90285575e-01 9.71447490e-03 3.54121599e-09]\n",
      " [9.94652748e-01 5.34720020e-03 2.01103911e-09]\n",
      " [9.52077448e-01 4.79220487e-02 4.67326799e-07]\n",
      " [9.88998711e-01 1.10012526e-02 1.28557183e-08]\n",
      " [9.87952769e-01 1.20472172e-02 3.25128759e-08]\n",
      " [9.82721031e-01 1.72789320e-02 1.88585521e-08]\n",
      " [9.83479142e-01 1.65208764e-02 2.79453509e-08]\n",
      " [9.93626714e-01 6.37328345e-03 1.26111654e-09]\n",
      " [9.87170994e-01 1.28290392e-02 1.29801494e-08]\n",
      " [9.94525194e-01 5.47481235e-03 7.90427668e-10]\n",
      " [9.90425169e-01 9.57486127e-03 4.23538715e-09]\n",
      " [4.24550427e-03 9.71003413e-01 2.47511882e-02]\n",
      " [5.79092233e-03 9.33069468e-01 6.11396171e-02]\n",
      " [1.61150505e-03 9.06857789e-01 9.15305987e-02]\n",
      " [3.31073394e-03 8.12807977e-01 1.83881313e-01]\n",
      " [1.92780467e-03 8.52606356e-01 1.45465866e-01]\n",
      " [1.78583839e-03 8.24321091e-01 1.73893094e-01]\n",
      " [3.09977471e-03 8.52915466e-01 1.43984690e-01]\n",
      " [3.54962312e-02 9.45409834e-01 1.90939605e-02]\n",
      " [2.77735502e-03 9.52286601e-01 4.49361391e-02]\n",
      " [8.33229348e-03 8.31797183e-01 1.59870505e-01]\n",
      " [9.12920851e-03 9.27016973e-01 6.38538301e-02]\n",
      " [8.09132773e-03 8.96708727e-01 9.51999798e-02]\n",
      " [4.33821045e-03 9.65923250e-01 2.97385436e-02]\n",
      " [1.31831726e-03 8.12329233e-01 1.86352402e-01]\n",
      " [4.47095446e-02 9.38569248e-01 1.67213008e-02]\n",
      " [8.15906283e-03 9.68979299e-01 2.28616856e-02]\n",
      " [2.07005953e-03 7.12650299e-01 2.85279632e-01]\n",
      " [7.11118896e-03 9.73950565e-01 1.89382713e-02]\n",
      " [5.04806987e-04 5.57316720e-01 4.42178398e-01]\n",
      " [8.36407114e-03 9.54843998e-01 3.67919803e-02]\n",
      " [7.46798876e-04 4.26772356e-01 5.72480857e-01]\n",
      " [1.31011000e-02 9.60539460e-01 2.63594985e-02]\n",
      " [2.17578694e-04 4.92133707e-01 5.07648706e-01]\n",
      " [1.23790349e-03 8.95049095e-01 1.03712909e-01]\n",
      " [6.83138007e-03 9.64570463e-01 2.85981633e-02]\n",
      " [6.41211122e-03 9.60214317e-01 3.33736539e-02]\n",
      " [1.23488496e-03 8.98687243e-01 1.00077882e-01]\n",
      " [6.01795618e-04 6.37371361e-01 3.62026781e-01]\n",
      " [2.35400326e-03 7.98809767e-01 1.98836297e-01]\n",
      " [3.97048257e-02 9.54879224e-01 5.41599700e-03]\n",
      " [9.16363392e-03 9.49681520e-01 4.11548726e-02]\n",
      " [1.31012034e-02 9.66488421e-01 2.04102974e-02]\n",
      " [1.32205104e-02 9.60073292e-01 2.67061777e-02]\n",
      " [6.99282027e-05 2.48316377e-01 7.51613736e-01]\n",
      " [1.67887006e-03 6.37809694e-01 3.60511422e-01]\n",
      " [6.36625011e-03 8.76864254e-01 1.16769485e-01]\n",
      " [2.86027719e-03 9.20664012e-01 7.64757171e-02]\n",
      " [1.38802605e-03 8.55320334e-01 1.43291667e-01]\n",
      " [1.03705712e-02 9.37674522e-01 5.19549251e-02]\n",
      " [5.08965459e-03 8.67253304e-01 1.27657115e-01]\n",
      " [1.56259537e-03 8.16323757e-01 1.82113662e-01]\n",
      " [2.36521009e-03 8.74142349e-01 1.23492509e-01]\n",
      " [7.84039963e-03 9.49280441e-01 4.28791754e-02]\n",
      " [3.11772283e-02 9.48830009e-01 1.99927930e-02]\n",
      " [4.02848003e-03 8.74976873e-01 1.20994665e-01]\n",
      " [8.07681307e-03 9.54703331e-01 3.72198448e-02]\n",
      " [6.43631862e-03 9.23627853e-01 6.99358433e-02]\n",
      " [6.12231856e-03 9.53942180e-01 3.99355553e-02]\n",
      " [1.15604959e-01 8.77338290e-01 7.05664000e-03]\n",
      " [7.46826828e-03 9.26158369e-01 6.63734004e-02]\n",
      " [2.28287377e-07 4.25251992e-03 9.95747268e-01]\n",
      " [1.41477140e-05 5.73592335e-02 9.42626655e-01]\n",
      " [2.78835387e-06 4.88333777e-02 9.51163888e-01]\n",
      " [7.34538435e-06 8.25909749e-02 9.17401671e-01]\n",
      " [8.76384490e-07 1.50188114e-02 9.84980285e-01]\n",
      " [1.32220762e-07 1.80549957e-02 9.81944919e-01]\n",
      " [7.59732065e-05 9.41144302e-02 9.05809641e-01]\n",
      " [1.03547848e-06 7.09640831e-02 9.29034889e-01]\n",
      " [1.14686964e-06 4.24109958e-02 9.57587898e-01]\n",
      " [1.86436239e-06 2.09795982e-02 9.79018569e-01]\n",
      " [2.36376422e-04 2.75551379e-01 7.24212289e-01]\n",
      " [1.55961316e-05 9.07795206e-02 9.09204900e-01]\n",
      " [1.51487156e-05 8.26278701e-02 9.17357028e-01]\n",
      " [6.06546109e-06 2.71024182e-02 9.72891510e-01]\n",
      " [2.34387016e-06 8.50434788e-03 9.91493225e-01]\n",
      " [1.89369257e-05 4.62888032e-02 9.53692198e-01]\n",
      " [2.79964497e-05 1.71096325e-01 8.28875661e-01]\n",
      " [1.32332593e-06 5.60757592e-02 9.43922937e-01]\n",
      " [3.34379742e-08 1.00015085e-02 9.89998400e-01]\n",
      " [2.98455288e-05 1.87984765e-01 8.11985373e-01]\n",
      " [5.28599458e-06 3.75950411e-02 9.62399721e-01]\n",
      " [2.82870787e-05 5.52733094e-02 9.44698393e-01]\n",
      " [1.09786264e-07 2.36321799e-02 9.76367772e-01]\n",
      " [1.71913372e-04 2.79224575e-01 7.20603526e-01]\n",
      " [1.21472049e-05 7.76429400e-02 9.22344923e-01]\n",
      " [1.38942769e-05 2.15835214e-01 7.84150898e-01]\n",
      " [3.38179671e-04 3.46084297e-01 6.53577447e-01]\n",
      " [3.31188261e-04 3.43234241e-01 6.56434536e-01]\n",
      " [1.65119104e-06 2.19488610e-02 9.78049457e-01]\n",
      " [3.79078338e-05 4.29904103e-01 5.70058048e-01]\n",
      " [1.80811026e-06 7.42280632e-02 9.25770164e-01]\n",
      " [2.25805634e-05 3.16312104e-01 6.83665335e-01]\n",
      " [1.05041443e-06 1.41222188e-02 9.85876679e-01]\n",
      " [2.05429140e-04 5.18338919e-01 4.81455714e-01]\n",
      " [8.18078388e-06 1.67761192e-01 8.32230687e-01]\n",
      " [1.29016928e-06 3.31837051e-02 9.66814995e-01]\n",
      " [3.62681044e-06 1.75184309e-02 9.82477963e-01]\n",
      " [3.31295960e-05 1.77190796e-01 8.22776020e-01]\n",
      " [4.66264901e-04 3.65460992e-01 6.34072781e-01]\n",
      " [4.75416527e-05 1.48058996e-01 8.51893485e-01]\n",
      " [2.67002588e-06 1.83161274e-02 9.81681168e-01]\n",
      " [1.15466682e-04 1.37271658e-01 8.62612844e-01]\n",
      " [1.41477140e-05 5.73592335e-02 9.42626655e-01]\n",
      " [1.30432886e-06 1.88007895e-02 9.81197894e-01]\n",
      " [2.06043433e-06 1.37814851e-02 9.86216426e-01]\n",
      " [2.96706967e-05 6.48981258e-02 9.35072243e-01]\n",
      " [3.32813033e-05 1.09414674e-01 8.90552104e-01]\n",
      " [6.81322344e-05 1.58947811e-01 8.40984046e-01]\n",
      " [1.49089792e-05 3.87691222e-02 9.61216033e-01]\n",
      " [8.04501615e-05 1.77507550e-01 8.22412014e-01]]\n"
     ]
    }
   ],
   "source": [
    "proba = model.predict(x)\n",
    "print(proba[0:150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 用“np.argmax”输出概率最大的一项。\n",
    "import numpy as np\n",
    "np.argmax(model.predict(x), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
