{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 古诗生成器（创意版）\n",
    "\n",
    "基于循环神经网络（Keras+LSTM-RNN），采用了浙江大学人工智能研究所提供的古诗词库，并且在其提供的AI学习平台上训练完成。本案例非原创，原来的代码用class来实现，并且重构了model的方法。为了方便初学者理解，重新调整了代码和参数，并且用jupyterlab写了一个完整的文档。\n",
    "\n",
    "虽然模型已经训练完成，但要应用这个模型，还需要提供原来用于训练的语料，即古诗词库。训练和应用使用的语料要保持一致。\n",
    "\n",
    "除了常规的藏头诗、随机诗外，还可以做哪些好玩的事情？创意版试图做些不一样的工作，供抛砖引玉。\n",
    "\n",
    "原案例地址：https://github.com/youyuge34/Poems_generator_Keras/blob/master/poem_model.ipynb\n",
    "\n",
    "模型下载地址（课程汇集/虚谷号内置课程目录/5.机器学习）：https://github.com/vvlink/vvBoard-docs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.导入必要的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import keras\n",
    "import numpy as np\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.models import Input, Model, load_model\n",
    "from keras.layers import LSTM, Dropout, Dense\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config(object):\n",
    "    # 输入的诗词库（语料库）\n",
    "    poetry_file = 'data/8-poetry_zju.txt'\n",
    "    # 模型名称\n",
    "    weight_file = 'model/8-model_zju.h5'\n",
    "    # 输出训练的信息\n",
    "    fixlog = 'poem_log.txt'\n",
    "    # 复合训练时，间隔多少次输出一次测试结果\n",
    "    predict_num = 5\n",
    "    batch_size = 32\n",
    "    learning_rate = 0.001\n",
    "    # 下面参数不能随意修改，改动将影响整个模型的大小\n",
    "    #根据前六个字预测第七个字，生成的是五言诗（含标点）\n",
    "    max_len = 6\n",
    "    # 去除低频率文字（避免生僻字）\n",
    "    frequence_num = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_file(poetry_file):\n",
    "    # 语料文本内容\n",
    "    files_content = ''\n",
    "    with open(poetry_file, 'r',encoding='UTF-8') as f:\n",
    "        for line in f:\n",
    "            x = line.strip() + \"]\"\n",
    "            # x = x.split(\":\")[1]\n",
    "            if len(x) <= 5 :\n",
    "                continue\n",
    "            # 确保导入的诗句，是五言诗\n",
    "            if x[c.max_len-1] == '，':\n",
    "                files_content += x\n",
    "            \n",
    "    words = sorted(list(files_content))\n",
    "    counted_words = {}\n",
    "    for word in words:\n",
    "        if word in counted_words:\n",
    "            counted_words[word] += 1\n",
    "        else:\n",
    "            counted_words[word] = 1\n",
    "\n",
    "    # 去掉低频的字，这样可以去除一些怪异的字。\n",
    "    erase = []\n",
    "    for key in counted_words:\n",
    "        if counted_words[key] <= c.frequence_num:\n",
    "            erase.append(key)\n",
    "    for key in erase:\n",
    "        del counted_words[key]\n",
    "    wordPairs = sorted(counted_words.items(), key=lambda x: -x[1])\n",
    "\n",
    "    words, _ = zip(*wordPairs)\n",
    "    words += (\" \",)\n",
    "    # word到id的映射\n",
    "    word2num = dict((c, i) for i, c in enumerate(words))\n",
    "    num2word = dict((i, c) for i, c in enumerate(words))\n",
    "    word2numF = lambda x: word2num.get(x, len(words) - 1)\n",
    "    return word2numF, num2word, words, files_content\n",
    "\n",
    "# 清洗、准备数据\n",
    "c=config()\n",
    "word2numF, num2word, words, files_content = preprocess_file(c.poetry_file)\n",
    "#分割诗词，记录在poems_num中\n",
    "poems = files_content.split(']')\n",
    "poems_num = len(poems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里返回的四个变量，分别介绍如下：\n",
    "- word2numF：返回不同汉字对应的字典位置。\n",
    "- num2word：字典，key是数字，值是字符。\n",
    "- words：列表。所有的字符表，按照频率排序，先大再小。\n",
    "- files_content：字符串。所有的诗词，用“]”分开。\n",
    "\n",
    "按照字符频率，最多的是“，”，其次是“。”。poems为列表，存储所有的诗歌，poems_num为诗歌的数量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.应用模型\n",
    "\n",
    "根据模型能够根据输入的一组字来预测后面的字，那么就可以做出很多应用，如藏头诗，藏字诗等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 导入模型设置参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "# 导入训练好的模型\n",
    "model = load_model(c.weight_file)\n",
    "\n",
    "# 热度参数，默认设置为1，正常的参数\n",
    "temperature=1.0\n",
    "\n",
    "# 输入字符串，返回字符串\n",
    "def m_preds(sentence,length = 23,temperature =1):\n",
    "    '''\n",
    "    sentence:预测输入值\n",
    "    lenth:预测出的字符串长度\n",
    "    供类内部调用，输入max_len长度字符串，返回length长度的预测值字符串\n",
    "    '''\n",
    "    sentence = sentence[:c.max_len]\n",
    "    generate = ''\n",
    "    for i in range(length):\n",
    "        pred = m_pred(sentence,temperature)\n",
    "        generate += pred\n",
    "        sentence = sentence[1:]+pred\n",
    "    return generate\n",
    "\n",
    "# 输入字符串，返回字符\n",
    "def m_pred(sentence,temperature =1):\n",
    "    '''内部使用方法，根据一串输入，返回单个预测字符'''\n",
    "    if len(sentence) < c.max_len:\n",
    "        print('in def m_pred,length error ')\n",
    "        return\n",
    "\n",
    "    sentence = sentence[-c.max_len:]\n",
    "    x_pred = np.zeros((1, c.max_len, len(words)))\n",
    "    for t, char in enumerate(sentence):\n",
    "        x_pred[0, t, word2numF(char)] = 1.\n",
    "    preds = model.predict(x_pred, verbose=0)[0]\n",
    "    next_index = sample(preds,temperature=temperature)\n",
    "    next_char = num2word[next_index]\n",
    "\n",
    "    return next_char\n",
    "\n",
    "# 根据输入的矩阵，返回一个数字\n",
    "def sample(preds, temperature=1.0):\n",
    "    '''\n",
    "    当temperature=1.0时，模型输出正常\n",
    "    当temperature=0.5时，模型输出比较open\n",
    "    当temperature=1.5时，模型输出比较保守\n",
    "    在训练的过程中可以看到temperature不同，结果也不同\n",
    "    就是一个概率分布变换的问题，保守的时候概率大的值变得更大，选择的可能性也更大\n",
    "    '''\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    exp_preds = np.power(preds,1./temperature)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    pro = np.random.choice(range(len(preds)),1,p=preds)\n",
    "    return int(pro.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**热度参数`temperature`说明**\n",
    "\n",
    "一般来说，temperature=1.0输出的比较正常。小于1比较开放，大于1则比较保守。就是一个概率分布变换的问题，保守的时候概率大的值变得更大，选择的可能性也更大。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 藏头诗升级版"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**思路分析：**\n",
    "\n",
    "总觉得普通的藏头诗效果不太好，于是设计了一种思路：先在诗词库中找到以“藏头字”开头的词语，优先嵌入诗句，让生成的诗句变得有意义。\n",
    "\n",
    "**实现方式：**\n",
    "\n",
    "首先需要分词，对诗词库中的所有诗句进行分词。不选择原始语料，毕竟诗词库中已经整理过，是五言诗句。\n",
    "\n",
    "这需要安装jieba库。这是一个常用的中文分词库，还支持词云之类的功能。jieba分词原理比较简单，根据固有的词库进行关联度分析，出现频率大的词语进行有效切分，因而产生的词语符合我们大众的组词习惯。\n",
    "\n",
    "安装命令（使用清华镜像）：pip install -i https://pypi.tuna.tsinghua.edu.cn/simple jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /var/folders/01/hcypgmm52h7fbcggc7sfr1140000gn/T/jieba.cache\n",
      "Loading model cost 1.174 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "txt=files_content  # 导入诗词库\n",
    "word_object = jieba.lcut(txt,cut_all=False)  # 生成一个列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1183798\n"
     ]
    }
   ],
   "source": [
    "# 输出分词的个数\n",
    "print(len(word_object))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "214177\n",
      "['(', ';', ']', 'ē', 'ī', 'ū', 'ǖ', '□', '、', '。', '々', '一', '一一', '一万', '一万二千', '一万匹', '一万卷', '一万里', '一丈', '一上', '一上计', '一下', '一不中', '一不借', '一不改', '一不逢', '一世', '一丘', '一丘乐', '一丘余', '一丘常', '一丘藏', '一丘趣', '一丛', '一丛叶', '一丛秋', '一丛芳', '一丛菊', '一丛金', '一东', '一丝', '一两', '一两声', '一两日', '一两杯', '一两点', '一两片', '一两首', '一个', '一中', '一串', '一临', '一临泛', '一丸', '一丸销', '一为', '一为制', '一为表', '一主', '一举', '一乐', '一乖', '一乘', '一书', '一书下', '一书壁', '一书扎', '一书来', '一事', '一二', '一于', '一云取', '一云毕', '一云返', '一井泉', '一交亲', '一亩', '一京尘', '一人', '一人佩', '一人农', '一人前', '一人常', '一人成', '一人来', '一人耕', '一人计', '一人调', '一人闲', '一人顾', '一仆', '一从', '一从亲', '一从入', '一从别', '一从化', '一从山', '一从弃', '一从持', '一从换']\n"
     ]
    }
   ],
   "source": [
    "#删除重复的元素（分词结果），这个工作需要一定的时间\n",
    "new_word_object=list(set(word_object))\n",
    "#合并后的词语个数\n",
    "print(len(new_word_object))\n",
    "# 排序\n",
    "new_word_object=sorted(new_word_object)\n",
    "# 输出前面几个对象\n",
    "print(new_word_object[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获得某个字开头的所有词语\n",
    "def find_str(s,li):\n",
    "    t=[]\n",
    "    for i in li:\n",
    "        if i[0]==s:\n",
    "            t.append(i)\n",
    "    #return t\n",
    "    index = random.randint(0, len(t)-1)\n",
    "    return str(t[index])\n",
    "\n",
    "# '''根据给4个字，生成藏头诗五言绝句'''\n",
    "def predict_hide(text,temperature = 1):\n",
    "    if len(text)!=4:\n",
    "        print('藏头诗的输入必须是4个字！')\n",
    "        return\n",
    "    islow=''\n",
    "    for t in text:\n",
    "        if t not in words:\n",
    "            islow = t\n",
    "            break\n",
    "    if islow:\n",
    "        print('输入的字存在冷僻字：'+ islow)\n",
    "        return\n",
    "    \n",
    "    # 在分词库中，随机找出一个词语\n",
    "    wlist=find_str(text[0],new_word_object)\n",
    "    #选取随机一首诗的最后max_len字符 + 找到词语作为初始输入\n",
    "    index = random.randint(0, poems_num)\n",
    "    sentence = poems[index][len(wlist)-c.max_len:] + wlist\n",
    "    generate = str(wlist)\n",
    "    # print('引子 = ',sentence)\n",
    "\n",
    "    for i in range(5-len(wlist)+1):\n",
    "        next_char = m_pred(sentence,temperature)           \n",
    "        sentence = sentence[1:] + next_char\n",
    "        generate+= next_char\n",
    "\n",
    "    for i in range(3):\n",
    "        # 在分词库中，随机找出一个词语\n",
    "        wlist=find_str(text[i+1],new_word_object)\n",
    "        generate += wlist\n",
    "        sentence = sentence[len(wlist):] + wlist\n",
    "        for i in range(5-len(wlist) + 1):\n",
    "            next_char = m_pred(sentence,temperature)           \n",
    "            sentence = sentence[1:] + next_char\n",
    "            generate+= next_char\n",
    "\n",
    "    return generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一人前见相，杯盘一为留。浊酒中传断，酒备香日东。\n",
      "一挥成若思，杯湖花得朝。浊世烟中曲，酒趣北登郡。\n",
      "一期剑一晚，杯中酒使倚。浊劫阳岂西，酒注青路阴。\n"
     ]
    }
   ],
   "source": [
    "# 执行\n",
    "for i in range(3):\n",
    "    #藏头诗\n",
    "    sen = predict_hide('一杯浊酒',temperature = temperature)\n",
    "    print(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
